---
title: "추천시스템을 위한 딥러닝"
escerpt: "추천시스템을 위한 딥러닝"

categories:
  - Recommand
tags:
  - [AI, Recommand, framework]

toc: true
toc_sticky: true

breadcrumbs: true

date: 2024-01-23
last_modified_at: 2024-01-23

comments: true
  

---

## Recommendation with deep learning
- 신경망을 활용한 추천시스템
- 사용자의 평점이나 구매 기록으로 신경망을 훈련해 추천시스템 만들수 있음. 딥러닝은 패턴 인식에 매우 뛰어나며 이미지 인식이나 연속된 사건 예측에 유용함. 추천시스템은 그럼 패턴인식문제에 해당하는가?
- tensorflow는 신경망을 알기보단 단지 행렬의 곱셈과 덧셈을 인공 뉴련과 가중치, 편향을 나타내는 텐서에 적용해 신경망 구조를 구현하는것이다. 신경망은 기본적으로 행렬 연산을 이용하며 기본적으로 유사한 작업을 수행하는 추천시스템에 대해 이미 잘 확립된 행렬 분해 기법이 있는데 가령 "특이값 분해" 기법인 SVD는 확률적 기울기 하강법으로 구한 가중치를 이용해 행렬 곱셈을 한다. 거의같지만 방식이 다를뿐인거다! 그러니 추천시스템을 타인의 행동에서 아주 복잡한 패턴을 찾아내는 패턴인식문제로도 볼수 있다. 
- 이렇듯 행렬분해는 신경망으로 구현할수있고 신경망을 활용한 추천 시스템 실험이 이루어질수 있음.
- ex) 아마존은 데스티니(DSSTNE)라는 오픈소스시스템을 공개함. 거대한 규모의 신경망을 활용해 클러스터 내의 희소 데이터를 효율적으로 처리할 수 있고 아마존에서 자체적으로 추천시스템에 사용하고 있음.
- 클러스터에서 텐서플로를 사용해 GPU를 총동원하는 방법도 있음.


## Restricted Boltzmann Machines(RBM)
- 넷플릭스는 행렬분해와 RBM이 RMSE로 축정했을때 최고의 성능을 보여줬으며 둘의점수가 거의 같았음. 즉, 행렬분해를 신경망으로 모델링 할수 있음. 하지만 행렬분해와 RBM을 결합해서 함께 활용한 결과 더 나은 결과를 발견했는데 RMSE가 8.9에서 8.8로 줄었음.
- 논문 : Restricted Boltzmann Machines for Collaborative Filtering
  - 토론토 대학 연구팀의 논문.
  - 2007년 제 24회 국제머신러닝 컨퍼런스의 보고에 실렸음.
  - RBM은 신경망의 가장 단순한 형태 중 하나로 은닉층과 가시층, 두층만 있는데 훈련데이터를 순방향으로 보내어 모델을 훈련하며, 두 층간의 가중치와 편향은 역전파 과정에서 학습하고 ReLU와 같은 활성화 함수를 사용해서 은닉 뉴런으로부터 출력층을 생성함.
  ![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/148cde3e-28ef-43dc-b0c7-640825c7d063)

  - 왜 이름이 제한된 볼츠만일까? 모델이 제한되는 이유는 같은 층의 뉴런들끼리 직접적으로 소통하지 못하기 때문. 두개의 다른 층에만 연결이 있을뿐이다. 요즘 신경망에서는 보통 이렇게 하지만, 이러한 제한은 볼츠만 머신 이전에는 존재하지 않았음.RBM은 샘플링함수에 사용되는 볼츠만 분포함수를 지칭하는데 RBM을 고안한 사람은 제프리 힌스턴으로 당시 카네기 멜런 대학의 교수였으며, 그 시기는 1985년. RBM은 설명한대로 순방향으로 훈련되며, 역방향을 통해 입력이 재구성되는데 이를 여러 에포크를 거쳐 반복 시행함. 심층 신경망을 훈련할때처럼. 오류를 최소화하는 가중치와 편향으로 수렴될 때까지 시행함.
  - 역방향에 대해 알아볼까? (rbm backward pass)
  ![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/2df8b510-0ee5-4e71-af9c-a391668b0a41)

    - 역방향 동안, 순방향의 출력을 은닉층을 통해 역으로 전달함으로써 원래의 입력을 재구성하면서 가시층으로 나오는 값을 살펴보게 되는데 가중치는 초반에 무작위이기에 처음의 입력과 재구성한 입력 간에는 큰 차이가 있을 수 있고 그 과정에서 가시층에서의 또 다른 편향들을 얻게 된다.  가중치는 순방향과 역방향에서 같지만 편향 세트는 층별로 다른데 순방향에서 쓰는 은닉 편향과 역방향에 쓰는 가시 편향이 존재하는데 우리는 결과적인 오차를 측정해 이를 바탕으로 다음 반복에서 가중치를 약간 조정해서 오차를 줄여나간다. 개념적으로 최근 용어로 Linear threshold unit(LTU)라고 부르는것과 크게 다르지 않다. 최근 딥러닝 아키텍처와 유사한 다층 RBM을 구축할수도 있다. 가장 큰 차이점은 RBM에서는 역방향 과정 동안 하위 레벨에서 출력을 읽지만 최근의 신경망에서는 반대편으로 산출되는 출력을 취한다는것이다. 이 모든 건 훈련데이터가 완벽할 땐 잘 작동하지만 그외에는 잘작동안한다.왜냐하면 데이터가 희소해졌기 때문이다. 데이터가 희소하기 때문에 대부분의 입력노드에 데이터가 안들어오니깐 어떻게 신경망을 훈련하냐?

  - rbm's for recommender systems
  ![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/5a75d7aa-e79a-4264-9127-61355cb05240)

    - 영화 추천의 5점 만점 평점에 RBM을 적용한다면 방금보았던 기존 RBM 아키텍처에서 약간의 변화를 줘야한다.
    - 전체를 보자. 기본아이디어는 훈련 데이터 내의 각 사용자를 입력으로써 RBM에 제공해 모델을 훈련하는 건데 훈련과정에서 각 사용자는 배치의 한 부분으로 처리되며 평가한 평점들을 전부 읽어오게 되는데 visible node는 제공된 사용자의 모든 영화에 대한 평점을 나타내며, 우리가 알지 못하는 사용자/영화 한 쌍의 평점을 재구성하고자 가중치와 편향을 학습하게 된다. visible node는 하나의 입력을 받아들이는 단순한 노드가 아니고 평점은 범주형 데이터이므로 각 5종류의 평점을 총 5개의 노드로서 취급해야 한다. 훈련데이터의 첫번째 평점이 5점이라고 하면 그림에서 보든 4개의 노드에서 0이고 하나의 노드에서 1인걸로 표현가능하다.사용자와 아이템 한 쌍의 일부에 대해선 아직 평점을 알지 못하며 예측해야 하는 부분이다. 또 3점까지 평점은 이렇게 3번째 슬롯에 1을 갖는 걸로 표현되는데 RBM훈련이 끝나면 어떤 사용자의 평점도 재구성할 수 있도록 가중치와 편향이 구해질 것이다. 이를 활용해 새로운 사용자의 평점을 예측하려면 해당 사용자의 공개된 평점을 가지고 학습을 한 번 더 실행하면 되는데 순방향으로 한번 진행하고 역방향으로 다시 진행한 후 해당 사용자의 평점이 재구성된다. 그 후 소프트맥스 함수를 각 평점 그룹에 적용해 출력을 각 아이템에 대한 5점 단위 평점으로 만들어 내는데 문제는 주어진 데이터가 희소하다는것! 가능한 모든 사용자와 영화 조합에 대해 RBM을 학습시킨다면 대부분의 데이터가 없을것이다. 특정 사용자가 평가하지 않은 영화가 대부분일 테니깐! 하지만 모든 영화의 평점을 예측해야 하니 그 모두를 위한 자리를 남겨둬야 하는데, 즉, N개의 영화가 있다면 N x 5개의 visible node를 갖게 되며 어떤 사용자가 됐든 그 node들은 대부분 비어있을것이다.  이 문제는 RBM을 훈련할 때 빠진 평점들을 처리 과정에서 모두 제외함으로써 해결한다. 이렇게 하긴 좀 까다로운데 딥러닝 대부분의 프레임워크는 언제나 모든 걸 병렬적으로 처리하길 원한다고 전제하기 때문! 이는 희소 데이터를 잘 처리하지 못하지만 우리가 원하는 바를 구현해낼 요령은 있다. 이  그림에서 선은 평점데이터가 존재하는 visible node와 은닉층 간에만 그어져 있음을 주목하고 제공된 사용자의 공개된 평점으로부터 RBM을 훈련할 때 사용자가 실제로 평가한 영화들을 통해서만 가중치와 편향들을 학습하게 된다. 다른 사용자들에 대해서 학습을 반복하면서 다른 가중치와 편향도 채워 넣게 된다. 참고할 부분은 최근 tensorflow에는 활용 가능한 희소 tensor가 있다는것. 또한 아마존 DSSTNE처럼 희소 데이터를 통해 일반적 신경망을 구성하는 framework도 존재하기 때문에 RBM은 어쩌면 유물이 될지도 모름. 신경망에서 완비된 데이터를 다루듯 희소한 데이터도 다룰수 있게 되었으니깐! 또 다른 사항은 거대한 희소데이터에서 RBM을 어떻게 최적으로 훈련햐느냐인데 경사하강법에는 최적화에 아주 효율적인 기대함수가 필요하며, 추천시스템에서는 이 함수를 "대조발산"이라 한다.(contrastive divergence). 이는 논문에서 사용한 함수는 기대함수인데 이 함수는 훈련하는 동안 깁스 샘플링(gibbs sampler) 이란것을 사용해 확률 분포를 샘플링한다는 것이다. 기본 평점으로만 모델을 훈련시키지만 다른 사용자에서 얻은 가중치와 편향을 재사용해서 평점을 예측한다.
    - 실습(deep learning)


## 추천을 위한 오토 인코더: Rec를 위한 딥러닝

## RNN을 사용한 클릭스트림 권장 사항

## 추천을 위한 Generative Adversarial Networks

## TensorFlow Recoomenders(TFRS) : 검색 단계 소개 및 구축

## TensorFlow Recoomenders(TFRS) : 순위 단계 구축

## TFRS : 부가 기능 및 심층 검색 통합

## TFRS : 다중작업 추천자 Deep & Cross Network ScanN 및 Serving

## Deep Factorization Machines


---


[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}