---
title: "추천시스템의 metric"
escerpt: "추천시스템의 metric"

categories:
  - Recommand
tags:
  - [AI, Recommand, metric]

toc: true
toc_sticky: true

breadcrumbs: true

date: 2024-01-14
last_modified_at: 2024-01-14

comments: true
  

---




## 추천시스템의 정확도 metric

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/ee7d796c-60ba-493e-9775-a1f32f7cb679)


  - MAE, RMSE는 데이터 scale에 의존하는 회귀 모델 평가 지표이다. 값이 작을 수록 회귀 성능이 좋다.
  - MAE는 각각의 평점 예측 오차의 절대값을 더함.
  - RMSE는  각각의 평점 예측오차를 제곱한 값을 더하는 것.

### MAE(mean absolute error)
![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/167cbe1b-5c77-4ccf-b672-cefe76f6815e)

  - 평균 절대오차
  - test set에 시험하고 싶은 N개의 평점이 있다면, 각각의 평점에 대해 평점 또는 시스템이 예측하는것을 y, 사용자가 실제로 준 평점을 x로 놓을수 있다. 그 평점 예측의 오차를 측정하기 위해서는 이 둘의 차이의 절댓값을 취하면 됨. 즉, 예측된 평점과 실제 평점 사이의 차이이다!
  - 오류는 나쁜값이기에, MAE가 낮을수록 좋은 모델이다. 

### RMSE (root mean square error)
![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/5adcae6a-6f6e-4021-99d4-5ae8dd3e6c9a)

  - 평균 제곱 오차의 제곱근
  - 예측값의 예상이 벗어낫을때 더 많은 패널티를 주고, 예측이 근접하다면 패널티를 적게 주는방식. 
  - 제곱값을 취하게 됨으로써 절대값처럼 양수를 얻게되고 이것은 또한 더 큰 오차에 대한 패널티를 부풀림.
  - 제곱근 : 각각의 개별적인 평점 예측 오차의 제곱을 의미.

## 추천시스템의 적중률의 종류,방법
- 오프라인으로 순위 추천의 유효성을 측정하기 위한 방법들.
- 좋은 적중률과 나쁜 RMSC점수의 추천 시스템을 구축가능. 
- 즉, RMSC와 적중률이 항상 관계있는것은 아님.

### evaluating Top-N recommenders
- hit rate(적중률) = hits/users 
  : 시험집단에 있는 사용자에 대한 순위 추천을 발생시킴. 사용자의 순위 추천들 중 선택한 추천이, 그들이 실제로 평가한 것이라면 이것을 적중이라고 판단.

  - 모든 적중들을 더하고(summation hits) 사용자의 수로 나누면 이것이 적중률!

```
## 추천시스템으로 부터 평가 예측 목록을 모두 받아서 사용자의 ID를 그들의 최고 평가 항목과 이은 뒤 dictionary로 넘겨줌.
## 그리고 최소 평가항목을 넘겨서 사용자들이 별로 좋아할 것 같지 않은 항목들은 추천하지 않을수 있음.
## 하지만 default empty value라는 개념이 있다.한번도 사용한적이 없는 키에 접근하려 할때 사용.

def GetTopN(predictions, n=10, minimumRating=4.0):                   
                                                                    

    topN = defaultdict(list)


    for userID, movieID, actualRating, estimatedRating, _ in predictions:
        if (estimatedRating >= minimumRating):
            topN[int(userID)].append((int(movieID), estimatedRating))

    for userID, ratings in topN.items():
        ratings.sort(key=lambda x: x[1], reverse=True)
        topN[int(userID)] = ratings[:n]

    return topN
```

### leave-one-out-cross validation

- 개별등급의 정확도를 측정하지 않기 때문에 정확도에 사용했던 것과 동일한 train-test cross validation 검증 방법은 사용 못함.
- 개별 사용자를 위한 최고 추천 목록의 정확도를 측정.

- 단순히 훈련데이터를 이용하여 개별 사용자가 평가하고 100% 적중률을 달성한 실제로 상위 10위 안의 영화들을 추천할수 있음.
- 우리가 하는것은 우리의 훈련 데이터 속 각각의 사용자에 대해 순위 추천을 계산하고 의도적으로 그 사용자의 훈련 데이터 중 한 항목을 제외하는 것. 그리고 우리의 추천시스템이 시험 단계에서 사용자를 위해 생성한 순위 결과 속 그 제외된 항목을 추천할수 있는지를 검증!! 그래서 그 훈련 데이터에서 제외된 각각의 사용자에 대한 순위 목록에 있었던 항목을 추천하는 우리의 능력을 측정하는것.
- 문제점) 검증을 하면서 하나의 특정한 영화를 알맞게 가져오는것이 더 어려워짐. 그냥 몇개의 추천중에서 하나를 가져오는 것보다어려움.
- leave-one-out 교차검증은 작업할 매우 큰 data set이 있지 않은 한 측정하기에 매우 작고 어려운 경향이 있음. 하지만 추천시스템이 현실 세계의 순위 목록을 산출할 것을 알고 있을때 이것은 더욱 사용자 기반 측정법이 됨.

```
## 적중률을 계산하면, 각 사용자의 우선순위 영화들로 이루어진 사전과 훈련데이터 세트에서 남겨진 시험 평점 데이터 세트를 넘겨야 한다.
## leaveoneout 교차검증 기술: 사용자마다 한개의 평가를 제외한 뒤 우선순위 목록에서 빠진 영화를 추천하는 능력을 시험하는 기술.
def HitRate(topNPredicted, leftOutPredictions):
    hits = 0
    total = 0

    # For each left-out rating
    for leftOut in leftOutPredictions:
        userID = leftOut[0]
        leftOutMovieID = leftOut[1]
        # Is it in the predicted top 10 for this user?
        hit = False
        for movieID, predictedRating in topNPredicted[int(userID)]:
            if (int(leftOutMovieID) == int(movieID)):
                hit = True
                break
        if (hit) :
            hits += 1

        total += 1

    # Compute overall precision
    return hits/total
```

### average reciprocal hit rate(ARHR)

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/42c24e70-170a-437d-9039-835d989c27c5)

  - "적중률"의 변형은 **평균 상호 적중률**
  - 측정 항목은 "적중률"과 비슷, 최상위 목록에서 조회수가 표시되는 위치를 설명하므로 하단 슬롯보다 상단 슬롯의 항목을 성공적으로 추천하면 더 많은 신뢰를 얻음
  - 사용자 중심 측정법이다. 왜냐면 사용자들은 목록의 시작 부분에 집중하는 경향이 있기 때문.
  - 적중률과의 차이점)  적중의 수를 더하는 것 대신에 각 적중의 상호 순위를 더하는 것. 그래서 만약 우리가 3번 슬롯에서 성공적으로 추천을 예측한다면 이것은 단지 1/3로 간주되지만, 숨겨진 슬롯인 우리의 최상위 추천 중 하나는 1.0의 전체 값을 얻을수 있음.
  - 이것은 순위 추천이 표시된 방법에 좌우됨. 만약 사용자가 순위 목록에서 하위 항목들을 보기 위해 스크롤을 하거나 페이지를 옮겨 다녀야 한다면 목록에서 너무 낮은 위치에 나타나서 사용자가 그걸 찾기 위한 작업을 해야 한다면 좋은 추천이라도 불리하게 만들 수 있음.


### cumulative hit rate(cHR)
![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/7be95392-dcca-4340-96a1-781545f785c3)

  - 목록에서 사용자가 찾아야 할 추천 사항이 너무 적으면 "누적적중률"이 좋은 대안.
  - 예상한 평점이 어떤 한계치 아래에 있다면 적중을 버리는 것.
  - 실제로 즐기지 못할 것이라고 생각되는 사용자에게 항목을 추천하는 것에 관해 우리가 신뢰를 얻으면 안된다는 것.
  - train data에서 실제로 관찰된 평점에 기반한 추천을 걸러내는 것.
  - 여기에 관한 아이디어는 사용자가 좋아하지 않았다는 것을 우리가 아는 항목을 추천하는 것에 대한 신뢰를 얻지 못할 것.
  - ex) 별점 3점이라는 경계가 있다면 우리는 이 검사 결과에서 두번째와 네번쟤 항목을 버리고 우리의 적중률 측정법은 이것들을 정말로 고려하지 않을것.

```
## cHR: 누적적중률
## 적중률과 다른점은 제외되는 값이 있다는것. 
## 우리가 선택한 항목보다 높은 예측값을 갖지 않는다면 해당 적중을 버리는것. 그외에는 적중률과 비슷.
def CumulativeHitRate(topNPredicted, leftOutPredictions, ratingCutoff=0):
    hits = 0
    total = 0

    # For each left-out rating
    for userID, leftOutMovieID, actualRating, estimatedRating, _ in leftOutPredictions:
        # Only look at ability to recommend things the users actually liked...
        if (actualRating >= ratingCutoff):
            # Is it in the predicted top 10 for this user?
            hit = False
            for movieID, predictedRating in topNPredicted[int(userID)]:
                if (int(leftOutMovieID) == movieID):
                    hit = True
                    break
            if (hit) :
                hits += 1

            total += 1

    # Compute overall precision
    return hits/total
```

### Rating hit rate(rHR)
![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/a8c8f7eb-2f9f-46bf-9402-654952e3a77b)

  - 평점적중률(예상 등급 점수 분류)
  - 이것은 우리의 알고리즘의 실제로 적중된 추천 영화가 얼마나 좋은지에 대한 생각을 분배하는 것에 대해 아이디어를 얻는데 좋은 방법이 될수 있음.
  - 실제로 선호되는 영화를 추천하기를 원하고 분배를 쪼개어 살펴보는 것은, 더욱 세부적으로 잘하고 있는지에 대한 감을 얻게 함.

```
## RHR : 평가적중률
## 적중률기능과 같은방식. 각각의 평균값을 얻기 위해 적즁률을 추적함. 적중과 전체 사용자 수를 추적하는 하나의 변수보다는 적중과 각 평가 종류별 총계를 추적하는 별개의 사전을 사용. 그리고 마지막에 결과를 도출.
def RatingHitRate(topNPredicted, leftOutPredictions):
    hits = defaultdict(float)
    total = defaultdict(float)

    # For each left-out rating
    for userID, leftOutMovieID, actualRating, estimatedRating, _ in leftOutPredictions:
        # Is it in the predicted top N for this user?
        hit = False
        for movieID, predictedRating in topNPredicted[int(userID)]:
            if (int(leftOutMovieID) == movieID):
                hit = True
                break
        if (hit) :
            hits[actualRating] += 1

        total[actualRating] += 1

    # Compute overall precision
    for rating in sorted(hits.keys()):
        print (rating, hits[rating] / total[rating])
```


## 추천시스템에서 정확성이외에 중요한 요소들.
### coverage : 제공률
  - 제공률은 추천시스템이 제공할 수 있는 추천 데이터의 비율. 
  - movielens의 영화 평점을 생각해보면 수넟ㄴ개에 달하는 영화에 평점이 달려있음.하지만 평점이 없는 영화들 역시 존재함. 그데이터를 IMDB에 적용한다면 이 추천시스템의 제공률은 낮다고 볼수 있음.적용대상인 IMDB의 영화 개수가 훨씬많기 때문.
  - 중요한건 제공률이 정확성과 상충할 수 있다는 점.
  - 만약 추천 데이터의 품질을 높이고 싶다면 당신은 정확성을 높일것. 대신 제공률은 떨어질것. 추천하기에 적당한 균형점을 찾기가 쉽지는 않을수도 있음.
  - 제공률도 중요한 이유는 새로운 추천 데이터가 사용자에게 얼마나 빨리 나타나는지를 보여주는 요소이기 때문.
  - 아마존에 신간이 나왔다고 보면 사람들이 그 책을 사기 전까지는 그 책은 추천 목록에 뜨지 않을것이다.  그 책을 사야지만 동향 데이터가 생성되기 떄문.
  - 동향 데이터가 생기기 전까지 해당 책은 아마존의 제공률을 떨어뜨리는 요소일것.

```
## 제공률
## 여러 항목중에서도 최소 한가지의 좋은 추천 항목을 가진 사용자의 비율.
## 현실에서 여러분은 추천데이터에 있는 항목보다 더많은 항목이 있는 카탈로그가 있음.
## 그리고 카탈로그 항목으로 제공률을 계산할것임.
# What percentage of users have at least one "good" recommendation
def UserCoverage(topNPredicted, numUsers, ratingThreshold=0):
    hits = 0
    for userID in topNPredicted.keys():
        hit = False
        for movieID, predictedRating in topNPredicted[userID]:
            if (predictedRating >= ratingThreshold):
                hit = True
                break
        if (hit):
            hits += 1

    return hits / numUsers
```

### diversity : 다양성
  - (1-S) : S = avg similarity between recommendation pairs.
  - 항목간의 유사성 평균값을 1에서 뺀값.
  - 추천 시스템이 얼마나 다양한 범주의 데이터를 사용자들에게 제공하는가를 의미. 
  - 다양성이 낮다는 것은 예를 들면, 당신이 읽는 책의 다음 편을 추천하는 것과 같음. 다른 작가의 책이나 관련된 영화를 추천해주지는 않는것. 이는 주관적인 요소로 보일수 있지만, 사실 측정이 가능함.
  - 많은 추천 시스템들은 데이터 간 유사성을 계측해서 작동하기 때문에 거기서 계측한 값으로 다양성을 측정하는 것.
  - 우선 추천목록에서 데이터끼리 번갈아 짝을 지어 유사성 값을 내면 그 값의 평균값이 추천목록 데이터의 유사성을 나타내는 측정값이다. 우리는 그 값을 S라고 한다. 
  - 다양성은 유사성의 반대 개념이기 때문에 1-S를 하여 다양성의 값을 도출할 수 있다.
  - point) 다양성은 추천 시스템 안에서는 좋은 요소가 아님!
  - 이유) 다양성이 높다는 것은 아무거나 추천을 한다는 뜻이기 떄문.
  - 다양성 값이 높다는 것은 좋은 추천보다 좋지 못한 추천을 받았다는 의미임!

```
## 다양성
## 시스템의 우선 추천 순위뿐만 아니라 데이터 세트에 있는 항목별, 유사성 점수 metric도 필요.
## 먼저 유사성 metric을 보자. 
## 기본적으로 2x2배열이고 우리가 빠르게 찾을수 있는 항목들을 모두 짝지어서 도출한 유사성 점수를 포함. 
## 그다음 각 사용자별 우선 추천 항목들을 다룬다. 한번에 사용자 한명씩.
## itertools.combinations() 호출은 다시 우선순위 항목에서 모든 항목을 짝지어 준다. 그럼 한 쌍씩 반복해서 각 쌍의 유사성을 찾을수 있다.
## surprise에는 각 사용자와 항목의 내부 ID들이 차례대로 나열되어 있고 이 데이터들은 우리의 실제 평가 데이터에 있는 사용자나 영화의 ID와는 다른 데이터이다.
## 유사성 metric은 이런 내부 ID를 사용하기 때문에 우리가 가진 ID들도 유사성 점수를 찾아보기 전에 내부 ID로 바꿔줘야 한다.
## 모든 유사성 점수를 합친 뒤 평균값을 내고 1에서 빼주면 다양성 metric값이 나온다.
## 우리가 가진 데이터 세트에서 각 사용자별 추천 항목들의 모든 조합을 이용해서 이코드를 실행하는것은 꽤 복잡한 계산임.
def Diversity(topNPredicted, simsAlgo):
    n = 0
    total = 0
    simsMatrix = simsAlgo.compute_similarities()
    for userID in topNPredicted.keys():
        pairs = itertools.combinations(topNPredicted[userID], 2)
        for pair in pairs:
            movie1 = pair[0][0]
            movie2 = pair[1][0]
            innerID1 = simsAlgo.trainset.to_inner_iid(str(movie1))
            innerID2 = simsAlgo.trainset.to_inner_iid(str(movie2))
            similarity = simsMatrix[innerID1][innerID2]
            total += similarity
            n += 1

    S = total / n
    return (1-S)
```

### novelty : 참신성
  - mean popularity rank of recommended items
  - 추천항목이  인기가 없는지 있는지 인기척도를 확인해주는 metric
  - 추천데이터가 얼마나 대중적인지 나타냄.
  - 참신성을 높이려면 아무거나 추천을 하면 안됨. 대부분의 데이터는 대중적인 데이터가 아니기 떄문. 참신성도 추측이 되지만 그 값의 해석은 주관적인 경우가 많기 때문.
  - 추천 시스템에는 사용자 신뢰라는 개념이 있음.

```
## 참신성
## 각 항목의 인기 순위를 담은 사전을 설정값으로 두고 그다음은 그냥 각 사용자의 우선 추천항목을 거친 뒤 각 추천 항목들의 인기 순위의 평균값을 계산.
def Novelty(topNPredicted, rankings):
    n = 0
    total = 0
    for userID in topNPredicted.keys():
        for rating in topNPredicted[userID]:
            movieID = rating[0]
            rank = rankings[movieID]
            total += rank
            n += 1
    return total / n

```

### long tail
![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/ed7d6bb8-39fa-46ff-9db6-b0735c19f200)

  - 추천시스템의 핵심은 **long tail** 전략으로 데이터를 제공하는 것이기 때문. 추천시스템은 노란 선상의 제품들을 보고 자신들의 희귀한 수요에 부합한 추천이라고 생각하게 만든다. 이를 활용하면 시스템이 추천한 데이터들이 새로운 사용자들을 발견할 수도 있고 사용자들에게 다양한 기회를 줄 수도 있고 사용자들에게 다시 돌아갈 이익을 창출할것임.
  - 추천시스템에서 "참신성"값을 잘 활용하면 더 좋은 방향으로 갈것임.
  - 즉, 참신성과 신뢰성사이에서 균형점을 찾는것이 중요한것!


### churn : 이탈
- 변화가 많다면 이탈점수가 높다는 뜻.
- 같은 사용자에게 계속 같은 추천을 하는것은 no good.
- 만약 사용자가 계속 뜨는 추천을 눌러보지 않는다면 추천시스템은 해당 항목 추천을 멈추고 다른항목을 추천해야 하는가?
- 다양성과 참신성과 같이 높은 이탈 값으 그다지 좋은게 아님.
- "이탈"점수를 높이기 위해서는 무작위로 항목들을 추천할 수도 있으며, 이는 좋은 추천시스템이 아니다.
- 이런 종류의 metric들은 모두 묶어서 고려할필요가 있음. 서로 균형을 맞춰야 한다는점도 알아야 함.

### responsiveness : 민감성
- 사용자의 새로운 행위가 추천 시스템에 얼마나 빠르게 영향을 주는지를 나타냄.
- ex) 신작영화를 평가한다면 그 행위가 다른 사람들의 추천 목록에 즉시 영향을 줄까? 아니면 하루 뒤 어떤 작업후에 추천 목록이 뜰까?
- 민감성 값이 높은 것이 좋다고 느낄수 있음. 하지만 사업적인 측면에서는 추천시스템이 어느 정도 민감해야 하는지를 설정할필요가 있음. 왜냐면 추천시스템이 즉각 반응하는 방식이라면 복잡해지고, 시스템 유지가 어렵고 시스템을 구성하는 비용이 커짐.시스템 고유의 균형값을 찾아야 함.민감성과 간소성사이에서!

### online A/B test
- 가장 중요한 metric이다!
- 실제 고객에게 맞춰서 추천 시스템을 수정하고 고객들이 추천목록에 어떻게 반응하는지 측정하는 것.
- 여러 가지 알고리즘에서 나온 추천 목록들을 서로 다른 사용자 데이터 세트에 입력할수 있음. 그리고 여러분들이 추천항 항목들을 실제로 사는지, 훑어보는지 혹은 다른식으로 관심을 표현하는지 측정해보는것.
- 통제가 가능한 온라인 실험을 이용해서 추천시스템의 변화를 시험해보면 추천시스템이 사람들에게 새로운 항목을 보여주고 더 많은 구매가 이루어지도록 하는지 관찰가능.

### surrogate problem : 대리문제
- 평가를 정확하게 측정했는데도 영상 추천이 좋지 못할수도 있음.
- 정확성이 꼭 좋은 추천 목록을 만들어낸다는 보장은 없음.
- 온라인 A/B test의 결과가 추천시스템에서 가장 중요한 평가 지표임. 고객들이 추천항목에 얼마나 돈을 써서 품질을 평가했는지 측정해보는것.



---


[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}