---
title: "추천시스템의 metric"
escerpt: "추천시스템의 metric"

categories:
  - Recommand
tags:
  - [AI, Recommand, metric]

toc: true
toc_sticky: true

breadcrumbs: true

date: 2024-01-14
last_modified_at: 2024-01-14

comments: true
  

---

## 1.추천시스템의 평가 방법
1. 오프라인 평가
2. 온라인A/B Test

### Q) 추천시스템에 오프라인 평가 사용이유는?
- 추천시스템을 개발하는 동안 확인할 수 있는 유일한 지표.
- 온라인 A/B Test는 사용자 행동 수집하고 시스템 점수 매기는것은 비용과 시간이 많이 소요됨.
- 오프라인 지표의 정확성이 좋고 온라인 A/B Test 점수가 좋은것이 가장 좋은 추천시스템이다.

## 2. 오프라인으로 테스트할수 있는 방법
1. 훈련-테스트 분할
2. K-fold 교차 검증

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/76a87620-fcc3-428c-b0d0-17b28d518f93)

  - 위 방법의 정확성은 과거 데이터에 따라 달라지며, 실제 사용자가 이미 본 내용을 예측하려고 함.
  - 수집된 데이터가 너무 old하면 정확도가 높아도 무의미함.
  - 왜냐하면 1년전 관심사가 1년 후의 관심사와 다르기 때문.
  - 오프라인 측정항목은 우리가 원하는 것을 측정하지 않으며, 오프라인 측정항목에 전적으로 의존하는 것은 권장되지 않음.

## 3. 다양한 오프라인 지표
1. MAE (Mean Absolute Error, 평균 절대 오차)
2. RMSE (Root Mean Square Error, 제곱평균제고급 오류)
3. HR (Hit Rate,적중률)
4. ARHR (Average Reciprocal Hit Rate, 평균 상호 적중률)
5. cHR (cumulative Hit Rate, 누적 적중률)
6. rHR (rating Hit Rate, 평가 적중률)
7. coverage (적용 범위)
8. diversity (다양성)
9. novelty (참신성)
10. churn (이탈)
11. responsiveness (민감성)

### 3-1. MAE(mean absolute error) , RMSE (root mean square error)
추천시스템의 정확도 metric

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/ee7d796c-60ba-493e-9775-a1f32f7cb679)

  - RMSE는 각각의 "실제값(등급)과 예측값의 차이"를 **제곱**후 summation 한후, 전체로 나눠준것을 비교를 위해 전체 항의 제곱근
  - MAE는 각각의 "실제값(등급)과 예측값의 차이"의 **절대값**을 summation 후 전체로 나눠주기.
  - MAE, RMSE는 데이터 scale에 의존하는 회귀 모델 평가 지표. 값이 작을 수록 회귀 성능이 좋다.
  - RMSE는 항상 MAE보다 크다.



![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/1cf54707-22fb-4562-9a8d-f9e6e5892416)

  - MAE
    - 평균 절대오차
    - N : test set에 실험하고 싶은 N개의 평점
    - y : 각각의 평점에 대해 평점 또는 시스템이 예측하는것.
    - x : 사용자가 실제로 준 평점
    - 평점 예측의 오차를 측정하기 위해서는 이 둘의 차이의 절댓값을 취하면 됨. 즉, 예측된 평점과 실제 평점 사이의 차이이다!
    - 오류는 나쁜값이기에, MAE가 낮을수록 좋은 모델이다. 
  - RMSE
    - 평균 제곱 오차의 제곱근
    - 예측값의 예상이 벗어낫을때 더 많은 패널티를 주고, 예측이 근접하다면 패널티를 적게 주는방식. 
    - 제곱값을 취하게 됨으로써 절대값처럼 양수를 얻게되고 이것은 또한 더 큰 오차에 대한 패널티를 부풀림.
    - 제곱근 : 각각의 개별적인 평점 예측 오차의 제곱을 의미.
    - 알고리즘의 RMSE값이 1.87로 나왔을때 비교할 수 있는 다른 RMSE값이 있을때까지 아무 의미가 없음.


 
### 3-2. HR (Hit Rate,적중률)
적중률 = (테스트 중 조회수)/(사용자수)
hit rate = hits / users

- evaluation Top-N recommenders의 방법이다.
- 시험집단에 있는 사용자에 대한 순위 추천. 사용자의 순위 추천들 중 선택한 추천이, 그들이 실제로 평가한 것이라면 이것을 적중이라고 판단.
- 모든 적중들을 더하고(summation hits) 사용자의 수로 나누면 이것이 적중률!
- 적중률은 미래데이터가 아닌 과거 데이터를 예측한다.(온라인 A/B Test를 통해서만 가능)
- 적중률 측정위해선 제일 먼저 상위 N추천을 생성. 생성된 상위N 추천에 사용자가 평가한 항목이 포함된 경우 test set완료.
- 적중률이 높을수록 추천시스템이 좋아질것이다.


```
## 추천시스템으로 부터 평가 예측 목록을 모두 받아서 사용자의 ID를 그들의 최고 평가 항목과 이은 뒤 dictionary로 넘겨줌.
## 그리고 최소 평가항목을 넘겨서 사용자들이 별로 좋아할 것 같지 않은 항목들은 추천하지 않을수 있음.
## 하지만 default empty value라는 개념이 있다. 한번도 사용한적이 없는 키에 접근하려 할때 사용.

def GetTopN(predictions, n=10, minimumRating=4.0):                   
                                                                    

    topN = defaultdict(list)


    for userID, movieID, actualRating, estimatedRating, _ in predictions:
        if (estimatedRating >= minimumRating):
            topN[int(userID)].append((int(movieID), estimatedRating))

    for userID, ratings in topN.items():
        ratings.sort(key=lambda x: x[1], reverse=True)
        topN[int(userID)] = ratings[:n]

    return topN
```
### 3-2-1. leave-one-out-cross validation(LOOCV)
적즁률을 측정하기 가장 좋은방법으로 Leave One Out 교차검증을 사용.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/136302a3-6402-4072-b73a-aaaa0f76a19a)

- 개별 사용자를 위한 최고 추천 목록의 정확도를 측정.

- 훈련데이터를 이용하여 개별 사용자가 평가하고 100% 적중률을 달성한, 실제로 상위 10위 안의 영화들을 추천할수 있음.
- 훈련 데이터 속 각각의 사용자에 대해 순위 추천을 계산하고 그 사용자의 훈련 데이터 중 한 항목을 제외하는 것. 
- 그리고 추천시스템이 시험 단계에서 사용자를 위해 생성한 순위 결과 속, 그 제외된 항목을 추천할수 있는지를 검증!! 


- 문제점) 검증을 하면서 하나의 특정한 영화를 알맞게 가져오는것이 더 어려워짐. 그냥 몇개의 추천중에서 하나를 가져오는 것보다 어려움.

- leave-one-out 교차검증은 작업할 매우 큰 data set이 있지 않은 한 측정하기에 매우 작고 어려운 경향이 있음. 
- 하지만 추천시스템이 현실 세계의 순위 목록을 산출할 것을 알고 있을때 이것은 더욱 사용자 기반 측정법이 됨.

```
## 적중률을 계산하면, 각 사용자의 우선순위 영화들로 이루어진 사전과 
## 훈련데이터 세트에서 남겨진 시험 평점 데이터 세트를 넘겨야 한다.

## leaveoneout 교차검증 기술: 사용자마다 한개의 평가를 제외한 뒤 
## 우선순위 목록에서 빠진 영화를 추천하는 능력을 시험하는 기술.

def HitRate(topNPredicted, leftOutPredictions):
    hits = 0
    total = 0

    # For each left-out rating
    for leftOut in leftOutPredictions:
        userID = leftOut[0]
        leftOutMovieID = leftOut[1]
        # Is it in the predicted top 10 for this user?
        hit = False
        for movieID, predictedRating in topNPredicted[int(userID)]:
            if (int(leftOutMovieID) == int(movieID)):
                hit = True
                break
        if (hit) :
            hits += 1

        total += 1

    # Compute overall precision
    return hits/total
```

### 3-3.ARHR (Average Reciprocal Hit Rate, 평균 상호 적중률)

적중률의 변형버전

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/cc7c5e54-af44-4ca1-a12e-71fa8cf50d75)

  - 측정 항목은 "적중률"과 비슷, 최상위 목록에서 조회수가 표시되는 위치를 설명하므로 하단 슬롯보다 상단 슬롯의 항목을 성공적으로 추천하면 더 많은 신뢰를 얻음
  - 사용자 중심 측정법이다. 왜냐면 사용자들은 목록의 시작 부분에 집중하는 경향이 있기 때문.


![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/42c24e70-170a-437d-9039-835d989c27c5)

  - 적중률과의 차이점)  적중의 수를 더하는 것 대신에 각 적중의 상호 순위를 더하는 것. 
    - 그래서 만약 3번 슬롯에서 성공적으로 추천을 예측한다면 이것은 단지 1/3로 간주되지만, 
    - 숨겨진 슬롯인 최상위 추천 중 하나는 1.0의 전체 값을 얻을수 있음.
  
  - 이것은 순위 추천이 표시된 방법에 좌우됨. 
    - 만약 사용자가 순위 목록에서 하위 항목들을 보기 위해 스크롤을 하거나 페이지를 옮겨 다녀야 한다면
    - 목록에서 너무 낮은 위치에 나타나서 사용자가 그걸 찾기 위한 작업을 해야 한다면 좋은 추천이라도 불리하게 만들 수 있음.


### 3-4. cHR (cumulative Hit Rate, 누적 적중률)
![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/7be95392-dcca-4340-96a1-781545f785c3)

  - 목록에서 사용자가 찾아야 할 추천 사항이 너무 적으면 **누적적중률**이 좋은 대안.
  - 예상 평점이 특정 임계값보다 낮을 경우 해당 평점 버리는 것.
  - train data에서 실제로 관찰된 평점에 기반한 추천은 걸러내는것.

```
## cHR: 누적적중률
## 적중률과 다른점은 제외되는 값이 있다는것!
## 특정 임계값보다 높은 예측값을 갖지 않는다면 해당 적중을 버리는것. 그외에는 적중률과 비슷.
def CumulativeHitRate(topNPredicted, leftOutPredictions, ratingCutoff=0):
    hits = 0
    total = 0

    # For each left-out rating
    for userID, leftOutMovieID, actualRating, estimatedRating, _ in leftOutPredictions:
        # Only look at ability to recommend things the users actually liked...
        if (actualRating >= ratingCutoff):
            # Is it in the predicted top 10 for this user?
            hit = False
            for movieID, predictedRating in topNPredicted[int(userID)]:
                if (int(leftOutMovieID) == movieID):
                    hit = True
                    break
            if (hit) :
                hits += 1

            total += 1

    # Compute overall precision
    return hits/total
```

### 3-5.rHR (rating Hit Rate, 평가 적중률)

적중률을 보는 또다른 방법-예상등급 점수로 분류함.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/a8c8f7eb-2f9f-46bf-9402-654952e3a77b)

  - **평점적중률(예상 등급 점수 분류)**
  - 실제로 적중된 추천 영화가 얼마나 좋은지에 대해, 쪼개어서 살펴보는 아이디어.

```
## RHR : 평가적중률
## 적중률기능과 같은방식. 각각의 평균값을 얻기 위해 적즁률을 추적함. 
## 적중과 전체 사용자 수를 추적하는 하나의 변수보다는 
## 적중과 각 평가 종류별 총계를 추적하는 별개의 사전을 사용. 
## 그리고 마지막에 결과를 도출.

def RatingHitRate(topNPredicted, leftOutPredictions):
    hits = defaultdict(float)
    total = defaultdict(float)

    # For each left-out rating
    for userID, leftOutMovieID, actualRating, estimatedRating, _ in leftOutPredictions:
        # Is it in the predicted top N for this user?
        hit = False
        for movieID, predictedRating in topNPredicted[int(userID)]:
            if (int(leftOutMovieID) == movieID):
                hit = True
                break
        if (hit) :
            hits[actualRating] += 1

        total[actualRating] += 1

    # Compute overall precision
    for rating in sorted(hits.keys()):
        print (rating, hits[rating] / total[rating])
```

## 4.정확성 외에 추천시스템의 예측 능력을 결정하는 요소들

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/f94fa481-042f-4e4e-9583-f3e8068be802)


### 4-1.coverage (적용 범위)

시스템이 예측할 수 있는 가능한 추천(사용자-항목 쌍)의 비율

  - ex) movielens의 영화 평점을 생각해보면 수천개에 달하는 영화에 평점이 달려있음.
    - 하지만 평점이 없는 영화들 역시 존재함. 
    - 그 데이터를 IMDB에 적용한다면 이 추천시스템의 적용범위는 낮다고 볼수 있음(적용대상인 IMDB의 영화 개수가 훨씬많기 때문)
  - 중요한건 적용범위의 정확성과 상충할 수 있다는 점.
  - 만약 추천 데이터의 품질을 높이고 싶다면 당신은 정확성을 높일것. 대신 적용범위는 낮아짐.추천하기에 적당한 균형점을 찾기가 쉽지는 않을수도 있음.
  - coverage가 중요한 이유는 **새로운 추천 데이터가 사용자에게 얼마나 빨리 나타나는지를 보여주는 요소이기 때문.**
    - ex) 아마존에 신간이 나왔다고 보면 사람들이 그 책을 사기 전까지는 그 책은 추천 목록에 뜨지 않을것. 
      - 그 책을 사야지만 동향 데이터가 생성되기 때문.
      - 동향 데이터가 생기기 전까지 해당 책은 아마존의 제공률을 떨어뜨리는 요소임.

```
## 여러 항목중에서도 최소 한가지의 좋은 추천 항목을 가진 사용자의 비율.
## 현실에서 추천데이터에 있는 항목보다 더 많은 항목이 있는 카탈로그가 있음.
## 그리고 카탈로그 항목으로 적용범위를 계산

# What percentage of users have at least one "good" recommendation
def UserCoverage(topNPredicted, numUsers, ratingThreshold=0):
    hits = 0
    for userID in topNPredicted.keys():
        hit = False
        for movieID, predictedRating in topNPredicted[userID]:
            if (predictedRating >= ratingThreshold):
                hit = True
                break
        if (hit):
            hits += 1

    return hits / numUsers
```

### 4-2.diversity (다양성)

추천시스템이 사람들에게 얼마나 광범위하고 다양한 항목을 제공하는지 측정

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/8472d393-e968-49f0-b17f-e1d1d9224f79)

  - S : avg similarity between recommendation pairs(유사성 평균값)
  - 항목간의 유사성 평균값을 1에서 뺀값 = 추천 시스템이 얼마나 다양한 범주의 데이터를 사용자들에게 제공하는가 를 의미함. 
  
  - ex ) 다양성이 낮다는 것은 예를 들면, 당신이 읽는 책의 다음 편을 추천하는 것과 같음. 
    - 다른 작가의 책이나 관련된 영화를 추천해주지는 않는것. 이는 주관적인 요소로 보일수 있지만, 사실 측정이 가능함.
  
  - 많은 추천 시스템들은 데이터 간 유사성을 계측해서 작동하기 때문에 거기서 계측한 값으로 다양성을 측정하는 것.
  -  S : 우선 추천목록에서 데이터끼리 번갈아 짝을 지어 유사성 값을 내면 그 값의 평균값이 추천목록 데이터의 유사성을 나타내는 측정값.
  - 다양성은 유사성의 반대 개념이기 때문에 1-S를 하여 다양성의 값을 도출할 수 있다.

  - point) 다양성은 추천 시스템 안에서는 좋은 요소가 아님!
  - 이유) 다양성이 높다는 것은 아무거나 추천을 한다는 뜻이기 때문.
  - 다양성 값이 높다는 것은 좋은 추천보다 좋지 못한 추천을 받았다는 의미임!

```
## 시스템의 우선 추천 순위뿐만 아니라 데이터 세트에 있는 항목별, 유사성 점수 metric도 필요.
## 유사성 metric을 보자. 
## 기본적으로 2x2배열이고  빠르게 찾을수 있는 항목들을 모두 짝지어서 도출한 유사성 점수를 포함. 
## 그다음 각 사용자별 우선 추천 항목들을 다룬다. 한번에 사용자 한명씩.
## itertools.combinations() 호출은 다시 우선순위 항목에서 모든 항목을 짝지어 준다. 
## 그럼 한 쌍씩 반복해서 각 쌍의 유사성을 찾을수 있다.
## surprise에는 각 사용자와 항목의 내부 ID들이 차례대로 나열되어 있고 
## 이 데이터들은  실제 평가 데이터에 있는 사용자나 영화의 ID와는 다른 데이터이다.
## 유사성 metric은 이런 내부 ID를 사용하기 때문에  
## ID들도 유사성 점수를 찾아보기 전에 내부 ID로 바꿔줘야 한다.
## 모든 유사성 점수를 합친 뒤 평균값을 내고 1에서 빼주면 다양성 metric값이 나온다.
## 데이터 세트에서 각 사용자별 추천 항목들의 모든 조합을 이용해서 이코드를 실행하는것은 꽤 복잡한 계산임.

def Diversity(topNPredicted, simsAlgo):
    n = 0
    total = 0
    simsMatrix = simsAlgo.compute_similarities()
    for userID in topNPredicted.keys():
        pairs = itertools.combinations(topNPredicted[userID], 2)
        for pair in pairs:
            movie1 = pair[0][0]
            movie2 = pair[1][0]
            innerID1 = simsAlgo.trainset.to_inner_iid(str(movie1))
            innerID2 = simsAlgo.trainset.to_inner_iid(str(movie2))
            similarity = simsMatrix[innerID1][innerID2]
            total += similarity
            n += 1

    S = total / n
    return (1-S)
```

### 4-3.novelty (참신성)
추천하는 상품이 얼마나 인기있는지 확인해주는 metric

  - 추천데이터가 얼마나 대중적인지 나타냄.
  - 참신성을 높이려면 아무거나 추천을 하면 안됨. 
    - 대부분의 데이터는 대중적인 데이터가 아니기 떄문. 
    - 참신성도 추측이 되지만 그 값의 해석은 주관적인 경우가 많기 때문.
  - 추천 시스템에는 사용자 신뢰라는 개념이 있음.

#### 4-3-1.long tail

아래 그래프의 x축은 제품, y축은 제품의 인기도를 나타냄.
음영 처리도니 부분은 "long tail"이라고 함.
항목의 가장 왼쪽에 있는 제품이 가장 오른쪽에 있는 항목에 비해 인기가 더 높다.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/32a26068-b5cd-4b28-b0cc-0e654bc46ea2)

  - 추천시스템의 핵심은 **long tail** 전략으로 데이터를 제공하는 것이기 때문.
    - 추천시스템은 "niche products" 부분의 제품들을 보고 자신들의 희귀한 수요에 부합한 추천이라고 생각하게 만든다. 
    - 이를 활용하면 시스템이 추천한 데이터들이 새로운 사용자들을 발견할 수도 있고 사용자들에게 다양한 기회를 줄 수도 있고 사용자들에게 다시 돌아갈 이익을 창출할것임.
  - 추천시스템에서 "novelty(참신성)"값을 잘 활용하면 더 좋은 방향으로 간다.
  - 즉, 참신성과 신뢰성사이에서 균형점을 찾는것이 중요한것!

```
## 각 항목의 인기 순위를 담은 사전을 설정값으로 두고 
## 그다음은 그냥 각 사용자의 우선 추천항목을 거친 뒤 
## 각 추천 항목들의 인기 순위의 평균값을 계산.

def Novelty(topNPredicted, rankings):
    n = 0
    total = 0
    for userID in topNPredicted.keys():
        for rating in topNPredicted[userID]:
            movieID = rating[0]
            rank = rankings[movieID]
            total += rank
            n += 1
    return total / n

```


### 4-4.churn (이탈)

사용자가 새 영화를 평가하면 권장사항이 크게 변경된다면 이탈점수가 높다는 뜻이다.

- 변화가 많다면 이탈점수가 높다는 뜻.
- 같은 사용자에게 계속 같은 추천을 하는것은 no good.
- 만약 사용자가 계속 뜨는 추천을 눌러보지 않는다면 추천시스템은 해당 항목 추천을 멈추고 다른항목을 추천해야 하는가?
- 다양성과 참신성과 같이 높은 이탈 값은 그다지 좋은게 아님.
- "이탈"점수를 높이기 위해서는 무작위로 항목들을 추천할 수도 있으며, 이는 좋은 추천시스템이 아니다.
- 이런 종류의 metric들은 모두 묶어서 고려할 필요가 있음. 서로 균형을 맞춰야 한다는점도 알아야 함.

### 4-5.responsiveness (민감성)
사용자의 새로운 행위가 추천 시스템에 얼마나 빠르게 영향을 주는지를 나타냄.

- ex) 신작영화를 평가한다면 그 행위가 다른 사람들의 추천 목록에 즉시 영향을 줄까? 아니면 하루 뒤 어떤 작업후에 추천 목록이 뜰까?
- 민감성 값이 높은 것이 좋다고 느낄수 있음. 
- 하지만 사업적인 측면에서는 추천시스템이 어느 정도 민감해야 하는지를 설정할 필요가 있음. 
  - 왜냐면 추천시스템이 즉각 반응하는 방식이라면 복잡해지고, 
  - 시스템 유지가 어렵고 시스템을 구성하는 비용이 커짐.
  - 시스템 고유의 균형값을 찾아야 함.민감성과 간소성사이에서!


## 4.online A/B test
- 가장 중요한 metric이다!
- 실제 고객에게 맞춰서 추천 시스템을 수정하고 고객들이 추천목록에 어떻게 반응하는지 측정하는 것.
- 여러 가지 알고리즘에서 나온 추천 목록들을 서로 다른 사용자 데이터 세트에 입력할수 있음. 
  - 그리고 여러분들이 추천항 항목들을 실제로 사는지, 훑어보는지 혹은 다른식으로 관심을 표현하는지 측정해보는것.
- 통제가 가능한 온라인 실험을 이용해서 추천시스템의 변화를 시험해보면 
- 추천시스템이 사람들에게 새로운 항목을 보여주고 더 많은 구매가 이루어지도록 하는지 관찰가능.

## 5.surrogate problem (대리문제)
- 평가를 정확하게 측정했는데도 추천이 좋지 못할수도 있음.
- 정확성이 꼭 좋은 추천 목록을 만들어낸다는 보장은 없음.
- 온라인 A/B test의 결과가 추천시스템에서 가장 중요한 평가 지표임. 
- 고객들이 추천항목에 얼마나 돈을 써서 품질을 평가했는지 측정해보는것.



---


[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}