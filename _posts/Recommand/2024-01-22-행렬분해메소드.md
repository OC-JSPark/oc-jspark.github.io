---
title: "Matrix Factorisation method"
escerpt: "Matrix Factorisation method"

categories:
  - Recommand
tags:
  - [AI, Recommand, Factorisation]

toc: true
toc_sticky: true

breadcrumbs: true

date: 2024-01-22
last_modified_at: 2024-01-22

comments: true
  

---

협업필터링은 현실의 대규모 상황에서 잘 작동하지만, 제한된 확장성을 이유로 비판받아왔다. 
매우 큰 집단의 항목이나 사용자에 대해 유사 행렬들을 연산하는 것은 많은 비용이 들기 때문이다.
그러나 item-based collaborative filtering을 사용하게 되면 해당 비용을 줄일수 있다. 
만약 위 방법이 안통한다면 아파치 스파크와 같은 기술들로 행렬의 구성을 클러스터에 분배할수도 있다.
그런데도 collaborative filtering의 문제점은 희소데이터에 민감하다는것. 즉, 꺠끗한 데이터에서만 좋은 결과를 얻을 수 있다.
추천시스템을 위한 다른 방법을 search해보자.



## 1. Model-based methods

평가 데이터로부터 예측치를 추출해내기 위해 서로 비슷한 항목이나 사용자를 찾으려 하는 것 대신 데이터사이언스와 머신러닝 기법을 적용하자.
머신러닝은 예측을 위한 모델을 훈련하는것이 끝! 그래서 추천하는것에 관한 문제를 같은방식으로 다룰것이다. 

  - 사용자 평가 데이터로 모델을 훈련하자.
  - 해당 모델을 새 항목의 평가를 예측하는데 사용하자.

## 2. matrix factorization(행렬 인수분해)
user/items의 더 넓은 특성을 관리해줌(ex.액션영화,로맨스 같은것을 스스로 찾아내서 관리)
  - ex) BOB = 80% Action + 20% Comedy 
    - BOB을 80% 액션, 20% 코메디가 섞인 영화와 짝지어서 추천시스템을 만들어야 한다.

## 3. PCA(pricipal component analysis)
- user-item matrix의 중요정보를 잃지 않는 차원에서 차원축소
  - 행 : 사용자
  - 열 : 항목
  - 예측을 통해 현재 모르는 셀들을 채우는것. 이것을 채우는 머신러닝 기법중 하나가 주성분분석(PCA)이다.

- ex) iris data
  - 특징 1) 바깥쪽에 큰 꽃잎 여러개가 있고 안쪽에 더 작은 꽃잎 여러개가 있는것을 알수 있다. 
  - 특징 2) 안쪽 꽃잎은 꽃받침이라 불린다.
  - 특징 3) 그래서 특정 한 붓꽃의 모양을 묘사하는 한가지 방법은 꽃잎의 길이와 너비, 그리고 꽃받침의 길이와 너비를 활용한것.


- eigenvectors are principal components

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/923335c0-f9cc-4897-8d44-9c09b29d90a8)

```
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA

iris = load_iris()
print(iris.data)
```

  - 총 4개의 데이터 차원들이 있다.
  - 꽃잎의 길이와 너비에 대해서만 생각해보자.
  - **eigenvector는 고유벡터라고 불린다.** 
  - 기본적으로 이것은 데이터의 분산과 그것에 직교하는 벡터에 대해 가장 잘 설명할수 있는 벡터이다.  
  - 그들은 데이터에 더 잘 들어맞는 새로운 벡터 공간 or 기반을 정의할 수 있다. 
  - 이 고유벡터(=eigenvector)들이 데이터의 주성분이다. 이것이 주성분 분석이라고 불리는 이유이다.
  - 우리가 찾으려는것은 우리의 데이터를 설명하는 이 주성분이고 그것은 이 고유벡터들로부터 주어진다. 
  ![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/067597f6-c9a1-4c9f-80ee-a2aa3204e0f4)

    - 고유벡터들로부터 데이터의 분산을 살펴볼수 있다. 
    - 고유벡터로부터의 거리는 하나의 숫자이고 하나의 차원이다. 그리고 이것은 우리가 꽃잎길이와 꽃잎 너비로부터 시작한 2차원의 데이터를 재구성하기에 꽤좋다. 
    - 즉, 해당 주성분을 구별해내는것이 적은 차원을 사용하여 데이터 표현 가능.
    - 고유벡터는 데이터에 내재한 흥미로운 특성을 찾아내는 방법을 갖고 있다. 
      1. iris data의 꽃잎의 전체적인 크기
      2. 그것이 어떤 종의 iris data인지 분류하는 데에 중요하다는 것
      3. 길이에 대한 너비의 비율이 다소 일정하다는 사실을 밝혀내고 있다. 
    - 그래서 이 고유벡터에 따른 거리는 기본적으로 꽃의 크기를 측정하는 것(math는 크기가 어떤 의미인지 모름)
    - 다만, 중요한 특성을 정의하는 벡터를 찾았으며, 그것은 "크기" 이다. 
    - 즉, **PCA = 특징추출도구**
    - PCA가 발견한 특징은 "잠재적 특징"
    - 4D 공간 안에서 두개의 차원들을 구별하고 그 두차원들을 도표화하기 위해 PCA를 사용함.
    - PCA는 가장 적은 정보를 포함하고 있는 차원버림을 선택가능.
    - 그래서 PCA로부터 가장 적게 정보를 포함하는 두차원을 버림으로써 4개의 차원에서 2개로 줄임.
    - 남은 두자원이 표현하는것이 무엇인지 알수없음. 또한, 해당 도표에서 x,y축이 의미하는것은 무엇인지 알수없다. 
    - 확실히 아는것은 PCA가 데이터에서 추출해낸 어떤 종류의 잠재적 요소나 특성을 의미한다는것! 
    



## 4.특이값분해(SVD, singular value decomposition)

- PCA를 진행한 movie rating
![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/8aa36eb4-a6be-47e7-848b-c70aa1233355)

  - 여기서 각차원은 영화이다.
  - iris data set에서 PCA 진행시 중요한 차원들만 남기고 차원 축소. 
  - 그 결과는 사람이 영화를 분류할 때 사용하는 특성들과 일치함.
    - ex) 영화의 action, SF, Romance 등으로 분류됨. 
    - 즉, 개인의 영화평점 결정요인이 무엇이든간에 PCA는 그 잠재적 특성들을 데이터에서 추출함.
    - PCA에서 3가지로 차원 축소요청하면 PCA는 평점데이터에서 3가지 잠재적 특성을 추출.
    - 그것들이 무엇을 의미하는지 PCA는 모르지만 임의로 action, SF, Classic 장르에 대한 개인의 선호도를 반영한다고 설정해보자.

  ![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/b230ce23-89b5-4b5e-af4d-0fe01b4b2e78)

    - R : 행에 유저를 배치한 평점 행렬
    - U : 각 열은 유저들의 선호를 설명해주는 잠재적 특성들로 구성된 선호도행렬
    - R^T : 전치행렬 = input data를 재조정해 영화를 행에 배치하고 유저를 열에 배치한 행렬 
      - 이렇게 하면, 유저 평점 데이터에서 PCA를 수행해서 주어진 유저들의 프로파일을 찾아내듯 역으로 PCA를 통해 주어진 영화들의 프로파일도 찾아낼수 있다. 
      - 전치행렬로 PCA를 진행해도 잠재적 특성들을 추출할수 있으며, 각각의 영화는 그 잠재적 특성들의 결합으로 설명 가능.
    - M : 주어진 영화들을 설명해주는 잠재적 특성으로 구성된 행렬
      - 이들은 내재적 의미가 없지만 결과론적으로 영화 장르가 될수 있음.
    - 그럼 주어진 유저들과 영화들을 설명하는 matrix factorization이 나온다. 

이제, 각각의 행렬로 어떻게 평점을 예측할까?

  ![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/27213409-281b-479f-a1a5-cd802c7723ce)
    
  - "주어진 영화들로 설명하는 행렬 + 주어진 유저들을 설명하는 행렬의 전치 행렬" 은 원래 평점 행렬의 인수 행렬들이다. 
  - 그러니 M과 U로 R을 재구성할수 있다. 
    - 만약 R 내에 몇몇 평점들이 빠져있다면 M과 U로 그 빈칸을 모두 채울수 있다. 
    - 그래서 이를 행렬 인수분해라고 한다.

  - 예측하려는 평점 행렬의 인수 행렬들로서 훈련데이터를 설명하는것.
  - summation(시그마) 행렬 : 대각행렬로써 주어진 값들을 적절한 스케일로 바꿔줄뿐이다.
    - 해당 스케일링 행렬을 M이나 U에 곱한 뒤 그 두행렬을 곱한게 R이라고 볼수 있다. 
  - 이들 인수 행렬들을 한데 곱함으로써 R을 다시 재구성할 수 있으며, 유저의 영화에 대한 평점을 구할수 있다. 
  
  - 특정 유저의 특정 영화에 대한 평점은 U에서 해당 유저에 할당된 행과 M^T에서 해당 영화에 할당된 열의 내적이 된다. (행렬의 곱이란게 그런거니깐!)


### 4-1. 특이값분해란?
- U,summation,M^T를 동시에 함께 효율적으로 연산해내는 방법. 
- 즉, SVD는 유저와 영화에 대해 PCA를 실행한 후, 필요로 하는 평점 행렬의 인수 행렬들을 구해줌. 
- SVD는 이 3개의 인수 행렬을 구해주는 방법일 뿐이다.

- 하지만 어떻게 U와 M^T를 구할까?
  - 원래 R 행렬의 cell들은 대부분 누락. 누락된 행렬은 PCA가 불가. 
  - 누락된 cell들은 채울수 있는방법은 평균값이나 디폴트값 등을 주면 된다.
  - 모든 평점은 U행렬의 행과 M^T행렬 열의 곱의로 설명된다는걸 기억해봐라. 
  - 예를들면 incredibles에 대한 bob의 평점을 예측하고 싶다면 ?


|-|indian jones|star wars|Empire|Incredibles|Casablanca|
|---|---|---|---|---|---|
|bob|4|5|?|?|?|
|Ted|?|?|?|?|1|
|Ann|?|5|5|5|?|

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/3701e600-d22d-4afd-865f-4ec46bbbfe77)

  - U-Bob 행과 M^T-incredible 열을 내적
  - ex) U의 행들과 M^T의 열들을 알면 이로부터 R의 몇몇 원소를 안다고 가정
    - 이를 최소화 문제로서 다룰수 있다. 
    - R에서 알려진 평점과의 오차를 최소화해주는 U의 행과 M^T의 열의 값을 찾아가는것. 
    - 해당 작업을 수행 가능한 다양한 머신 러닝 테크닉이 있다. (ex.SGD)

### 4-2.확률적 경사하강법(SGD, Stochastic Gradient Descent)
- 이는 주어진 학습률에서 반복을 계속하여 오차를 최소화하는 방법.
- 아파치 스파크의 경우는 **교대최소제곱법**을 활용한다.
  - 갑자기 M,U를 직접 찾는게 SVD라고 해놓고 간접적으로 찾는게 SGD, 교대최소제곱법등을 말하고 있으니 헷갈릴수도!!
  - 추천에 쓰이는 SVD는 실은 SVD가 아니다.
  - 데이터가 빠진 상태로는 SVD가 불가능하기 때문. 
  - 넷플릭스 프라이즈의 알고리즘은 SVD에 착안해서 만들어진것일뿐. 순수 SVD는 아니다.
  - 넷플릭스 프라이즈의 우승 알고리즘은 SVD의 변형중 하나인 SVD++과 제한적 볼츠만 머신이라는 테크닉을 결합한것이다.

**요약) 유저의 아이템에 대한 평점 행렬이 R일때, R은 인수행렬들로 인수분해될 수 있으며, 그렇게 구해진 유저를 설명하는 행렬과 아이템을 설명하는 행렬을 함께 곱할 수 있다. 해당 행렬들을 빠르게 구하는 방법이 특이값 분해인 SVD이며, 그렇게 인수 행렬들을 구하고 나면 특정 유저의 특정 아이템에 대한 평점은 각 행렬의 내적으로 구할수 있다. 활률적 경사하강법이나, 교대최소제곱법 같은 테크닉을 통해서 인수분해된 행렬의 누락된 데이터의 최적값을 추정할 수 있다.(Maxtrix Factorization > SVDBackeOff.py)**


## 5.SVD개선
  - a matrix factorization bestiary
    - SVD를 변형해 다른 용도로 쓰거나 SVD의 성능을 개선하기 위해서.
    - 최근 논문도 SVD의 변형을 다루고 있다.

    - Non-Negative matrix Factorization(NMF)
    - Probabilistic Matrix Factorization(PMF)
    - Probabilistic Latent ZSemantic Analysis(PLSA)
    - PureSVD
    - UV Decomposition
    - Weighted Regularized Matrix Factorization(WRMF)
    - SVD++
    - timeSVD++
    - HOSVD
    - CUR
    - Factorization Machines
      - 평점을 예측하거나 평점 시스템 내에서의 클릭을 예측하는데 용이함.
      - 그 근본 아이디어는 SVD와 같지만 좀더 범용성이 높다
      - SVD처럼 억지로 무리하지 않고도 희소한 데이터를 다룰수 있으니깐.
      - AWS의 세이즈메이커 서비스에 Factorization Machines가 내장되어 있기 때문.
      - 클라우드 내의 거대한 데이터 세트로도  쉽게 실험해볼수 있음.
      - 이것의 유일한 단점은 분류 데이터에 대해서만 적용된다는 점.
      - 이를활용하기 위해선 사전에 데이터를 맞게 다듬어야 한다. 
    - Factorized Personalized Markov Chains

- 일련의 이벤트들을 추천하는 데에 특화된 SVD변형도 있다.
- 최근 히스토리를 바탕으로 유저가 다음에 시청할 영상이나 클릭할 대상을 예측함.
- neighborhood-based methods에서는 translation-based 추천이 이걸 해준다고 했지만 model-based 섹션에서는 timeSVD++, Factorized Personalized Markov Chains 처럼 해당 문제를 해결하는 도구들이 있다. 콘텐츠에서 잠재 특성들을 추출하는데 쓸수 있음. 영화제목이나 설명글에 PLSA를 적용해서 이를 유저들과 매칭할수도 있음. 마치 PCA와 유사하게. 이러한 콘텐츠 기반 방법론들은 그 자체만으론 잘 먹히지 않지만 유저 행동 데이터로 만든 모델들과 결합하면 유용해질수 있음.

- 복잡한 알고리즘들을 다룰 때에는 알고리즘의 파라미터들을 튜닝함에 따라 결과가 대폭 향상되기도 함. SVD엔 그러한 파라미터가 몇개 있음.이를 다루는게 하이퍼파라미터 튜닝임. 머신러닝의 아주 중요한 주제임! 많은 알고리즘은 학습률과 같은 파라미터들에 민감하며 데이터 세트에 따라 파라미터 세팅도 달리하면 좋다. 예를들면 SVD에서 추출하고자 하는 잠재 요인들의 개수를 조정하거나 차원을 몇개로 좁힐지를 정함. 여기에 정답은 없음. 각 데이터의 성질에 달린 문제임. surpriseLib에서 SVD를 활용할때 이 값은 SVD모델 생성자 내부에 들어간다.n_factors란 이름의 파라미터로! 그 값은 마음대로 설정가능함. SVD단계에서의 학습률 또한 lr_all로 정할수 있으며 SGD가 수행할 에포크 횟수도 n_epochs파라미터로 정할수 있음. 대개 하이퍼파라미터 튜닝은 시행착오를 반복하며 최적을 찾아가는 과정이다. 일단은 디폴트 세팅에서 시작해서 최적의 값을 추측해나가는 것.값을 두배로 하거나 아님 1/2로 하거나 그렇게 추측해가는것이다. 결과물이 확연하게 개선되는 지점이 오지 않을때까지 좁혀보는것. 다행히도 surpriseLib의 GridSearchCV패키지가 하이퍼파라미터 튜닝을 도와준다. 여러분이 값을 변경해보길 원하는 여러가지 파라미터들을 정의해주면 가능한 모든 조합을 자동적으로 시도해본 다음 그중 무엇이 최고인지 알수있다. 코드를 잠깐 보자.
[tuning svd]
```
print("Searching for best parameters...")
param_grid = {'n_epochs':[20,30], 'lr_all':[0.005, 0.010],
              'n_factors':[50,100]}
gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)
gs.fit(evaluationData)

# best RMSE score
print("Best RMSE score attained: ", gs.best_score['rmse'])
params = gs.best_params['rmse']
SVDtuned = SVD(n_epochs = params['n_epochs'], lr_all = params['lr_all'], n_factors = params['n_factors'])
```
  - param_grid 딕셔너리를 보면 파라미터 이름들에다가 우리가 시도하려는 값들을 할당해놨음. 가능한 모든 조합에 대해 알고리즘을 돌려봐야 하니까 한번에 너무 많은 값을 시도하지 말것.
  - GridSearchCV로 테스트하려는 알고리즘과 시험해보려는 파라미터 딕셔너리를 설정하고 결과물을 판단할 척도도 설정해줌. 여기서는 RMSE,MAE 둘다 사용함.
  - 매번 몇번의 교차 검증을 할 건지도 정해줌.
  - 그 후 훈련 데이터로 GridSearchCV에서 fit을 실행하면 주어진 훈련 데이터에서 가장 잘 먹히는 파라미터 조합이 구해짐.
  - 작업이 끝나면 RMSE, MAE점수가 GridSearchCV의 best_score멤버가 될 것이며 최고로 판명된 파라미터 조합들이 best_params딕셔너리가 된다. best_params를 평가하는 척도는 뭐든 가능. RMSE, MAE도 된다.
  - 이제 최고의 파라미터 조합으로 새로운 SVD를 만들어 더욱 흥미로운 일들을 해볼수 있음.

  - exercise!!
  - SVD와 함께 사용할 최상의 하이퍼파라미터를 검색하도록 SVDBakeoff스크립트를 수정. 그런 다음 이를 사용하여 최고의 권장 사항을 생성하고 확인해보자.
  - SVDTuning.py파일보자.
    - tune the hyperparameters for SVD with the MovieLens dataset

## 6.SLIM(Sparse Linear Methods)
- 미네소타 대학에서 고안된것.
- SLIM: Sparse Linear Methods for Top-N Recommender Systems
- 추천시스템의 초기부터 선도적인 역할을 해오고 있는곳.
- 해당 논문에서 인상적인건 results부분이다.
  - 다양한 데이터 세트에서 SLIM이 일관되게 경쟁 알고리즘들보다 나은 결과를 냈다는 것. SVD++가 대조군에서 빠져있긴 하지만 다른 많은 알고리즘과 비교가 되어 있음. 또한 추천의 퀄리티를 적중률로 측정함.
  - SIM은 top-N 추천에 초점을 맞춘다는 것. 예측정확도가 아님! 즉, 초점을 올바르게 맞추고 있는것이다. 경쟁자들에 대한 SLIM의 압도적 우위는 넷플릭스 데이터뿐만 아니라 서점이나 야후 뮤직을 비롯해 신용 카드 구매 데이터와 몇몇 주문 소매 데이터에서도 드러난다. 1위를 차지하지 못한 유일한 데이터 세트는 movieLens인데 이것도 근소하게 2위이다.

- SLIM이 특정 유저의 특정 아이템에 대한 추천 점수를 생성하는 방식은 유저의 기본 평점들을 희소 종합한 다음 이를 희소 종합한 가중치로 곱해서 더하는 것. 이 가중치가 SLIM의 정수임!! 

- how SLIM works
![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/73f4dea6-42d1-40cc-aec3-a6ed1753d3c1)

  - 틸다 a ij는 특정 아이템에 대한 특정 유저의 점수로서 알아내고자 하는 값이다. 이는 해당 유저의 기존 평점들로 구성된 행 a iT에다가 그 행과 결부되어 미리 값이 구해진 가중치들을 곱한것과 같다. 유저는 일반 아이템만 평가했으며 가중치도 평가된 아이템에만 존재하기에 "희소"라는 표현이 쓰인것. 이를 유저 평점 행렬 전체로 확장하면 아래 그림처럼 된다.

  ![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/6aa6cc56-66e9-4a36-8215-8e5ad840b029)

    - 평점을 예측하기 위해서는 기존의 평점들을 마법의 가중치 행렬 W로 곱하면 된다. A와 W는 희소 행렬이며 이는 행렬의 데이터가 불완전하다는 뜻.  그러니 핵심은 W를 어떻게 계산하느냐임!
    - 너무 복잡해서 많은 사람들이 활용하기 어려워함. 그래서 SLIM의 채택률이 낮은이유일수 있음. W를 구하는 방법을 설명한 초반의 몇 문단이 논문에 있으니 참고할것.(Learning W for SLIM)
      - 이것의 핵심은 최적화 문제이다. 마치 경사 하강법을 통해 SVD의 행렬들을 알아냈던것처럼.

      - 확장형으로서 contextual SLIM, higher-order SLIM, HOSLIM등이 있음. 원래의 논문에도 fsSLIM이라는 feature-selection SLIM을 설명해두었음. 사용자가 원하는 것과 가장 유사한 열들만 사용하도록 SLIM을 제한한 버전이다. 이를 통해 적중률이 근소하게 올라감.

---


[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}