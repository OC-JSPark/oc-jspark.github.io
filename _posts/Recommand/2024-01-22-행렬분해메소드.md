---
title: "행렬 분해 메소드"
escerpt: "행렬 분해 메소드"

categories:
  - Recommand
tags:
  - [AI, Recommand, framework]

toc: true
toc_sticky: true

breadcrumbs: true

date: 2024-01-22
last_modified_at: 2024-01-22

comments: true
  

---

## 주성분분석(PCA)

- 협업필터링은 현실의 대규모 상황에서 잘작동함을 확인함.
- 그런데 왜 더 나은걸 찾아야 하는가?
- 협업 필터링은 제한된 확장성을 이유로 비판받아왔음.
- 매우 큰 집단의 항목이나 사용자에 대해 유사 행렬들을 연산하는 것이 많은 전산 마력을 차지할수 있기 때문.
- 그러나 항목 기반 협업 필터링을 사용하는 것은 그것의 복잡성을 상당 부분 감소시킨다. 한대의 기계를 가지고 극도로 큰 제품 목록들을 위한 유사 행렬 전체를 연산할 수 있는 정도까지!!
- 우리가 못한다면 아파치 스파크와 같은 기술들이 이 행렬의 구성을 클러스터에 분배할수 있게 해준다.
- 그런데도 협업 필터링에 대한 타당한 문제는 잡은 데이터와 희소 데이터에 민감하다는 것.  즉, 깨끗한 데이터에서만 좋은 결과를 얻을수있다.
- 추천을 하기 위한 다른 방법을 알아보자.

- Model-based methods
  - 모델기반 방식으로 꼬리표 하에 이것들을 한데 묶을 것인데 우리의 평가 데이터로부터 예측치를 추출해내기 위해 서로 비슷한 항목이나 사용자를 찾으려 하는 것 대신 데이터사이언스와 머신러닝 기법을 적용하자.
  - 머신러닝은 예측을 위한 모델을 훈련하는것이 끝. 그래서 추천하는것에 관한 문제를 같은방식으로 다룰것이다. 우리는 사용자 평가 데이터로 모델을 훈련하고 우리의 사용자에 의한 새 항목의 평가를 예측하는 데에 이 모델을 사용할것이다.
  - 이것은 surpriseLib의 기반을 둔 평가 예측 아키텍쳐의 공간에 직면하게 된다. 그리고 항상 가장 효율적인 접근은 아닐수 있다.
  - 이것은 추천시스템을 만들어 내기 위해 머신 러닝 알고리즘들이 재목적화하게 하는데 그들중 몇몇은 평점을 아주 잘 예측한다.
  - 그들이 좋은 순위 추천을 이루어낼수있는지는 알아봐야 한다.

- matrix factorization(행렬 인수분해)
  - 사용자와 항목에 대한 더 넓은 특성, 예를들어 액션영화나 로맨스 같은 것들을 그들 스스로 찾아낸다. 즉, 단지 데이터로부터 떨어져 나온 어떤 특성이든지 묘사하느 행렬로 설명된다. 일반적인 생각은 사용자와 영화를 다른 양을 가진 각각의 특성의 조합으로 설명하는것. 예를들어 아마도 bob은 80%는 액션영화 팬, 20%는 코미디 영화 팬으로 정의될것이다. 그러면 우리는 그를 80%의 액션과 20%의 코미디가 섞인 영화와 짝지어야 한다는것을 알수있다. 그것이 일반적이고 어떻게 작동하는지 보자.  

  - ex) 행은 사용자, 열은 항목이다. 우리가 도전할것은 예측을 통해 현재 모르는 셀들을 채우는것. 이것을 채우는 머신러닝 기법중 주성분분석(PCA)가 있다.
|-|indian jones|star wars|Empire|Incredibles|Casablanca|
|---|---|---|---|---|---|
|bob|4|5|?|?|?|
|Ted|?|?|?|?|1|
|Ann|?|5|5|5|?|

  - PCA(pricipal component analysis)
    - 차원축소문제로 설명됨.
    - 사용자가 평가했을 수 있는 모든 영화들과 같은 많은 차원에 존재하는 데이터를 영화를 정확하게 묘사할 수 있는 작은 집단의 차원들. 예를들어 그것의 장르와 같은 것에 넣기를 원한다.
    - ex) 붓꽃(iris) data
      - 바깥쪽에 큰 꽃잎 여러개가 있고 안쪽에 더 작은 꽃잎 몇개가 있는것을 알수 있다. 이 안쪽 꽃잎은 전혀 꽃잎이라고 불리지 않고 꽃받침이라고 불린다. 그래서 특정 한 붓꽃의 모양을 묘사하는 한가지 방법은 꽃잎의 길이와 너비, 그리고 꽃받침의 길이와 너비를 활용한것.

    - eigenvectors are principal components

    ```
    from sklearn.datasets import load_iris
    from sklearn.decomposition import PCA

    iris = load_iris()
    print(iris.data)
    ```
      - 확인해보면 총 4개의 데이터 차원들이 있다.
      - 꽃잎의 길이와 너비에 대해서만 생각해보자.
      - eigenvector는 고유벡터라고 불린다. 기본적으로 이것은 데이터의 분산과 그것에 직교하는 벡터에 대해 가장 잘 설명할수 있는 벡터이다.  그들은 데이터에 더 잘 들어맞는 새로운 벡터 공간 or 기반을 정의할 수 있다. 이 고유벡터들이 우리의 데이터의 주성분이다. 이것이 주성분 분석이라고 불리는 이유이다.
      - 우리가 찾으려는것은 우리의 데이터를 설명하는 이 주성분이고 그것은 이 고유벡터들로부터 주어진다. 왜 이 주성분들이 유용한지 생각해보자.
      ![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/067597f6-c9a1-4c9f-80ee-a2aa3204e0f4)

        - 가장먼저 우리는 이 고유벡터들로부터 데이터의 분산을 살펴볼수 있다. 고유벡터로부터의 거리는 하나의 숫자이고 하나의 차원이다. 그리고 이것은 우리가 꽃잎길이와 꽃잎 너비로부터 시작한 2차원의 데이터를 재구성하기에 꽤좋다. 그래서 어떻게 이 주성분을 구별해내는것이 우리가 시작할때 갖고 있던 것보다 적은 차원을 사용하여 데이터를 표현할 수 있게 한다는것을 보실수 있다.
        - 또한 이 고유벡터는 데이터에 내재한 흥미로운 특성을 찾아내는 방법을 갖고 있다. 이 경우에는 우리는 기본적으로 분꽃들의 꽃잎의 전체적인 크기와 그것이 어떤 종의 붓꽃인지 분류하는 데에 중요하다는 것과, 길이에 대한 너비의 비율이 다소 일정하다는 사실을 밝혀내고 있다. 그래서 이 고유벡터에 따른 거리는 기본적으로 꽃의 크기를 측정하는 것이다. 수학은 크기가 어떤 의미인지 모른다. 그러나 이것은 데이터를 묘사하는데에 있어 중요해 보이는 그것의 숨은 특성을 정의하는 벡터를 찾았고 이것은 우리가 크기라고 부르는 것과 상응하게 된다. 
        - 그래서 우리는 또한 PCA를 특징추출도구라고 생각할수 있다. 우리는 이것이 발견한 특징을 잠재적 특징이라고 한다. 그래서 우리의 완전한 4차원읜 붓꽃데이터 세트에 대한 CPA의 최종 결과가 어떻게 생겼는지 알수있다. 우리는 데이터를 가장 잘 표현하는 4D 공간 안에서 두개의 차원들을 구별하고 그 두차원들을 도표화하기 위해 PCA를 사용했다.
        - PCA 자체는 여러분이 갖고 시작한 차원들을 가능한 한 많이 돌려줄 것이지만 여러분은 가장 적은 양의 정보를 포함하고 있는 차원들을 버리기로 선택할수 있다. 그래서 PCA로부터 우리의 데이터에 대해 가장 적게 설명하는 두차원을 버림으로써 4개의 차원에서 2개로 내려갈수 있다. 이 두차원이 표현하는 것이 무엇인지 확실하게 알수 없다.해당 도표에서 x,y축이 의미하는것은 무엇인지 알수없다. 확실히 아는것은 PCA가 데이터에서 추출해낸 어떤 종류의 잠재적 요소나 특성을 의미한다는것! 이것은 의미가 있지만 우리가 데이터에 어떠한 라벨을 붙이려는 시도가 가능하도록 이것을 검사하는 것을 통해서만이다.


## 특이값분해(SVD)

- PCA를 진행한 movie rating

|-|indian jones|star wars|Empire|Incredibles|Casablanca|
|---|---|---|---|---|---|
|bob|4|5|5|4|4|
|Ted|3|3|3|5|1|
|Ann|4|5|5|5|2|

  - 여기서 각차원은 영화이다.
  - 행에 유저를 배치한 평점 행렬을 R이라 하자.
  - 붓꽃 데이터 세트에서 했던 것처럼 PCA는 데이터의 분산을 가장 잘 설명하는 중요한 차원들만으로 차원을 축소해준다. 그 결과는 종종 사람이 영화를 분류할 떄 사용하는 특성들과 일치하기도 하다. 예를들면 영화의 액션 요소나 로맨스요소, 코미디 요소 등. 개인의 영화 평점을 결정하는 요인이 무엇이 됐든 간에 PCA는 그 잠재적 특성들을 데이터에서 추출할 거다.
  - PCA에게 3가지로 차원을 축소해달라고 요청한다면 PCA는 평점 데이터들에서 3가지 잠재적 특성을 규명해낼것이다. 그것들이 무엇인지 PCA는 모르지만 일단 임의로 액션,SF,고전 장르에 대한 개인의 선호도를 반영한다고 해보자.

  - (PCA한거)
==>
|-|action|SF|Classic|
|---|---|---|---|
|bob|0.3|0.5|0.2
|Ted|0.1|0.1|0.8|
|Ann|0.3|0.6|0.1|
    
  -  각 열은 해당 특성을 구성하는 유저들의 선호도.
  - 이 행렬을 U라고 하자. U의 열은 주어진 유저들의 선호를 설명해주는 잠재 특성들이다. 유저 평점 데이터에서 PCA를 수행해서 주어진 유저들의 프로파일을 찾아내듯 역으로 PCA를 통해 주어진 영화들의 프로파일도 찾아낼수 있다. input data를 재조정해 영화를 행에 배치하고 유저를 열에 배치하면 R의 T승이 되면서 아래처럼 된다.(R^T)

|-|bob|Ted|Ann|
|---|---|---|---|
|indian jones|4|3|4|
|star wars|5|3|5|
|Empire|5|3|5|
|Incredibles|4|5|5|
|Casablanca|4|4|2|

  - 이를 원래 평점 행렬의 전치행렬이라고 한다. 이 행렬로 PCA를 해도 역시 잠재 특성들이 규명될것이며, 각각의 영화는 그 잠재 요인들의 결합으로 설명이 된다. 

  - (PCA한거)
==>
|-|action|SF|Classic|
|---|---|---|---|
|indian jones|0.6|0.3|0.1|
|star wars|0.4|0.6|0|
|Empire|0.4|0.6|0|
|Incredibles|0.8|0.2|0|
|Casablanca|0.2|0|0.8|
  - 이제 각 열은 주어진 영화들을 설명해주는 잠재 특성으로 구성됨.
  - 이들은 내재적 의미가 없지만 결과론적으론 영화 장르가 될지도 모른다.
  - 위에 행렬을 M이라고 하자.
  - 그럼 주어진 유저들과 영화들을 설명하는 matrix factorization이 나온다. 각각의 행렬로 어떻게 평점을 예측할까?

  ![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/27213409-281b-479f-a1a5-cd802c7723ce)
    - 주어진 영화들로 설명하는 행렬과 주어진 유저들을 설명하는 행렬의 전치 행렬은 원래 평점 행렬의 인수 행렬들이다. 그러니 M과 U로 R을 재구성할수 있다. 만약 R 내에 몇몇 평점들이 빠져있다면 M과 U로 그 빈칸을 모두 채울수 있다. 그래서 이를 행렬 인수분해라고 한다.
    - 예측하려는 평점 행렬의 인수 행렬들로서 훈련데이터를 설명하는것.
    - 가운데에는 summation(시그마) 행렬이 있다.  summation은 단지 대각행렬로써 주어진 값들을 적절한 스케일로 바꿔줄뿐이다. 해당 스케일링 행렬을 M이나 U에 곱한 뒤 그 두행렬을 곱한게 R이라고 볼수 있다. 
    - 이들 인수 행렬들을 한데 곱함으로써 R을 다시 재구성할 수 있으며, 유저의 영화에 대한 평점을 구할수 있다. 인수행렬들을 이렇게 구했다면 특정 유저의 특정 영화에 대한 평점은 U에서 해당 유저에 할당된 행과 M^T에서 해당 영화에 할당된 열의 내적이 된다. 행렬의 곱이란게 그런거니깐!
    - 강의에서는 SVD라는 내장 추천시스템을 썼는데  그 정확도는 아주 높다. SVD란 singular value decomposition 으로 특이값 분해란 뜻이다. 
    - 특이값분해란?
      - U,summation,M^T를 동시에 함께 효율적으로 연산해내는 방법이다. 그러니까 SVD는 유저와 영화에 대해 PCA를 실행한 후 우리가 필요로 하는 평점 행렬의 인수 행렬들을 구해준다는 것이다. SVD는 이 3개의 인수 행렬을 구해주는 방법일 뿐이다.

    - 하지만 어떻게 U와 M^T를 구할까?
      - 원래 R 행렬의 원소들이 대부분 누락됬었잖아.. 원소가 누락된 행렬로는 PCA가 불가하다. 누락된 원소가 없어야만 진행가능. 빠진값들을 채울수도 있다.적당히 합리적인 디폴트 값들로! 관련 평균값이라던가..
      - 모든 평점은 U행렬의 행과 M^T행렬 열의 곱의로 설명된다는걸 기억해봐라. 예를들면 incredibles에 대한 bob의 평점을 예측하고 싶다면 ?


|-|indian jones|star wars|Empire|Incredibles|Casablanca|
|---|---|---|---|---|---|
|bob|4|5|?|?|?|
|Ted|?|?|?|?|1|
|Ann|?|5|5|5|?|

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/3701e600-d22d-4afd-865f-4ec46bbbfe77)

  - U에서 밥의 행과 M^T에서 incredible 열을 내적하면 된다. 그러니 U의 행들과 M^T의 열들을 알면 이로부터 R의 몇몇 원소를 안다고 가정해보자. 이를 최소화 문제로서 다룰수 있다. R에서 알려진 평점과의 오차를 최소화해주는 U의 행과 M^T의 열의 값을 찾아가는것. 해당 작업을 수행 가능한 다양한 머신 러닝 테크닉이 있다. 확률적 경사하강법이 대표적인 예다(sgd, stochastic gradient descent)
    - 이는 주어진 학습률에서 반복을 계속하여 오차를 최소화하는 방법.
    - 아파치 스파크의 경우는 교대최소제곱법을 활용한다.
    - 갑자기 M,U를 직접 찾는게 SVD라고 해놓고 간접적으로 찾는게 SGD, 교대최소제곱법등을 말하고 있으니 헷갈릴수도!! ㅋㅋ 추천에 쓰이는 SVD는 실은 SVD가 아니다.데이터가 빠진 상태로는 SVD가 불가능하거든. 넷플릭스 프라이즈의 알고리즘은 SVD에 착안해서 만들어진것일뿐. 순수 SVD는 아니다.넷플릭스 프라이즈의 우승 알고리즘은 SVD의 변형중 하나인 SVD++과 제한적 볼츠만 머신이라는 테크닉을 결합한것이다.

  - 요약) 유저의 아이템에 대한 평점 행렬이 R일때, R은 인수행렬들로 인수분해될 수 있으며, 그렇게 구해진 유저를 설명하는 행렬과 아이템을 설명하는 행렬을 함께 곱할 수 있다. 해당 행렬들을 빠르게 구하는 방법이 특이값 분해인 SVD이며, 그렇게 인수 행렬들을 구하고 나면 특정 유저의 특정 아이템에 대한 평점은 각 행렬의 내적으로 구할수 있다. 활률적 경사하강법이나, 교대최소제곱법 같은 테크닉을 통해서 인수분해된 행렬의 누락된 데이터의 최적값을 추정할 수 있다. 
    (Maxtrix Factorization > SVDBackeOff.py)


## SVD개선
  - a matrix factorization bestiary
    - SVD를 변형해 다른 용도로 쓰거나 SVD의 성능을 개선하기 위해서.
    - 최근 논문도 SVD의 변형을 다루고 있다.

    - Non-Negative matrix Factorization(NMF)
    - Probabilistic Matrix Factorization(PMF)
    - Probabilistic Latent ZSemantic Analysis(PLSA)
    - PureSVD
    - UV Decomposition
    - Weighted Regularized Matrix Factorization(WRMF)
    - SVD++
    - timeSVD++
    - HOSVD
    - CUR
    - Factorization Machines
      - 평점을 예측하거나 평점 시스템 내에서의 클릭을 예측하는데 용이함.
      - 그 근본 아이디어는 SVD와 같지만 좀더 범용성이 높다
      - SVD처럼 억지로 무리하지 않고도 희소한 데이터를 다룰수 있으니깐.
      - AWS의 세이즈메이커 서비스에 Factorization Machines가 내장되어 있기 때문.
      - 클라우드 내의 거대한 데이터 세트로도  쉽게 실험해볼수 있음.
      - 이것의 유일한 단점은 분류 데이터에 대해서만 적용된다는 점.
      - 이를활용하기 위해선 사전에 데이터를 맞게 다듬어야 한다. 
    - Factorized Personalized Markov Chains

- 일련의 이벤트들을 추천하는 데에 특화된 SVD변형도 있다.
- 최근 히스토리를 바탕으로 유저가 다음에 시청할 영상이나 클릭할 대상을 예측함.
- neighborhood-based methods에서는 translation-based 추천이 이걸 해준다고 했지만 model-based 섹션에서는 timeSVD++, Factorized Personalized Markov Chains 처럼 해당 문제를 해결하는 도구들이 있다. 콘텐츠에서 잠재 특성들을 추출하는데 쓸수 있음. 영화제목이나 설명글에 PLSA를 적용해서 이를 유저들과 매칭할수도 있음. 마치 PCA와 유사하게. 이러한 콘텐츠 기반 방법론들은 그 자체만으론 잘 먹히지 않지만 유저 행동 데이터로 만든 모델들과 결합하면 유용해질수 있음.

- 복잡한 알고리즘들을 다룰 때에는 알고리즘의 파라미터들을 튜닝함에 따라 결과가 대폭 향상되기도 함. SVD엔 그러한 파라미터가 몇개 있음.이를 다루는게 하이퍼파라미터 튜닝임. 머신러닝의 아주 중요한 주제임! 많은 알고리즘은 학습률과 같은 파라미터들에 민감하며 데이터 세트에 따라 파라미터 세팅도 달리하면 좋다. 예를들면 SVD에서 추출하고자 하는 잠재 요인들의 개수를 조정하거나 차원을 몇개로 좁힐지를 정함. 여기에 정답은 없음. 각 데이터의 성질에 달린 문제임. surpriseLib에서 SVD를 활용할때 이 값은 SVD모델 생성자 내부에 들어간다.n_factors란 이름의 파라미터로! 그 값은 마음대로 설정가능함. SVD단계에서의 학습률 또한 lr_all로 정할수 있으며 SGD가 수행할 에포크 횟수도 n_epochs파라미터로 정할수 있음. 대개 하이퍼파라미터 튜닝은 시행착오를 반복하며 최적을 찾아가는 과정이다. 일단은 디폴트 세팅에서 시작해서 최적의 값을 추측해나가는 것.값을 두배로 하거나 아님 1/2로 하거나 그렇게 추측해가는것이다. 결과물이 확연하게 개선되는 지점이 오지 않을때까지 좁혀보는것. 다행히도 surpriseLib의 GridSearchCV패키지가 하이퍼파라미터 튜닝을 도와준다. 여러분이 값을 변경해보길 원하는 여러가지 파라미터들을 정의해주면 가능한 모든 조합을 자동적으로 시도해본 다음 그중 무엇이 최고인지 알수있다. 코드를 잠깐 보자.
[tuning svd]
```
print("Searching for best parameters...")
param_grid = {'n_epochs':[20,30], 'lr_all':[0.005, 0.010],
              'n_factors':[50,100]}
gs = GridSearchCV(SVD, param_grid, measures=['rmse', 'mae'], cv=3)
gs.fit(evaluationData)

# best RMSE score
print("Best RMSE score attained: ", gs.best_score['rmse'])
params = gs.best_params['rmse']
SVDtuned = SVD(n_epochs = params['n_epochs'], lr_all = params['lr_all'], n_factors = params['n_factors'])
```
  - param_grid 딕셔너리를 보면 파라미터 이름들에다가 우리가 시도하려는 값들을 할당해놨음. 가능한 모든 조합에 대해 알고리즘을 돌려봐야 하니까 한번에 너무 많은 값을 시도하지 말것.
  - GridSearchCV로 테스트하려는 알고리즘과 시험해보려는 파라미터 딕셔너리를 설정하고 결과물을 판단할 척도도 설정해줌. 여기서는 RMSE,MAE 둘다 사용함.
  - 매번 몇번의 교차 검증을 할 건지도 정해줌.
  - 그 후 훈련 데이터로 GridSearchCV에서 fit을 실행하면 주어진 훈련 데이터에서 가장 잘 먹히는 파라미터 조합이 구해짐.
  - 작업이 끝나면 RMSE, MAE점수가 GridSearchCV의 best_score멤버가 될 것이며 최고로 판명된 파라미터 조합들이 best_params딕셔너리가 된다. best_params를 평가하는 척도는 뭐든 가능. RMSE, MAE도 된다.
  - 이제 최고의 파라미터 조합으로 새로운 SVD를 만들어 더욱 흥미로운 일들을 해볼수 있음.

  - exercise!!
  - SVD와 함께 사용할 최상의 하이퍼파라미터를 검색하도록 SVDBakeoff스크립트를 수정. 그런 다음 이를 사용하여 최고의 권장 사항을 생성하고 확인해보자.
  - SVDTuning.py파일보자.
    - tune the hyperparameters for SVD with the MovieLens dataset

## SLIM(Sparse Linear Methods)
- 미네소타 대학에서 고안된것.
- SLIM: Sparse Linear Methods for Top-N Recommender Systems
- 추천시스템의 초기부터 선도적인 역할을 해오고 있는곳.
- 해당 논문에서 인상적인건 results부분이다.
  - 다양한 데이터 세트에서 SLIM이 일관되게 경쟁 알고리즘들보다 나은 결과를 냈다는 것. SVD++가 대조군에서 빠져있긴 하지만 다른 많은 알고리즘과 비교가 되어 있음. 또한 추천의 퀄리티를 적중률로 측정함.
  - SIM은 top-N 추천에 초점을 맞춘다는 것. 예측정확도가 아님! 즉, 초점을 올바르게 맞추고 있는것이다. 경쟁자들에 대한 SLIM의 압도적 우위는 넷플릭스 데이터뿐만 아니라 서점이나 야후 뮤직을 비롯해 신용 카드 구매 데이터와 몇몇 주문 소매 데이터에서도 드러난다. 1위를 차지하지 못한 유일한 데이터 세트는 movieLens인데 이것도 근소하게 2위이다.

- SLIM이 특정 유저의 특정 아이템에 대한 추천 점수를 생성하는 방식은 유저의 기본 평점들을 희소 종합한 다음 이를 희소 종합한 가중치로 곱해서 더하는 것. 이 가중치가 SLIM의 정수임!! 

- how SLIM works
![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/73f4dea6-42d1-40cc-aec3-a6ed1753d3c1)

  - 틸다 a ij는 특정 아이템에 대한 특정 유저의 점수로서 알아내고자 하는 값이다. 이는 해당 유저의 기존 평점들로 구성된 행 a iT에다가 그 행과 결부되어 미리 값이 구해진 가중치들을 곱한것과 같다. 유저는 일반 아이템만 평가했으며 가중치도 평가된 아이템에만 존재하기에 "희소"라는 표현이 쓰인것. 이를 유저 평점 행렬 전체로 확장하면 아래 그림처럼 된다.

  ![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/6aa6cc56-66e9-4a36-8215-8e5ad840b029)

    - 평점을 예측하기 위해서는 기존의 평점들을 마법의 가중치 행렬 W로 곱하면 된다. A와 W는 희소 행렬이며 이는 행렬의 데이터가 불완전하다는 뜻.  그러니 핵심은 W를 어떻게 계산하느냐임!
    - 너무 복잡해서 많은 사람들이 활용하기 어려워함. 그래서 SLIM의 채택률이 낮은이유일수 있음. W를 구하는 방법을 설명한 초반의 몇 문단이 논문에 있으니 참고할것.(Learning W for SLIM)
      - 이것의 핵심은 최적화 문제이다. 마치 경사 하강법을 통해 SVD의 행렬들을 알아냈던것처럼.

      - 확장형으로서 contextual SLIM, higher-order SLIM, HOSLIM등이 있음. 원래의 논문에도 fsSLIM이라는 feature-selection SLIM을 설명해두었음. 사용자가 원하는 것과 가장 유사한 열들만 사용하도록 SLIM을 제한한 버전이다. 이를 통해 적중률이 근소하게 올라감.

---


[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}