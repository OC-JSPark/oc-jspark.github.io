---
title: "자연어 corpus수집"
escerpt: "자연어 corpus수집"

categories:
  - NLP
tags:
  - [AI, NLP]

toc: true
toc_sticky: true

breadcrumbs: true

date: 2024-09-05
last_modified_at: 2024-09-05

comments: true
 

---



# 1. 한국어 데이터 전처리

## 1-1. 자연어 Corpus수집

- Corpus란?
    
    : 말뭉치, **대량의 텍스트 데이터**

- 수집과정

    1. corpus 유형 결정

        1-1. 풀고자 하는 문제정의

        - **어떤 문제를 풀 것인지**

        - **어떻게 풀 것인지**

        1-2. 솔루션 설정

        - 학습을 어떻게 시킬지?

        - 모델구조는 어떻게 할지?
        
        - 평가는 어떻게 할지?

        1-3. 모델 학습위한 corpus 유형 결정

        - 언어의 종류 결정

        - corpus의 종류(어떤작업을 수행할지)

            - ex)번역작업 : source와 원본이 paried되어 있는지,
        
            - label있는 데이터라면, 해당 데이터를 어떻게 수집할수 있는지 결정필요
        
        1-4. corpus의 규모 결정

        - 얼마나 모을것인지?

        - 자연어처리 작업의 복잡도 or 학습에 사용할 언어 모델의 크기와 관련 있음.


        - 언어모델의 크기와도 관련이 있음.

            - 모델이 클수록 데이터 많이 수집해야함.

            - 안그러면 **overfitting** 되기 쉬움.
                - overfitting? 모델이 학습 데이터셋에 드러난 일부 패턴들을 너무 잘 학습해서, 일반화 성능이 오히려 낮아지는 현상을 말함.
        
        - 대규모 언어 모델의 사전 학습에 관련된 논문 (모델 크기와 corpus규모에 대한 관계를 유추가능)

        : 모델 크기와 모델을 saturation할수 있는 데이터에 대한 인사이트를 제공.
        
        - [(paper)Training Compute-Optimal Large Language Models](https://arxiv.org/abs/2203.15556)

            - 모델크기가 2배 늘어날때마다 최소 token도 2배 필요
            - 175 billion parmater를 갖는 model은 3.7 Trillion Token이 있어야 효과적으로 학습이 가능하다는 주장.
            - 성능지표 : MMLU benchmark

    2. 자료탐색 및 corpus수집

        2-1. 외부 데이터
        - [KLUE 벤치마크 데이터셋](https://klue-benchmark.com/tasks)
        - [GLUE 벤치마크 데이터셋](https://gluebenchmark.com/)

        2-2. 자료수집시 자료의 출처, 저작권 등 공지 등을 고려해야함.

        - [(paper)Improving Neural Machine Translation Models with Monolingual Data](https://arxiv.org/abs/1511.06709)
            - [자세한 내용 설명 사이트](https://lokalise.com/blog/back-translation-best-practices/)
            - back Translation기법(역번역) : 원본문장을 source문장으로 번역하는 작업에서 원본문장에 비해 source문장이 많을때,
            source문장을 원본문장으로 역번역해서 pair data를 증강시키는 방법
        
        - 자동생성 corpus : 레이블링 일부는 모델에게 맡기는 방식

            - 모델을 사용하여 사용자들의 피드백의 긍/부정 여부를 자동으로 판단하고, 그 결과를 학습에 활용하는 법

            - [(paper)When Life Gives You Lemons, Make Cherryade: Converting Feedback from Bad Responses into Good Labels](https://arxiv.org/abs/2210.15893)

# 2. tokenization, Cleaning, Normalizing

- 토큰화?

    ![image](https://github.com/user-attachments/assets/5ad2e97b-f757-478d-b81e-cd4e5178d46e)

    : 최소 의미 단위인 token단위로 나누는 tokenization.

- 정제(Cleaning)?

    : Corpus로부터 noise data 제거한것

- 정규화(Normalization)?

    : 표현방법이 다른 단어들을 통합시켜서 같은 단어로 만듬.

## 2-1. Tokenization
## 2-2. BFE Algorithm(서브워드 토큰화)
## 2-3. Cleaning & Normalization

- 토큰화 전 : 토큰화 작업에 방해가 되는 특성들을 검사하고, 해당 특성에 해당하는 데이터들을 필터링

- 토큰화 후 : 자연어처리 작업을 이해하는데 방해가 되는 특성(noise)들을 검사하고, 해당 특성에 해당하는 데이터들을 필터링

1. 불용어처리(stopword)

    - NLTK library 활용해서 불용어 처리가능
    - 영어 불용어가 179개 정도 있음.

        ```
        import nltk
        nltk.download('punkt')
        nltk.download('stopwrods')

        from nltk.corpus import stopwords

        english_stopwords = stopwords.words('english')

        print(f"전체 불용어 개수 : {len(english_stopwords)}")
        print()
        ```
2. 불필요한 태그 및 특수 문자 제거

3. Corpus내 등장 빈도가 적은 단어 제거

## 2-4. 한국어형태소 분석(used KoNLPy, Mecab)



---


[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}