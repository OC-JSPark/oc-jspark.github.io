---
title: "자연어처리 Task"
escerpt: "자연어처리 Task"

categories:
  - NLP
tags:
  - [AI, NLP]

toc: true
toc_sticky: true

breadcrumbs: true

date: 2024-09-04
last_modified_at: 2024-09-04

comments: true
 

---

# 1. 자연어처리 task 
## 1-1. 자연어처리 workflow

- 한국어 데이터 자연어처리task에 대해서 
- 데이터 수집 > 전처리 > 구축 > 모델학습 > 평가 


- NLU : natural language understand 자연어 이해
- NLG : natural language generation 자연어 생성


## 1-2. 자연어처리 task 분류

- [33 important NLP tasks explained](https://medium.com/nlplanet/two-minutes-nlp-33-important-nlp-tasks-explained-31e2caad2b1b)

    ![image](https://github.com/user-attachments/assets/7e4c1fbc-2e40-43e3-a22d-be25f192746d)

    - Text Classification : 문서단위의 텍스트에 사전 정의된 카테고리를 할당하는 작업

        ![image](https://github.com/user-attachments/assets/478dac8c-3133-4340-a133-379f377408a9)

        1. Sentiment Analysis : 텍스트의 감정 분류
        2. Abusing Detection : 텍스트의 어뷰징 여부를 판별

    - Information Retrieval and Document Ranking 
    : 두 문서나 문장 간 유사도를 결정하는 작업(Text Similarity Task)에서 시작

        ![image](https://github.com/user-attachments/assets/af4d8bf3-6f40-434b-bed2-d90e9484dd90)

        - Text Similarity Task : 두 문장이 얼마나 유사한지를 결정하는 작업으로 0과1 사이의 숫자로 유사도를 결정

            - Query text와 document DB 내 모든 문서 쌍에 대해 유사도를 계산한 후, 가장 높은 유사도를 가지는 문서를 retrieval하는 방식으로 작업을 구성.
    
    - Text-to-Text Generation : 텍스트를 입력으로 받아 목표를 달성하는 텍스트를 생성하는 작업.
        1. Machine Translation Task (기계번역 작업)
            : 소스 언어의 텍스트를 의미를 유지한 채 타겟 언어의 텍스트로 번역하는 작업

        2. Text Summarization Task
            : 여러 문서들의 의미를 유지한 채 더 짧은 버전의 텍스트로 요약하는 작업
            
            ![image](https://github.com/user-attachments/assets/48001ced-08a6-47b0-b419-52c3d1bb1799)

            2-1. Extractive summarization (추출요약)

            2-2. Abstractive summarization (추상요약)

        3. Text Generation
            : 보다 포괄적인 관점에서 사람이 작성한 것 같은 텍스트를 생성하는 작업

    - Knowledge bases, entities and relations 
        : 지식 기반 혹은 의미론적인 엔티티나 관계를 파악하는 자연어처리 분야

        1. Named Entity Recognition (NER) Task
            : 의미론적인 2개의 entity(개체를 의미) 간 관계를 식별하는 작업
        
            ![image](https://github.com/user-attachments/assets/0a6612d7-9e50-4fb5-899e-6b4b16fcfe56)

        2. Relation Extraction Task 
            : 텍스트에서 의미론적인 관계를 추출하는 작업

            ![image](https://github.com/user-attachments/assets/88c4b9b3-ff13-47d3-b8ac-6335dcae3f7b)

    - Topics and Keywords
        : 주제나 키워드를 파악하는 자연어처리 분야
        
        1. keyword extraction task 
            : 추출(내용에 있는것만 뽑음)

            ![image](https://github.com/user-attachments/assets/ee298156-3990-4110-8679-d3f72225f76f)

        2. topic modeling task : 추상화하여 주체 뽑기

            ![image](https://github.com/user-attachments/assets/ffc51104-cd8e-4bbf-885d-e7e91ffd2ddf)

            - 그래서 topic이 더 난이도가 높은 task이다.

    - Chatbots
        : 작업의 목표에 따라서 나뉨 / "주어진 context에 대해서 다음에 올 문장이 무엇이 적절할까"를 판단하는 역할!

        1. task oriented chatbot 
            : 사용자의 QA처리, 배달주문, 쇼핑몰의 CS처리 등의 특정한 목적이 있는것. 
            
            - ex) intent detection(의도감지), slot Filling

            - 정해진 규칙을 처리하기 위해서는 규칙에 도달하기 위한 정보들이 필요.
            
            - 이럭서들을 채워주는게 slot Filling이란 기법

            - 텍스트에서 주어진 entity에 대해서 특정 유형 혹은 값을 추출하는것.
                - ex) 내가 공연일정을 예약하고 싶다면 언제, 어떤가수꺼 이러한 entity를 추출해줘야한다.
            - 또한, 목적에 따라 사용자의 쿼리 의도를 잘파악해야 하므로, intent detection이란기법도 활용됨. 
                - 이것은 사용자의 메시지의 뒤에 있는 의미를 파악하는것.

        2. relation oriented chatbot 
            : 특수한 목적없이 관계형 챗봇. 
            
            - ex)일상챗봇인 이루다 같은것. 일상대화는 대화가 목적없이 그냥 대화가 된다. 
            - 여기서는 일상대화가 자연스럽게 흘러갈수 있도록 자연어를 매니지먼트를 하거나, 컨텍스트에 해당하는 답변들을 실제로 잘생성하는 Task generation이 필요하다. 즉. 흐름이 자연스럽게 흘러가야한다.

## 1-3. 데이터 수집 및 전처리

- 데이터 수집
    - [Public or Benchmark Dataset](https://paperswithcode.com/datasets?mod=texts&task=question-answering)

- 데이터 전처리
    1. Tokenization 

        : 주어진 데이터셋에서 문장이나 문서들을 token이라 불리는 단위로 나누는 작업

    2. cleaning

        : 노이즈데이터(이상치, 편향 등)를 제거하는 작업

    3. Normalization

        : 표현 방법이 다른 데이터들을 통합시켜서 같은 항목으로 합침.


## 1-4. 모델링, 모델학습 및 평가

- 모델링의 핵심은?

    :  어떤 언어모델이 내가 풀고자 하는 문제에 가장 적합한지 탐색하는것.

- 언어모델(Language Model)

    : 문장 혹은 단어에 확률을 할당하여 컴퓨터가 처리할 수 있도록 하는 모델.

    - 단어 시퀀스 자체의 확률은 모든 단어(w1, w2, w3..,wn)의 예측이 완료되었을때, 즉 **문장이 완성되었을때** 알 수 있음.
        - ex)  언어 모델이 생성한 여러 개의 문장 중 가장 확률 높은 문장을 return해준다.

- 모델 평가

    1. 시점에 따라

        1-1. 학습도중
        
        : 데이터셋을 Train / Val / Test 로 나누고, 이중 Valid dataset을 사용하여 학습 중간에 체크 가능.  Valid로 평가시 가장 높은것을 선정하고, 이를 가지고 test진행.

        1-2. 학습종료 후

        : Test dataset을 사용하여 검증.

    2. 방법에 따라
        
        2-1. 정량평가(Auto-Evaluation)

        : 정량적인 수치로 평가하는 방법

        2-2. 정성평가(Human evaluation)

        : 명확한 기준설정이 어렵거나, 확실한 정답을 결정할 수 없을 확률이 높은 경우에 사용.



## 1-5. 자연어처리 프레임워크
 
: dependency에 따른 문제점이 있어서 가상화하여 쉽게하면 좋다.

|-|tensorflow|pytorch|keras|
|:---:|:---:|:---:|:---:|
|설명|Data Flow Graph구조, 학습과정에서 필요한 수학 계산식과 데이터의 흐름을 node와 edge를 사용한 방향성 그래프로 표현|파이썬 기반 오픈소스 머신러닝 라이브러리로 Torch라는 머신 러닝 라이브러리에 바탕을 두고 만들어짐| 앞의것들보다 단순화된 인터페이스 제공, sequence model로 원하는 레이어를 쉽게 순차적으로 쌓을수 있음|
|장점|최적화good, 대규모 모델 학습이나 추론에 뛰어남|쉽고, 빠른구현 가능, ref가 많음, 그래프를 만들면서 값을 할당할 수 있는 define by run방식으로 깔끔한 코드 작성 가능| 각각의 모듈이 독립성을 갖기 때문에 새로운 모델을 만들 시, 모듈을 조합해 쉽게 새로운 모델 생성가능|
|단점|추상화수준이 높으나, 입문자 사용 어려움|최적화 관점에서 tensorflow보다 디테일한 모델링이 불가능함|모듈화의 한계로 확장성이 낮고, 오류발생시 keras문제인지 백엔드 언어의 문제인지 특정하기 어려움|

### 1-5-1. 자연어처리 라이브러리

1. Scikit-learn
    - 파이썬 머신러닝 라이브러리
    - 분석(분류, 회귀, 클러스터링)에 대해서 적절한 모델 선택 가능

        ```
        # Bag of Word기반 문서-단어 행렬을 활용한 문장 간 유사도 측정
        from sklearn.feature_extraction.text import CountVectorizer
        bow_vectorizer = CountVectorizer()
        bow_vectorizer.fit(augmented_training_documents)
        CountVectorizer()
        ```

    - 머신러닝 연습하기 위한 iris data, 당뇨병 데이터 등 자체 데이터 제공

        ```
        from sklearn.datasets import fetch_20newsgroups
        dataset = fetch_20newsgroups(shuffle=True, random_state=1, remove=('headers', 'footers', 'quotes'))
        dataset = dataset.data
        ```

2. NLTK(Natural Language Toolkit)
    - 여러 자연어처리 기능을 제공하는 파이썬 라이브러리
    - 각각의 기능이 모듈형식으로 제공됨
        - ex)   
            1. 언어 전처리 모듈
                - tokenization
                - stemming
            2. 분석모듈
                - sentiment analysis
                - tagging
            3. 고차원적인 추론 모듈
                - semantic reasoning

3. KoNLPy
    - 한국어 자연어 처리를 위한 파이썬 패키지
    - 한국어 형태소 단위에 대한 토크나이징이 필요할 떄 사용가능.

        ```
        from konlpy.tag import Kkma, Komoran, Okt, Mecab
        mecab = Mecab()
        okt = Okt()
        kkm = Kkma()
        komoran = Komoran()

        sentence = "가나다라마바사"
        print(f"mecab: {mecab.morphs(sentence)}")
        print(f"okt: {okt.morphs(sentence)}")
        print(f"kkm: {kkm.morphs(sentence)}")
        print(f"komoran: {komoran.morphs(sentence)}")
        ```

---


[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}