---
title: "Computer Vision 고전 이론 1"
escerpt: "Computer Vision 고전 이론 1"

categories:
  - Vision
tags:
  - [AI, Vision]

toc: true
toc_sticky: true

breadcrumbs: true

date: 2023-10-07
last_modified_at: 2023-10-08

comments: true
 

---

# 1. Local Image Features
: 이미지에 대해서 feature를 어떻게 뽑을것인지, feature engineering이나 어떻게 학습할것인지

- image에서 ROI(Region of interest, 흥미로운부분)부분을 말한다.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/cf5a6252-5c7e-40f9-971c-931499e7a607)

  - local feature는 image representation을 만드는데 사용된다.
  - local feature들의 aggeregation(집합) descriptor이다.

  - image representation은 object appearance modeling이 가능하다.
    - local feature를 사용하면 object가 조금 가려져 있더라도 일부분만 가지고도 전체를 찾아낼수도 있다.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/cad7a288-746c-497d-97b0-797580ae8eff)

  - local feature들은 여러장의 multiple view사이에서 match를 찾아낼수도 있다  

## 1-1. 좋은 local feature란?

- 1) saliency : feature는 image에서 ROI part를 잡고있어야 한다.
- 2) locality : feature는 image보다 small area를 가지고 있어야 한다.
- 3) Repeatability : 이미지가 회전하거나 변하더라도 같은 feature라면 같은 descriptor를 만들어내야 한다. photometric(조도변화)에도 일관성있게 뽑혀야 한다.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/7b039d52-8f3d-4646-b472-76790a57e6af)

  - 즉 애매하지 않은게 뽑히는게 좋은 local feature이다.
  - **좋은 local feature = Interest point = keypoint**
    - consistent(일관된) reproducible(재생할 수 있는)
    - rich information, well-defined position
    - blob : 이미지 내에서 주변보다 더 밝거나 어두운 영역

# 2. Convolution
: 여기서 image processing이 어떻게 쓰였는지

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/77beb7d1-44c6-45e7-9bef-a532befc2ead)

  - convolvution란?
  
  : 미리 정해져있는 convolution kernel이 image에 각 pixel들과 곱해지면서 output of convolution을 만들어내는 연산이다.
  
  - 즉, convolution은 matrix multiplication 후에 summation을 통해 위치값의 결과를 만들어 내는 operation이다.

  - 성질
    - 교환법칙성립(Commutative)
    - 결합법칙성립(Associative)
    - 분배법칙성립(Distributive)
      - 가능한 이유는 convolution 연산이 linear operation이기 때문에 가능.
    - convolution은 linear filtering이다.  input의 linear combination으로 conv output을 만들수 있기 때문에.
  
  - convolution kernel을 이용하여 smoothed, sharpened, gradient(x 또는 y축 방향으로) 등 가능

    - 1) smoothing filter
    ![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/ee386e78-7087-42f8-9554-91e07572fd86)

      - bilinear filter를 활용 (주변의 값들을 이용해서 가운데 값을 보간한다.) 해서 conv kernel 통과시킴. 
      - avarage filter를 통과시키면 흐릿하게 image가 나옴.
      - gaussian filter를 통과시키면 주변에 조금더 큰 영역에 대해서 값을 가져와서 merge하게 된다.
      - 장점) 주변의 noise값들을 reduction해서 조금더 깔끔한 이미지가 나오고 noise에 insensitive(무감각한) 이미지를 만들어 낼수 있다

    - 2) Gaussian filter
    : Noise reduction by smoothing

    ![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/43cbde94-7616-4bc5-9d91-8a79c880c7d0)

      - 2-1) 2차원 가우시안필터
        - x,y값에 대해서 중심부로 갈수록 값이 커짐.
      - 2-2) size는 x,y의 kernel사이즈이며, sigma가 커질수록 noise가 많이 제거 된다. 대신 image가 blur처리 된다.

    - 3) Gradient Filters
    : 어느방향으로 (x or y 축) 많이 받았는지에 따라 다름 

    ![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/1e801004-7728-48f0-a2d9-5d60303e48ad)

      - 3-1) horizontal filter 사용하면 x방향으로 변화가 큰 위치에 대해 detect하게 된다.(x방향으로 변화가 더 큰게 더 값이 크게 나오니깐)즉, 세로선이 더 검출이 잘된다.
      - 3-2) gradient filter를 사용해서 이미지의 gradient를 구할수 있고, gradient 방향의 x,y방향을 합쳐서 탄젠트를 적용하면 orientation도 구할수 있다(방향도 구할수 있다)

  
## 2-1. padding
: 이미지 필터링에서 boundary를 없애주기 위해 주로 사용한다.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/392d2c94-a361-4b14-9494-3075d5f872e8)

  - zero padding : original image에서 0을 패딩으로 채운것. 이러면 input feature 같은 size가 output feature로 나온다. 단점은 0로 채워져있기 때문에 boundary factor를 피할수 없다.
  - wrap padding : 랩 패딩은 가장자리 주위로 이미지를 감싸 이미지가 가장자리 주위를 감싸는 것처럼 보이는 연속 패턴을 생성하여 이미지의 원래 내용을 유지합니다.
  - clamp padding : 가운데 있는 pixel값들을 그대로 이미지 끝부분에 적용시킨 패딩
  - mirror padding : 이미지에서 반을 접어서 padding된 영역에 대해서 값들을  펼쳐서 데칼코마니 처럼 적용시킨것.

# 3. Edge and Corner
: 이미지 filter중에서 image의 edge와 corner를 어떻게 검출하는지

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/c322740c-d404-4de8-a2f1-844ecbcce819)
  - edge란? 
    - 점들의 집합.(= 선)
    - 구분된 region의 boundary.(선으로 구분하니깐)

  - 선은 이미지에서 어떤 조건이 있어야 하는가?
    - pixel intensity가 갑자기 변해야 한다. ( = gradient magnitudes가 크게 작용해야 한다.)

  - edge는 무엇으로 만드나?
    - 질감
    - 깊이의 차이
    - 방향성이 달라질수 있다

## 3-1. Edge Detection Process

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/33bc47b8-6e68-4930-9382-32ef74e0d22a)

  - input image를 smoothing한 후 그 image에 대해서 gradient magnitude를 계산한다. x와y방향에 대해 gradient를 구한 후 gradient map에 제곱해서 더한후 루트씌워서 gradient의 크기값을 visualization했을 때 흰색(흰색이 높은 value이다.)이 보이게 나온다.이걸 non-max suppression(NMS) 후 thresholding하여 주변값에 대해서 작은값들은 없애버리면 edge들이 나오게 된다.이것이 edge detection process이다.
    - NMS후 thresholding하는 이유는? edge도 아닌데 먼가 image상에 존재하는 부분들을 제거하기 위해서이다.

### 3-1-1. edge detection
: gradient image를 활용해서 진행한다. denoing을 활용(smooting을 사용한 convolution) + edge detection(gradient filter를 활용)

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/6ddedb0f-450e-48f3-88d2-65466e4b24e0)

  - incoding x : Gaussian에 x,y방향으로의 Derivative(파생) 을 incoding 에 적용하면 구할수 있다.
  - x방향으로 편미분한것을 2차원 으로 보면 x방향으로 gradient 필터가 있다고 해석 할수 있다. (색깔이 급격하게 변하는 부분이 x축방향으로 생긴다)
  - output으로 나온 결과는 양수,음수범위 모두 될수 있다.(양수,음수는 방향성을 나타낸다.)

## 3-2. Corners
: corners는 영역들이 합류되는 부분, 즉 edge들이 모이는 부분을 corners라고 할 수 있다.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/ac629e32-df69-400e-8696-09898dfa8e08)

  - corner는 match하기에 좋은 local feature이다.
  - match는 컴퓨터 비전에서 가장 고전적인 task이다. 
  - 2장의 image에서 corner가 얼마나 다른지만 판단해서 match를 쉽게 할수도 있다. corner의 sparse(희박한)한 match를 통해서 3차원 정보를 복원하기도 쉽다. 

### 3-2-1. Harris Corner Detection
: corner를 찾는 가장 대표적인 알고리즘

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/6c854114-b193-485e-b8c3-fe8cb7af440c)


  - corner : 모든방향으로 중요한 변화가 있는것
  - edge : pixel값이 한 방향으로만 변화가 있는것
  - flat : pixel 값이 어느 방향으로든 변화가 없는것.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/a854e2e4-1c67-4aea-8222-92dd8557948c)

  - u,v라는 위치에 대해서 이동된 위치와 비교하고있는 위치의 차이의 제곱(intensity, magnitude를 말함) 이 얼마나 큰지 정의.
  - 이것도 convolution 처럼 window function이 있어서 window funcion이 돌아다니면서 pixel의 중요값들이 얼마나 변하는지를 알고있는 representation이다.

  - gaussian weights를 주면 보고있는 위치에 대해서 주변값들의 weight들을 낮춰줄수 있다. 즉, **gaussian weight를 쓰면서 noise에 대해서 robust(강건성)하게 된다.**

  - u,v가 주변과 비슷한값을 갖게 되면 0을 갖게된다. 
  - 주변과 구분되는 patch를 가지게 되면 해당 값이 커지게 된다.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/8f1b0abd-60bb-4b0b-9035-860561a0b6b3)

  - unit vector : image space에서 0,1,2,3,4 이렇게 되어있는데 -1~1사이로 옮겨진것
  - unit vector는 eigenvector로 표현가능 : 벡터에 스칼라값을 곱한 (a1과 a2를 곱한)것으로 표현 가능.
  - corner point라면 image에서 u,v 위치는 unit vector값보다 큰 값을 가져야 한다. 왜냐하면 주변의 값들에 대해서 많이 변화된것이기 때문에. 즉, min E(u,v)했을때 가장 큰값이 corner라고 할수 있다. 
  - γ₂(람다2) : eigenvalue중에 더 작은 eigenvalue라는걸 말한다.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/43dcc5f7-d651-4ad2-b1ea-7b724442f288)

  - input patch가 flat하다고 하면, x,y 에 대해 derivate(미분) 하면 noise값이 나온다
  - x방향으로 linear 한 edge가 있다고 가정하면 x-derivate하면 x방향에 위치에 대해서 window function을 통과시켰을때 가운데 줄에 intensity가 높게 나온다.반면에 y방향으로는 edge가 없으니 intensity가 낮게 나온다.
  - 분포(distribution)에 대해 plot하게 되면 flat한 region에 대해서는 모두 작은 intensity를 갖게 된다. 하지만 linear edge같은경우, x방향으로 큰 intensity를 갖는다. corner는 x,y방향으로 모두 큰 intensity(세기)를 갖게 된다.
 
![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/dac88419-8626-47ad-aa88-1faa939339b4)

  - 두개의 eigenvalue는 corner의 shape과 size를 결정하게 된다.
  - 아무것도 없는경우 작은범위.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/9c74422f-13a2-4f3b-8912-a440e7f73e13)

  - 다음과 같이 eigenvalue값의 분포에 따라, edge인지 corner인지 flat인지 판단할 수 있습니다.
  - 하지만 eigenvalue를 직접 계산하는 것은 복잡한 일이기 때문에, 다음과 같은 공식을 이용해 corner인지, edge인지, flat인지를 판단합니다.
    - determination와 trace값을 통해서 판단
    - R=det(H)−k(trace(H))²
    - 1) R>0 (eigenvalue가 모두 0보다 클때): corner
    - 2) R<0 (y방향으로 큰 eigenvalue가질때) : y방향 edge
    - 3) R<0 (x방향으로 큰 eigenvalue가질때) : x방향 edge
    - 4) R≈0 : flat

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/bdc83aaf-0f0a-4a49-bf24-c2ad585244ac)

  - 조도변화가 rotation으로 있는 image를 사용할떄, corner response map사용하면 빛같은 noise까지 잡히게 되고, 이것을 큰값에 대해 Thresholding 하게된다. 그것을 주변값에 비해 더 큰값을 찾게되는 Non-max suppression 하게 되면 아래결과값이 나온다.

### 3-3. Edge detection algorithm
- 1차 미분
- 2차 미분 : Laplacian filter
- sobel edge
- Laplacian edge
- canny edge


# 4. Blob
: blob이란 이미지의 영역을 말하며, 주변보다 밝거나 어두운영역을 말함

## 4-1. blot detection 절차

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/3536432b-fafe-4e4d-afef-899fc26704a9)
  - 1) image를 smoothing진행
  - 2) LoG(Laplacian of Gaussian) or difference of Gaussian 적용
  - 3) optimal scale과 orientation parameter 찾기
  - 위 이미지에서 동그라미로 표시된게 blob 부분이다.

### 4-1-1. edge detection 다시보기

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/3bcc3c5b-80c2-4c2b-b7c4-4547a1a1cdc1)

  - edge detection을 보면 DoG(Deriavative of Gaussian)를 적용시킬때 response가 가장 큰 부분을 edge라고 표현함.

### 4-1-2. blob detection

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/81f42bc3-f6ac-4c51-90d5-31b88804621f)

  - 이때 사용하는 operator는 LoG(Laplacian of Gaussian)이며, 이를 적용시킬때 response가 가장 큰 region을 blob라고 표현함

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/10d0d9d4-5206-4a0c-abe4-3efcb1479f5c)

  - Laplace operator는 2개의 gradient vector operator의 내적으로 구해진다.

## 4-2. Laplacians for Blobs with Different Size
: 모든 blob의 사이즈는 다르기 때문에 적용되는 Laplacian을 찾아줘야 한다

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/c14643d3-7dac-49f7-a153-5bbedef1ad63)

  - **서로다른 시그날에 대해서 적절한 Laplacian의 magnitude를 찾아줘야 한다.**
  - blob의 center에서는 Laplacian 값이 maximize가 된다 
  - optimal scale은 Laplacian 값이 maximize되는 곳이다.

## 4-3. scale selection

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/fa3205ff-3332-4790-8002-74066c0b2de0)

  - Laplacian 그 blob의 반지름의 루트 2 분의 r값이 best scale값이다

## 4-4. scale normalization이 필요한 이유

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/9eb9dee6-2f79-46c3-b0ce-358e3ac783d6)

  - Laplacian의 magnitude는 scale에 따라 달라진다.
  - "시그마"값이 작으면 반지름 값이 작기때문에 magnitude를 알기 어렵다. 

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/47a2042c-2fbf-4c31-ba8c-455e9bff581f)

  - 즉, Laplacian of Gaussian filter를 image에 돌리게 되면 다양한 size의 blob이 detection하게 된다. 
  - keypoint로 나오는게 아니라 scale을 알고있는 blob들의 모음집으로 나오게 된다.
  - **결국 scale Invariant(규모 불변량) 한 local featrue를 뽑을때 중요한 컨셉으로 작용한다.**


# 5. Scale-Invariant Feature Transform (SIFT)
keypoint와 descriptor를 뽑아내는 알고리즘
가장 유명한 image descriptor이다.

## 5-1. feature point란?
![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/4ae50041-97f1-4e93-b2fa-a04d43157967)

  - feature point란 주변의 neighboring point과는 다른 위치들을 말한다.
  - flat한것은 homogeneous해서 구분이 잘안됨
  - linear한곳은 edge로 표현이 되고
  - corner는 x와y 모든방향으로 모든 gradient값이 많이 변화되는곳.
  - 옆에는 corner decision algorithm돌린결과.
  - 이러한 점들을 feature point라고 하고 이것을 pixel값으로 나타낼수 없기 때문에 pixel의 descriptor한 feature vector로 나타내는 방식을 배울것이다.


## 5-2. SIFT overview

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/ec278653-66d1-4384-aa53-9e0791b9a396)

  - SIFT는 크게 2가지 단계로 나누어진다.
  - keypoint detector
    - blob detection이나 harris corner 업데이트 버전
    - scale-space extrema값을 찾는다. 그리고 그것을 thresholeding을 통해서 keypoint filtering을 한다.
  - keypoint descriptor
    - 찾아진 keypoint들에 대해서 image gradient값을 찾아내서 orientation 을 assignment한다. 그러면 4x4 grid의 어떤 orientation을 가진 histogram들로 표현이 된다. 어떤 dominant(우세한) orientation(방향)으로 normalization한 후에 descriptor를 계산하게 된다. 

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/d715f280-80ae-4ca5-8e56-8e9a822f2c07)

  - SIFT로 local feature를 찾아내게 되면 어떤 affine(공간)된 region들.즉, orientation과 scale을 가지고 있는 region들에대한 정보,keypoint들도 사실 affine tranformation이라 할수 있다
  - 그 region들을 visualization할때 그부분들의 patch를 때서확인해보면 4개의 값들이 나온다.
  - 이것을 더 정확하게 뽑기 위해 orientation을 사용해서 normalization해줘야 한다.
  - 그래서 SIFT는 scale variation에 대해서도 robust하다
  : 왜냐하면 scale-space extrema를 detection했기 때문에.
  - 그리고 illumination change에도 robust하다
  : 왜냐하면 gradient값을 사용해서 하기 때문에 . gradient는 illumination이 변한다고 해서 크게 변하지 않는다. 
  - 또한 rotation에서도 robust하다. 
  : 왜냐하면 dominant orientation을 사용해서 그걸 다시 normalization해주기 때문에.
  - viewpoint change에 대해서도 robust하다.

## 5-3. find scale-space extrema

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/9a433076-99e6-4fcd-bcfd-c38741f13f89)

  - 아까 blob detection에서 했던 Laplacian of Gaussian을 보면 가우시안에 Laplacian operator를 사용해서 구하게 된다.그러면 gaussian 값에 있는 내부 선형적인 표현으로 상수를 구해서 Laplacian of gaussian filter를 얻어낼수 있다.
  - 이것을 사용하는 이유?
    - interest point(keypoint나 blob)들을 detection하기 위해서 Laplacian of Gaussian(LoG)을 사용하게 된다.
    - 또한 다양한 시그마값에 대해서 (blob의 사이즈가 여러개 차이가 있을건데) 그 optimal scale을 search하게 된다.하지만 그것을 일일히 모두 search할수 없기 떄문에 scale normalization된 LoG를 사용하게 된다.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/df8d96f4-2728-4f2e-bd16-fd6c2853c809)

  - Laplacian of Gaussian은 좋지만 연산량이 너무 크다.
  : 왜냐하면 모든 위치에 대해서 값들을 모두 계산해서 넣어줘야 하기 때문에.
  - 그래서 SIFT algorithm에서는 DoG(Difference of Gaussian)을 사용한다.
  - DoG는 서로다른 bandwithds를 가진 2개의 Gaussian을 뺀 값이다.
  - 쉽게 LoG의 효과적인 approximation version이라고 생각하면 된다.(빨강-초록 = 파랑 그래프가 나온다)
 
- 수학적표현 : LoG -> DoG

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/0c7639d2-f30d-462e-aab0-c8209d6b79bb)

- SIFT의 DoG workflow

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/737d191b-a5eb-45c5-96d7-b7591deb5c44)

  - Gaussian filter를 적용하면 image가 blur하게 된다.그것은 결국 scale을 키우는것이다. 그리고는 image를 1/2한다
  - 각각의 image에 대해서 값을 뺀다. 그러면 Difference of Gaussian(DoG)가 된다.즉, 이건 scale pyramid에서의 차이이다.여기서 3x3x3 neighbourhood를 보게 되면 가로x세로x위아래scale 피라미드 = 총 27개의영역중에 가장 extrema한값이 높은값을 keypoint라고 말해주는 filter를 한번도 통과시켜준다.(즉, non-maximal suppresion이다, 왜냐하면, 크지않은값들을 0으로 만들어주고 큰값들에 대해서만 extrema값이라고 말해주기때문에)이것이 scale-space를 찾는 SIFT의 과정이다.

## 5-4. keypoint filtering
: low contrast(대비)한 keypoint 제거

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/c9be76f6-7747-42e5-9e10-c206bd2ccb34)

  - non-maximal suppresion적용시 local하게 confidence값이 높은 corner들이 나오는데 그게 keypoint가 아닐수도 있다. 너무 contrast가 낮아서 noise같은게 생길경우도 있기 때문에. 이런부분을 없애주기위해!

  - 즉, DoG의 응답값이 threshold값보다 낮은 위치에 대해서는 제거.
  - 다만, 주의할점은 image pixel value값이 0~1사이에 있는 normalization된 값이라고 생각해야한다. 왜냐하면 원래 image input을 받으면 0~255사이의 값인데(jpg,png image input받을시)이걸 받아서 0~1사이 값으로 normalization한것을 DoG로 한다고 가정한다. 

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/6150f5fc-e9d4-4607-9340-d65e2840d3ec)

  - 한방향으로 간건..즉 edge같은건 제거
  - edge는 hessian matrix가 있으면 hessian matrix의 difference of Gaussian에 eigenvalue들을 구할수 있다. 그래서 trace와 determint를 구하고 한방향으로의 edge response들을 수식으로 구하게 된다.
  - 람다1을 람다2의 scale한값으로 볼수있다.eigenvalue이기 때문에.그래서 람다1을 람다2의 갑마라고 표현.그래서 이값이 델타값이상일때 keypoint위치를 direction해주는 거라고 생각. 


## 5-5. orientation assignment
: 찾아낸 keypoint들에 대해서 point만있으면 구분하기 어렵기 때문에 그것에 descriptor를 뽑아주자.각 위치의 local을 표현하는 표현자를 만들어주자.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/b7127ae4-d0a4-4785-9b59-963f42190321)

  - 고전적인 방법은 gradient를 사용해서 orientation과 magnitude를 구할수 있다.x방향,y방향의 gradient가 있으면, 그 값을 삼각함수를 사용해서 탄젠트를 이용하여 각도를 구할수 있다. 어디를 가르키고 있는지.
  - 그것이 쎄터이다. x와 y의 벡터의 value값을 사용해서 magnitude값을 구할수 있다.그것이 m 값이다.
  - m은 L. 즉, image gradient값에서 벡터 크기와 방향구하는 식이라고 생각하면 된다. sin/cos = tan 이니깐.

  - 그래서 dominant한 orientation을 prediction하게 되는데 초록색네모칸안에가 이미지의 pixel이라고 한다면 각 pixel에 대해서 orientation과 magnitude 즉, 크기와방향이 할당이 된 pixel의 local feature값을 구할수 있다. 이것을 각각 사용하게 되면 구분되기가 어렵기 때문에 SIFT에서는 위치들을 aggregation 해준다.주변의 방향성의 값들을 histogram에 voting해준다. 그래서 0~30도 사이에는 몇개가 있고 이런식으로.. 그래서 가장 많이 나온 dominant(우세한)한 orientation을 구하게된다. 특정한 패치영역에 대해서 많이 배치되어있는놈이 얻어진다. histogram의 representation의 장점은 60도나 127도등 이렇게 애매하게 많은게 2개가 있다면  sencond best가 80%이상 ratio일때 즉,,60도를 가르키는 orientation과 170도를 가르키는 orientation이 애매하다고 판단되었을때 keypoint를 2개 뽑아버린다. 그리고 2개의 descriptor를 뽑개된다. 그래서 multiple한 candidate을 만들수 있다.

  - 즉 이것은 repeatability 많이 증가시키고 noise에 대해 robust하다고 말할수 있다.즉, noise가 있을떄 여러개를 뽑아서  그것끼리 비교할수 있으니깐.


## 5-6. calculating descriptor
: 마지막으로 얻어진 orientation histogram 이라고 하는 orientation patch 벡터와 dominant orientation을 사용해서 descriptor를 만들게 된다. 

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/6fe51664-8e1a-45e0-81df-e705553b187c)


  - histogram을 만드는 방식은 8x8을 예시로 보자면, 8x8 grid의 patch가 있다고 가정했을때 그것을 4x4로 쪼갠다. 그럼 16개의 cell이 존재하게 된다.16개의 cell에 대해서 orientation을 aggregation하여 구한다. 4개에 대해서 orientation의 반응을 구하게 되고 그러면 각각의 방향성을 가지게 된다.  그 histogram을 8가지 방향성에 대해서 0도~45까지 5개, 45~90도까지는 3개 이런식으로 각도를 기준으로 histogram을 하나의 벡터로 펼친다. 그것을 왼쪽 부터 시작해서 붙인다.  실제로 SIFT는 4x4 grid로 나누게 된다. 그래서 16개의 cell에 대해서 8개의 bin histogram이 나오게 된다. 즉, 12x8=128 dimension이 된다. 그걸 8개로 쪼갠다.그러면 8개의 quantized orientation이 나오게 된다.
  - histogram은  max인값으로 shifting해서 normalize해준다. 그래서 가장 큰값이 앞으로 가도록. 그래서 rotation에 inveriant(불변)하도록.roatiation에 robust하도록. dominant orientation을 normalization해준다.
  - 그리고 최종적으로 만들어진 값들을 (5,4,3,1,2, 등 이런값들) 현재 normalize가 안되있어서 unit length로 최종적으로 128짜리 dimenstion의 벡터를 normalize해준다. 그리고 최종적으로 PCA를 통해서 dimension reduction을 한다.이건옵션이다. 왜냐면 128짜리 dimension은 ...옛날pc는 128짜리 descriptor뽑는것도 어려웠다. 요즘은 gpu도 있고 computation한 이슈가 많이 사라졌으니 이런건 옵션으로 둔다.그래서 dimension reduction(차원축소)은 요즘 잘안한다. opencv나 kornia등에 있는 SIFT구현들도 굳이 pca를 잘안한다. 

- 전체정리1

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/b890912c-cacb-419f-923b-8ba264bcb684)

  - 그래서 diagram의 illustrator를 통해서 descriptor를 만드는것을 보게 되면 DoG를 이용해서 keypoint를 뽑고 DoG로 뽑힌 blob들과 orientation들을 통해서 그 근처의 pixel을 보고 gradient를 사용해서 pixel의 orientation map을 구한다. 각 orientation cell들에 대해서 각각 histogram voting을 해준다. voting된 histogram을 나열을 해서 128짜리 dimenstion짜리 descriptor를 만드는게 SIFT descriptor이다.

- SIFT 요약하기(고전 이미지 descriptor의 집합체이다.)
![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/8f81f4a9-931d-46d4-bfff-92d237eeda94)

  - 가우시안 이미지 피라미드로부터 scale-space를 찾아서 scale invariance를 만족시킴
  - dominant orientation estimate를 통해서 rotation invariance도 획득.(image gradient를 magritation하여서)
  - histogram representation을 통해서 작은 variation에 대해서 robust하도록 만든다. 
  - 조도변화에 대해서도 민감하지않다. 왜냐면 gradient는 조도가 바뀐다고 해도 크게 바뀌는게 아니니깐. gradient값을 normalize해주면 illumination change에 대해서도 robust할수 있다.
  - 촬영시에 생기는 noise도 robust하다.왜냐하면 gaussian smoothing을 통해서 이미지 피라미드를 만들때 gaussian blur를 통해 만드니깐.

  - 왜 DoG가 LoG보다 효과적일까?
    - LoG할때 Gaussian smoothing이나 separability나 cascadability를 고려해야하는데 그런거에 비해서 조금더 자유롭기 때문에.

- SIFT한결과

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/42fa7b9d-121b-4e6d-ba5d-a0bd97256489)

  - SIFT keypoint와 keypoint들은 모두 scale orientation을 가지고 있다. 그것을 matching한 결과. scale과 orientation도 같고, image descriptor는 invarient하게 뽑혀서 값들이 잘 매칭된다.

- SIFT의 feature matching하는 순서

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/8ad592e2-94de-48ee-9417-6d9554542d0a)
  
  - feature 뽑기(keypoint descriptor를 뽑는다. keypoint는 이미 scale과 distriction이 있다)
  - match 되는 정답들을 찾는다.
    - a. Lowe's ratio : 가장 가깝게 찾은 descriptor의 거리와 2번째로 가까운애의 거리가 어느정도 비율 이하라면 둘다 envigious한 match이다. 그러면 둘다 drop해준다. outlier나 envigious match들을 제거해주기
    - b. cycle consistency check를 이용해서 최종적인 match를 찾는다: 그림1에서 그림2로 가는 match와 그림2에서 그림1로가는 match는 같아야 한다.이걸함으로써 outlier 제거효과가 생긴다.

  - paranoma 이미지 만들떄 사용가능 : 카메라 돌릴때 대응점들을 많이 찾을수 있고 그것을 사용해서 이미지들을 붙일수 있다. 그래서 긴 이미지또는 넓은 이미지 등을 찍을수 있다. 다만 그걸 평면으로 표현하는게 어렵다. 파노라마 이미지는 그래도 2차원 평면으로 표현했을때 자연스럽게 나온다. (Brown and Lowe, **Recognising Panoramas**, ICCV'03)
  - wide-baseline matching : 파노라마 이미지나 stereo match같은경우 카메라 2장의 사이가 가깝거나 특정하게 이동하는 가정이 있다. 즉, 카메라 사이의 baseline이 멀수도 있고, 카메라 방향이 rotation될수도 있는경우. 
  즉 실험실내부말고 외부에서 찍힌 현장의 사진들. 최근에는 이런가정을 더 없애서 semantic matching application들도 많다.
    - non-diffuse reflection  : 그림자 진거.

- opencv sift 공부
  - sift는 image에서 keypoint와 descriptor를 뽑는 알고리즘이다.
  - "Distinctive Image Features from Scale-Invariant Keypoints" 여기논문에서 나옴.
  - 가장먼저 scale-space extrema detection을 한다. LoG를 하는게 아니라 DoG를 통해서 값을 efficient하게 구한다. 
  - DoG로 얻어진 feature에서 scale 피라미드에서 3x3x3 kernel을 돌려서 non-maximal-supression을 해주게 된다. 그러면 이미지 피라미드의 위아래의 3x3x3 사이에서 가장 값이 큰 부분을 keypoint라고 말한다. 
  - 두번째로 keypoint localization위해서 non-maximal-supression해서 얻어진 값들에 대해서 너무 값이 낮은값들을 threshold를 통해 없애준다. 그래서 contrastThreshold,edgeThreshold, 총 2가지 방법으로 keypoint localization해준다. 그래서 low-contrast keypoints and edge keypoints 를 제거해주고 strong interest point만 남긴다.
  - 세번째로는 orientation assignment해준다. 즉, image gradient map이 있으면 x,y방향의 gradient map을 합쳐서 orientation map을 구하게 되고 이것을 통해서 histogram map을 찾아서 10도마다 voting하게 된다.(36개 나옴.360도이니깐) 그래서 gradient magnitute를 갖게되고 그것을 region에 대해서 regregation해서 histogram을 주욱 세운후 128짜리 dimenstion의 descriptor를 만들게 된다. 
  - 네번째로 16x16 neighborhood, 즉 16x16 patch를 잘라내고 4x4짜리 subblock을 펼치게 된다. 
  - 최종적으로 matching을 수행하게 된다. 첫번째 이미지에서 두번째이미지가 가장 near neightborhood를 찾아서 second closest-match를 ratio test를 통해서 rejection을 해준다.

## 5-7. ORB(ORB : an efficient alternative to SIFT or SUFT)
: FAST detector + Oriented BRIEF


---


[맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}