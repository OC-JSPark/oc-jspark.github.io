---
title: "의료 인공지능 기술"
escerpt: "의료인공지능 기본이론 2"

categories:
  - healthcare
tags:
  - [AI, healthcare]

toc: true
toc_sticky: true

breadcrumbs: true

date: 2024-08-17
last_modified_at: 2024-08-17

comments: true
 

---

# 1. Machine Learning기법

## 1-1. 지도학습

: 학습하게 될 모델이 어떤 값을 출력하느냐에 따라 나눠짐.
 
1. 회귀(Regression)
    - 연속적값
    - ex) 자기공명영상을 통한 환자 연령 예측 모델
2. 분류(Classification)
: 주어진 학습 데이터들이 위치하는 다차원 공간에서 여러 레이블들로 구분하는 **경계를 찾는것**

    - 불연속적값
    - ex) 유방영상에서 유방암 진단 수행 모델(악성,양성 구분)



![image](https://github.com/user-attachments/assets/a2bbe1bb-5366-4854-8aa4-5df284264027)

- 성능의계산
    - i번째 학습데이터에 대한 오차(Error)
        - 오차는 음수값과 양수값이 나올수 있으므로 **절대값을 취하거나 제곱을 하여 더함**

- 손실함수(Loss function)
    - 오차들의 합.
    - 최적값을 일반적으로 줄여나가는 방향으로 진행
    - 경사하강법(Gradient descent algorithm)

        ![image](https://github.com/user-attachments/assets/0205acf6-3b04-439b-9411-7a32f63bd6a7)

        - 학습률(Learning rate)
            - 학습률만큼 한발자국씩 이동해서 미분값을 취하여 최소값을 구하고 최적값을 찾는다.
        - **정형화된 손실함수(loss function)를 정의하고 이를 줄이는 방향으로 최적값을 찾아가는 과정**
        - 최적값 찾기 절차
            - 높은 학습률에서 시작
            - 손실함수의 감소 추세 확인
            - 안정적으로 감소하지 않는다면 지수형태로 감소
    
    - 미니배치 경사하강법(mini batch Gradient descent algorithm)
        - local optimum
        - 매 경사값 계산시 적은 수의 학습데이터를 이용하여 경사값을 계산하는 방식

## 1-2. 로지스틱 선형회귀(Logistic Regression)

- 선형회귀란?
    종속변수 y와 한개 이상의 독립변수 X와의 선형 상관관계를 모델링하는 회귀분석 기법
- 모델의 출력값이 0~1 사이
- 경사하강법을 쉽게 적용할 수 있도록 미분 가능한 함수면 좋음.
- sigmoid function
- 선형회귀 분석에서 사용한 값에 적용하여 만든 분류모델로 로지스틱 회귀모델이 있다.
- -log함수 

    ![image](https://github.com/user-attachments/assets/e553c642-65f2-462c-b10b-dff601f5077a)

    - y=0인 양성인 경우에는 예측값을 뒤집어 1-h세타x를 만들고 1을 -log함수에 넣으면 된다.

- 이진 크로스엔트로피(Binary cross entropy)
    - 분류모델에서 주로 사용되는 손실함수

- 경계선(decision boundary)
    
    ![image](https://github.com/user-attachments/assets/606c14c3-fe0c-4c27-936c-5a58d1db3aae)

    - 로지스틱 회귀 알고리즘이 학습을 통해 얻게 되는것이 이들을 나누는 경계선이다.

- 한계점
    - 로지스틱 선형회귀는 분류 알고리즘이며, 학습을 통해 두 분포를 나누는 경계를 찾는것.
    - N개의 특징값이 있는 경우, 학습과정을 통해 -log함수를 기반으로 하는 손실함수인 cross entropy를 최소값으로 갖는 세타를 찾는것.
    - 두 분포가 비선형적으로 주어진 경우 못찾음.
    - 경계를 전체 분포를 보고 결정하므로 Outlier에 취약함.
    - 분포가 복잡할 경우 적용할수 없음.

## 1-3. 서포트벡터머신(SVM, Support Vector Machine)

- 로지스틱 선형회귀의 문제점 해결
    - 최대마진분류기(large margin classifier)라고도 불림.
- 손실함수의 재정의
    - y=1 인 경우, Z값이 양수면 경계 위쪽에 위치 손실값을 작게 정의, Z값이 음수면 손실값을 높게 정의

    - 여러가지 손실함수
    
        ![image](https://github.com/user-attachments/assets/4ef30594-bb38-4f52-8110-33e102ccd1d8)

        - 경계의 위에 있으면 손실 0
        - 경계의 아래에 있으면 손실 1
        - 로지스틱 선형회귀에서 사용한 손실함수인 -log 함수 기반의 cross entropy는 빨간색으로 표시되어 있으며, 경계의 위에 있어도 손실함수가 0이 아닌 작은 값을 가지며, 특히 3 이상이 되어도 0은 아님을 알수 있음.

- 서포트 벡터 머신은 다른 손실함수를 사용함.
    - 경계의 주변(1이내)에 있는 데이터들이 중요함
    - 경계 바깥(1 이상)의 데이터들은 더 이상 손실함수에 영향을 주지 않음.

    - 이렇게 손실함수만의 변화를 통해 경계에 있는 데이터들을 좀 더 잘 나눌수 있도록 함.
    - **ReLU(Rectified Linear Unit)**
        - rectified : 정류한(ex.정류 알코올같은 느낌)
        - 힌지(Hinge)함수의 일종
        - 일정 한계값 보다 크면 0, 작으면 선형적으로 커짐.

- 소프트 벡터 머신은 새로운 손실함수를 도입하여, 두 분포의 경계를 보다 잘 나눌 수 있도록 함으로서 로지스틱 선형회귀가 갖는 첫번째 한계점을 극복.

- 로지스틱 선형회귀의 두번째 한계는 비선형 경계를 갖는 분포를 나눌수 없음이다. 이를 위해 **Kernel** 개념 도입.
    - 커널 : 두 데이터 간의 유사성 계산
        - 두 데이터의 특징값들이 서로 비슷하다면 1, 다르다면 0에 가까워짐.

    - 즉, 주어진 데이터를 서로 간의 유사성으로 모두 변환 > 새로운 데이터셋을 만들고 이를 학습에 사용함으로써 비선형 경계의 한계 극복.

- 정리) 서포트 벡터 머신은 커널 개념을 도입하여, 데이터들 간에 상대적 유사성을 기준으로 데이터를 변환하여 비선형 경계도 학습이 가능하도록 함.

- 커널 서포트 벡터 머신의 문제점
    1. 결과 설명 불가
    2. 특징값 선택 과정 필요
        - 특징값의 수가 점점 늘어날수록 분류 모델의 학습에 적합한 특징값을 찾는 특징값 선택과정이 추가적으로 필요하게 됨
     
## 1-4. 결정 트리 기반 기계학습 모델

- 서포트벡터머신 문제점 해결방법 = 결정트리

    - 결정트리는 연구자의 전문적 지식을 이용하여 이루어졌는데, 데이터를 통해 인공지능 모델학습을 통해서도 결정트리 얻을수 있음.

    - ex) 뎅기열 감염 고위험군 분류를 위한 결정트리
    
        ![image](https://github.com/user-attachments/assets/7ca08d80-097f-4396-8928-c0ab7f58af9b)

        - 병원에 내원한 환자에게 토하는 증상이 있었는지를 보고, 흉부 X-선 영상을 이용하여 흉막 산출이 있는지와 혈압정보를 이용한 구성

- 정보이득(Information gatin)
    - 결정트리 모델은 새로운 정량적 성능 평가함수 이용함.이를 "정보이득"이라 한다.
    - 최솟값이 아닌 최댓값을 갖도록 트리에서 노드를 하나씩 찾게 됨.

- 결정트리의 학습 방법
    - step1) 모든 특징값을 하나씩 가져와서 임계(Threshold)값을 변화시켜가며 정보이득값을 계산하고 최적의 임계값을 찾음.
    - step2) 임계값을 기준으로 한 각 특징값의 정보이득 중 최대값을 갖는 특징값을 이번 노드로 구성
    - step3) 다음 노드에서 위의 과정 반복
    - step4) 모든 데이터가 구분되었거나, 설정한 층 이상이 되면 중지.

- 한 특징값에서 임계값을 결정하는 예

![image](https://github.com/user-attachments/assets/d1f4bc06-375e-477e-8926-679148a581cd)

- 경계가 직교하게 그려짐
- 이는 결정트리 모델의 특징인, 한 가지 특징값을 한 노드에서 결정하게 되어서 지그재그 형태로 경계가 그려짐을 알수 있음.

- 장/단점

|장점|단점|
|:---:|:---:|
|중요한 특징값을 선별할 수 있음|비선형 모델보다 낮은 성능을 보임|
|사람이 쉽게 이해할 수 있음|-|

## 1-5. 랜덤포레스트(Random Forest)

- 결정트리를 좀더 보완하고자 등장한 모델.

- 부트스트랩(bootstrap)
    - 데이터 샘플링 방법
    - 학습데이터에서 중복을 허용하여 랜덤하게 샘플을 뽑는것

    * 결정트리는 주어진 데이터셋에서 하나의 구성을 가지게 되므로 다양성을 줄수 없음. 그러나 부트스트랩을 사용하면 같은 학습데이터 내에서도 조금씩 다른 여러 결정트리들을 만들어낼수 있어서 다양성을 줄수 있음.

- 예측결과의 앙상블(Ensemble)
    - 일반적으로는 다수결방식이 많이 사용됨.

# 2. CNN : Classification

- 영상에서 컨볼루션이 수행되는 과정

    ![image](https://github.com/user-attachments/assets/23b3b46d-78a6-47c0-876f-046e0940fffd)

    - step1) 컨볼루션 커널을 준비하고 영상을 겹침
    - step2) 겹치는 영역에서 컨볼루션 커널을 각각 영상값을 서로 곱한 후 모두 더함
    - step3) 그 값을 중심에 적고 영상에서 옆으로 이동하여 같은 과정 반복

- 컨볼루션을 이용한 CNN장점
    - 딥러닝 이전에 영상처리 분야에서도 영상의 경계를 찾거나 노이즈를 제거하거나 선명하게 하는데 컨볼루션이 사용됬음.


    - 기존 영상처리 방식과 다른점은 **목적에 알맞은 컨볼루션 커널을 스스로 결정함**

    - 컨볼루션은 **특징추출** 기능 있음.
        - ex) 차가 있는 영상을 찾은 인공지능 모델 > 동그란 원 형태의 컨볼루션 커널을 만들어 적용하면 타이어가 있는 위치가 큰 값을 가지게 됨. > 큰 값들이 몇 개 이상 영상에서 발견되면 "차" 라는 특징값으로 기능함.

## 2-1. CNN 구조

- 영상처리 기법의 차이

|과거|현재|
|:---:|:---:|
|sensor > Feature Representation > Learning algorithm|특징값 추출과정 자동화|
|영상에서 특징값추출후 이를 학습하여 수행|-|

- 과거에는 차를 분류하는 알고리즘을 가지고 있다면, 고양이 분류 알고리즘 다시 만들어야 했음
    - 차의 특징을 계산하는 부분을 고양이의 특징을 계산하도록 고양이 전문가가 다시 설계해야 했음.
- 현재의 딥러닝은 해당 알고리즘에 고양이 영상을 넣기만 하면된다.

- CNN(Convolutional neural network)을 이용한 영상분류 모델의 구조

    ![image](https://github.com/user-attachments/assets/dbf02381-8808-4722-b532-18b7288b38ac)

    - CNN구조는 입력영상에서 컨볼루션을 통해 자동적으로 특징값을 추출
    - 이렇게 추출된 특징값을 기반으로 간단한 분류 인공신경망 모델(MLP, Multi Layer Perceptron)을 결합하는 형태로 이루어짐.
    - 구조는 3가지로 간단하게 되어있다
        - convolution
        - Activation(일반적으로 ReLU)
        - Pooling(분류모델에서는 max pooling)


## 2-2. CNN모델의 진화과정

- AlexNet
    - 2012년 ImageNet challenge 우승 모델
    - 구조
    
        ![image](https://github.com/user-attachments/assets/7a40f1c0-5c7e-414d-b89e-60bccafc9321)
            
        - 11x11크기의 커널을 처음사용
        - max pooling이후에 5x5 커널을 사용하는 구조
- VGG16
    - 2013년 ImageNet challenge 우승모델
    - 구조

        ![image](https://github.com/user-attachments/assets/e6a88556-76c7-408a-a76c-9bfb18fd021e)
        
        - 3x3크기의 커널을 2~3번 연속해서 사용하고, max pooling수행
        - 현재까지도 **동영상 분석** 모델의 pretrained모델로 사용

- GooLeNet
    - 2014년 ImageNet challenge 우승모델
    - 구조(GooLeNet의 인셉션 모듈, 초기버젼과 개선버젼)

        ![image](https://github.com/user-attachments/assets/efae6bb6-37ad-417a-bfab-b7597eb6d971)
        
        - 인셉션모듈 사용
            - 인셉션 : 여러 Scale의 특징값을 한 층에서 함께 볼수 있음.
            - ex) 고양이 실험생각해보면, 막대의 굵기가 커지면 컨볼루션 커널과 맞지 않아 반응하지 않음. 그런데 고양이를 조금 멀리 위치하게 하거나, 영상을 축소하여 보여준다면 컨볼루션 커널과 다시 맞을 수 있음.

- ResNet
    - 2014년 ImageNet challenge 우승모델
    - 구조

        ![image](https://github.com/user-attachments/assets/fb3f3f41-0583-487a-b6cd-19c814190436)
        
        - 이전에는 CNN모델의 깊이가 깊어지면 학습이 어려워지는 문제 발견
        - Residual connection을 통해 해결책 제시
        - VGG network의 경우 19 layer정도가 가장 깊은구조였음.
        - ResNet은 50에서 시작하여 152 layer까지 깊어질수 있음.
        - **영상**연구에서 baseline구조로 많이 사용됨.

- DenseNet
    - 2016년 벤지오 교수 연구실에서 제안
    - 구조

        ![image](https://github.com/user-attachments/assets/70e9575a-50fe-4290-801c-c93a154ef8ce)
        
        - ResNet은 Residual connection이 addition영상으로 이루어져 있었다면, DenseNet은 Channel-wise concatenation을 사용
        - 장점 : 각 컨볼루션 층이 여러 계층에서 얻어진 다양한 특징정보를 통합적으로 고려할 수 있음.
        - 단점 : 채널의 수가 뒤로 갈수록 많아지고 크기도 달라, 이를 보정하는 추가적인 작업 필요. 그리고 채널수를 줄이기 위해 **병목현상**을 이용
        - CheXpert dataset에서 높은 성능보여줌.

- EfficientNet
    - 2019년 ImageNet challenge 우승모델
    - 구조
    ![image](https://github.com/user-attachments/assets/d641bc2f-a448-4d4e-86d6-6597c854e161)
        - CNN구조에 대한 고민
            - 깊이 : 컨볼루션 계층을 얼마나 쌓고 max pooling을 수행할것인가?
            - 너비 : 각 컨볼루션 층에서 몇 개의 커널을 만들 것인가?
        - NAS(Neural Architecture Search)기법
            - 깊이,너비등의 설계인자를 자동으로 찾아주는라는 기법
        - NAS기법 적용한 CNN모델

# 3. CNN : Detection, Segmentation

- (주) Arterys
    - 심장 MRI영상에서 Chamber를 자동으로 분할해주는 SW
    - 미국에서 첫 FDA의 승인을 받은 인공지능 기술을 이용한 의료 영상 Software
    -[Arterys 홍보자료](https://www.dicardiology.com/product/arterys-cardio-dl-cloud-mri-analytics-software-receives-fda-clearance)

- 의료 영상 진단영역에서 보면 의료 영상이 얻어지면 판독에는 영상에서 보이는 질병 장비들을 자동적으로 찾고 병변을 검출하며 이를 통해 진단을 수행하게 됨.

- 진단순서
    1. segmentation
    2. detection
    3. Diagnosis
    4. Classification

## 3-1. Segmentation 모델
: CNN구조를 변경하여 만들수 있다.

- 이러한 네트워크 구조 변경은 Network surgery라 불림.

- VGG16

    ![image](https://github.com/user-attachments/assets/743db4fe-2e1f-4562-b39a-de02187249f2)

    - 입력 영상이 224x224크기로 네트워크에 들어옴
    - 컨볼루션 계층과 max pooling layer거치며 1/2 인 112x112로 줄어듬
    - 이러한 과정이 반복되면서 최종적으로 7x7 크기로 줄어듬.
    - 7x7이라는 크기 안에는 영상이 있는 물체에 대한 정확한 정보 및 위치정보를 포함하고 있음.
    - 분류 모델에서는 이러한 위치정보는 사라지고 물체에 대한 특징값만 이용함.

- segmentation은 위 설명과 달리 인공지능 모델의 출력 크기가 입력 영상의 크기인 224x224여야 함.

- FCN(Fully convolutional network) model
    - 어떤 방식으로 7x7크기의 특징값을 224x224로 만드나?
    - 7x7을 몇배로 만들어야 224x224가 될수 있을까? 32배이다.
        - "FCN32S" 라는 이름으로 FCN 모델이 나옴.
        - 32배로 샘플링을 한다는 의미임.
        - 이때 사용되는 방법이 영상처리에 있어서 오랜 기간 사용되어 왔던 인터폴레이션 기법이다.

        * interpolation 기법
        
            ![image](https://github.com/user-attachments/assets/5f5856a9-9ed5-4eea-88d2-2c261a4d25d4)

    - interpolation 적용절차
    : 32개를 한번에 적용하지 않고 단계적으로 인터폴레이션 적용하는 방법.

        - step1) 7x7 > 14x14
            - 이전 컨볼루션 계층에서 14x14크기를 갖는 특징값들을 Skip connection으로 가져와 더한다.
            - 이는 화가가 얼굴을 그릴 때, 전체적인 윤곽을 먼저 7x7로 그리고 난 후 좀 더 세밀한 키와 코 등을 14x14에서 가져와 그리는 것으로 생각해 볼수 있음.
        - step2) 14x14 > 28x28 > ..8배인 224x224
            - 8배만 마지막에 interpolation을 적용하여 224x224크기를 얻음.
            - 이 모델의 이름이 "FCN8s"이다
        

    - FCN8s 생성 절차의 문제점
        - Skip connection을 이용할 때, 그냥 더함
        - CNN구조의 앞부분에 위치하는 컨볼루션 계층에서 얻어진 특징값들은 영상의 큰 영역을 보고 얻어진 것이 아니기 때문에 noise가 포함되어 있을 수 있음.
        - 이를 그대로 더하면 마지막 결과에도 노이즈가 남음.
    
    - 문제 해결방법
        - Skip connection을 이용할 때, 그냥 더하지 않고 채널을 그대로 가져와 합치는 concatenation 하는 방식을 적용.
        - 이렇게 concatenation을 수행하게 되면 화가는 세밀한 부분을 그릴 때, 현재 가지고 있는, 보다 정확한 특징값 정보를 그대로 두고 CNN 구조와 앞쪽에서 가져온 특징값 정보를 참고하여 그릴 수 있게 됨.
        - 이렇게 skip connection과 concatenation을 사용한 것이 UNet이다.

- UNet
    - 구조

        ![image](https://github.com/user-attachments/assets/96b004d5-37e5-4d31-8cf8-3cb11fa3ca86)

        - (왼쪽) 인코더 부분
            - 인코더 부분에는 VGG16, ResNet, GoogleNet, DenseNet, EfficientNet등의 다양한 CNN모델 사용가능
                - ex) EfficientNet의 구조를 인코더 부분에 사용하여 디코더 부분에서 Skip connection과 concatenation을 사용하여 구성하면 EfficientUNet이라고 이름할수 있음.
        - (오른쪽) 디코더 부분
    - UNet구조가 의료영상 Segmentation에서 주로 사용되고 있음.

- Segmentation모델에서는 학습 시 손실함수로서 **Pixel-wise cross entropy**를 사용할 수 있음.


    

## 3-2. Detection 모델

- Detection에서는 이전의 정보를 박스 형태로 이용하게 되어 Regression을 수행하여야 함.
- 이전 박스 클래스는 분류에서 사용한 cross entropy를 사용함.
- 이렇게 Detection모델은 regression과 classification에서 사용하는 손실함수를 모두 사용하게 됨.

- Detection모델은 Region proposal network를 사용하여 보다 많은 수의 예측박스를 생성 > 이를 다시 classification model을 통해 걸러주는 형태에는 R-CNN 계열과 주어진 개수만큼의 예측박스를 바로 출력하는 형태인 Yolo계열로 나뉘어지게 됨.

- R-CNN
    - 구조

        ![image](https://github.com/user-attachments/assets/39ee92f0-6b4e-4d13-84a9-6c82404851f5)

        - 입력 영상에서 CNN구조를 통해 특징값을 추출하고 이를 통해 여러 예측 박스들을 생성
        - 이를 일정 크기로 변환한 후 분류 모델을 통해 예측박스 숫자를 줄이는 형태로 구성
        - 이러한 초기 R-CNN방식은 후에 region proposal을 위해 계산된 특징값들을 뒷부분의 분류모델이 그대로 사용할 수 있게 개선되어 fast R-CNN, faster R-CNN으로 발전.
        - 이러한 R-CNN방식은 느리지만 정확도는 높음.

- Yolo
    - region proposal network와 함께 CNN분류모델을 사용하지 않고 바로 예측 박스를 출력하는 Yolo계열은 빠른 수행성능을 특징으로 함.
    - 주로 실시간이 중요한 분야에서 사용됨.

    - Yolo에서 예측박스를 출력하는 예

        ![image](https://github.com/user-attachments/assets/c031517d-db5a-427a-a7ba-a624dce6a6c2)

        - Grid로 영상을 나누고, 각 Grid마다 anchor 박스 숫자에 해당하는 예측 박스를 출력하는 형태로 구성
        - grid : 7x7개, anker box : 5개 이면, 245개의 예측박스를 출력

- 최근 detection model에서는 segmentation model이 CNN구조의 앞쪽 컨볼루션의 특징값들을 함께 고려하여 최종 분할 마스크를 만들었듯이, Skip connection과 Concatenation을 사용하여 예측박스들을 생성하게 됨. 이러한 구조 중 가장 유명한 것은 FPN(Feature Pyramid Networks)이다.

# 4. 생체 신호 데이터 분석 기법 : RNN

- **시간**정보를 처리할수 있도록 고안된 인공지능 모델
- 시간 관계를 처리할 수 있어야 데이터 간의 관계를 모델링 가능.

- 언어모델과 함께하고, 언어모델에서는 번역 task 가 핵심이다.
- 번역 task와 함께 발전한 RNN모델들과 Sequence-to-sequence modeling도 알아보자.


## 4-1. RNN의 기본구조와 학습 방법

- FNN(Feed Forward Neural Network), CNN(Convolutional Neural Network)는 전부 hidden layer에서 activation function을 거친 값이 출력 층 방향으로만 진행되었기 때문에 데이터의 시간적 정보 처리 못함!

- RNN은 FNN모델에 Recurrent path라는 순환구조를 추가하고, 시간정보 t를 고려하도록 설계된 인공지능 모델.

- 이러한 recurrent path가 존재하는 중간 layer를 hidden unit 혹은 hidden cell이라 함.
- t번째의 hidden unit은 t-1번째의 hidden unit의 결과 값을 받아와서 연산을 수행함.

- 즉, 기존의 FNN에서 입력값으로 x가 주어지고 y를 출력하는 문제였다면, RNN은 입력으로 x와 과거의 분석 데이터 h^t-1이 함께 주어지고 y를 출력하는 문제이다.

* hidden unit/cell 
    : 이전 값을 기억하고 있는 일종의 메모리 역할을 하므로 메모리 셀(memory cell)이라 부르기도 함.

- RNN의 구조

    ![image](https://github.com/user-attachments/assets/1a89ae08-1db7-4c3c-bb69-2fcdb594588a)

    - hidden unit에 recurrent path W가 존재하고, h^t를 계산하여 y^t 출력을 낼때, 이전 시간의 hidden unit(h^t-1)의 출력 값을 함께 입력으로 받아 분석.

    - h^t = memory cell
    - hidden state = memory cell이 출력 층 y^t 혹은 다음 시점의 메모리 셀 h^t+1로 보내는 값을 hidden state라고 함.

    - 재귀형태
        - RNN구조는 재귀형태로 표현가능함.
        - 해당 RNN구조를 unfold해보자.

            ![image](https://github.com/user-attachments/assets/d0c4ebb8-18cf-43f0-960f-cccdb785d50b)

            - 데이터가 순차적으로 입력되고 계속해서 과거의 데이터를 누적하기 때문에 순서 정보가 중요한 **sequential데이터를 처리하는데 강점**을 갖게 됨.

            - time t의 데이터 분석 시 이전 시간의 정보를 담고 있는 h^t-1을 입력으로 함께 사용하므로 시계열 데이터를 이해하고 처리가능.

- FNN과의 차이 비교

    ![image](https://github.com/user-attachments/assets/b94642bc-788c-4dd9-ae13-ecb0e20ca3f5)

    - RNN을 FNN과 비슷하게 뉴런 단위로 표시
    - 입력 층의 뉴런 갯수가 4, 히든 유닛의 뉴런이 2, 출력 층의 뉴런이 2개인 RNN모델의 구조
    - hidden layer부분에서 time t-1과 time t, 그리고 time t+1간의 connection이 존재하고 이전 시간의 데이터의 정보를 전달.

- backpropagation으로 RNN학습

    - RNN의 학습 역시 FNN과 CNN에서 진행되었던 것과 비슷하게 back propagation으로 이루어짐
    - 다만 시간 정보가 포함되어 있기 때문에 이를 고려한 **BPTT(back propagation through time)**으로 학습 진행
    - 인공지능 모델의 가중치 업데이트를 위해 필요한 미분 값 delta구하기
        - 상위 layer에서의 미분 값을 back propagation 하기 + 다음 time h^t+1에서 발생하는 미분 값도 함께 고려하여 h^t에서의 미분값을 계산

    - 아래처럼 RNN모델의 가중치를 업데이트 할수 있음

      ![image](https://github.com/user-attachments/assets/1e09e551-08a8-4bf0-b9e3-892dcf18b13b)

## 4-2. RNN의 종류와 변형모델(LSTM)

- 입/출력 길이 변화에 따른 RNN 모델 종류

    ![image](https://github.com/user-attachments/assets/c3166f12-5a4a-48b0-a1ee-affbdbb81de1)

    - 각각의 RNN모델형태는 서로 다른 용도로 사용가능
    - one-to-many형태는 스팸메일 분류등에 활용가능
    - many-to-many형태는 실시간 번역기 혹은 개체명 인식기 등으로 활용 가능.

- RNN모델의 단점

    - 시간 t의 출력 결과가 이전 시점의 hidden state에 의존
    - 즉, 시점 t의 출력에서 시점 t-1 정보는 충분히 반영될 수 있겠지만, t-10과 같이 시퀀스가 길어지면 그 정보가 희석되어 버림.
    - 시간이 지날수록 첫번째 데이터인 x0의 정보가 손실되어가는것.
    - Long- Term Dependenct문제
        : 문장을 말할때, 보통 주어가 맨 앞에 등장하는데 행위의 주체가 중요한 정보라고 하면 RNN은 충분히 이를 인지하지 못할 것이고 엉뚱한 결과를 내뱉을 수 있음.


- RNN의 Long-Term Dependency문제 해결 모델

1. LSTM(Long Short-Term Memory)
    - hidden unit안에 input gate, forgetting gate, output gate추가하여, 불필요한 정보를 지우고 필요한 정보들을 더 오래 메모리 셀에 저장
    - 이를 통해 LSTM은 RNN과 비교하여 sequence가 매우 긴 경우에서도 효과적으로 동작

    - 구조

        ![image](https://github.com/user-attachments/assets/7640d199-e30e-4c9b-af3a-8173ae2cba88)

        - hidden unit에 LSTM이 각각 적용
        - 이전 layer의 출력(u_j)가 입력 노드인 b로 들어오고, 출력 값(z_j)이 출력으로 나간다. 이전 layer에서의 출력은 모든 gate에서 정책을 결정할때 사용됨.

    - 구조 뜯어보기

        ![image](https://github.com/user-attachments/assets/97604c80-5c88-444a-8ecc-0bb49ffabbed)

        - (1) : 이전 레이어로부터 입력데이터가 들어온다. 이때는 sigmoid function를 통과하여 0과 1 사이의 값으로 변환됨. 
        - (2) : 들어온 입력은 이번 시점 t의 분석에 얼마나 사용할지를 input gate에서 결정
        - (3) : input gate에는 탄젠트h 함수를 통과하여 -1과 1 사이의 값으로 변환한다. forgetting gate에서는 입력값과 더불어 RNN의 또 다른 입력인 이전시점의 hidden state의 정보를 **지울것인지 말것인지** 결정
        - (4) : 이때, sigmoid function이 적용되어 0과 1 사이의 값으로 변환된다. forgetting gate의 값이 0에 가까울수록 많은 정보가 삭제되고, 1에 가까울수록 정보를 그대로 유지. 앞선 input gate와 forgetting gate의 정책에 따라 memory cell a의 정보를 업데이트 함.
        - (5) : input gate의 값과 forgetting gate의 값을 더하여 memory cell의 값이 결정되고, 이 값은 다음 시점 t+1에 LSTM cell로 넘겨짐. 이 과정이 LSTM의 Long-Term Memory를 결정하는 과정. Output gate는 LSTM의 Short Term Memory와 Long Term Memory를 어느 비율로 출력할지를 결정하는것을 담당. 우선 입력 데이터를 받아 sigmoid function를 통과하고 0과 1사이의 값으로 변환
        - (6) : 또한, Long Term 정보를 담고 있는 memory cell a의 값에 탄젠트h(tanh) 함수를 취하여 -1과 1 사이의 값으로 변환. 이 두 값을 더하여 시점 t의 출력 데이터를 결정.

2. GRU 
    - LSTM의 성능을 유지하면서도 구조를 단순화시킨 모델

## 4-3. Sequence-to-sequence model
: 번역 task에 주로 사용하는 모델

- 입력된 sequence로부터 RNN모델을 통하여 정보를 추출하고, 추출된 정보를 바탕으로 다른 RNN모델을 이용해 다른 도메인의 sequence로 출력하는 모델.
    - ex)한국어-> 영어로 번역모델, 대화일때는 질문이 들어오면 대답을 내뱉는 챗봇 agent

- 영어-불어로 번역하는 seq-to-seq번역모델 예시

    ![image](https://github.com/user-attachments/assets/c16a51fd-0474-49eb-b643-6bcef6b68457)

    - 인코더와 디코더로 구성
    - 각 모듈은 LSTM기반의 RNN모델로 구성
    - 인코더에 해당하는 RNN모델은 Many-to-one형태를 취하고 있고, 입력으로 영어 문장을, 출력으로는 입력받는 모든 데이터를 하나의 벡터로 압축해둔 context를 내뱉는다.

    - 디코더에 해당하는 RNN모델은 Many-to-many형태를 취하고 있고, 입력으로 인코더에서 추출된 context vector와 이전 시점의 디코더 출력 단어를 받으며 출력으로 번역된 불어를 내뱉는다.이러한 번역 모델의 구현을 위해서는 문장의 처음과 끝을 지정해줄 수 있는 특별한 tag가 필요한데, 여기서는 <sos>와 <eos>가 그 역할을 담당함.

## 4-4. 의료데이터 활용사례

- 시간특성을 고려할 수 있는 RNN모델을 통해 다양한 임상 의사결정 지원시스템 개발가능.

- JAMIA에 발표된연구(2017년) : RNN모델을 이용하여 12~18개월 분의 EMR 로부터 onset심부전을 예측할 수 있는 모델을 개발하고, AUC 0.77 ~ 0.833수준을 달성
    - 기존의 전통적 ML 혹은 multilayer perceptron모델보다 훨씬 정확한 성능.
- MLHC,2016 : longitidinal patient record로부터 **환자의 다은 event를 감별진단** 할 수 있는 RNN기반의 인공지능 모델 제안

- IEEE, JBHI, 2020 (paper) : RNN모델을 이용하여 웨어러블 장비에서 실시간으로 ECG를 분석하고 **심박동을 분류**할 수 있는 인공지능 모델 개발
    - 5개의 레이블에 대해 98% 이상의 정확도를 갖는 모델.

- CBM,2018 : CNN과 RNN을 함께 활용하여 98%의 정확도로 ECG에서 **부정맥을 예측**할 수 있는 인공지능 모델 제안

# 5. 생체 신호 데이터 분석 기법 : Transformer

1997년 RNN계열의 모델인 LSTM, 그리고 1998년 CNN계열의 모델인 LeNet이 발표된 이후 RNN과 CNN은 각각 자연어 처리 분야와 이미지 인식 분야에 효과적임이 밝혀지며, 인공지능 모델을 대표하는 양대 산맥이 됨.

- Transformer : Attention mechanism이라는 연산 방식을 개선한 self-Attention기법
    - 자연어 처리 분야에서 압도적 성능과 연산 효율성을 보이며 RNN모델의 역할을 대체함.
    - 이미지 인식 분야에서도 CNN을 대체할만큼 뛰어난 성능가짐.

## 5-1. Attention 메커니즘

RNN기반의 seq-to-seq 모델은 인코더에서 context vector를 추출하고, 디코더에서 context로부터 새로운 값을 예측하는 형태로 구성. 그러나 이러한 모델 구조는 문제점이 있음. 1. 모델이 깊어질수록 학습이 잘 이루어지지 않는 gradient vanishing문제 발생

- RNN모델학습
    - RNN모델은 학습 시 back propagation과정을 통해 gradient를 계산하고 가중치를 업데이트 하게 되는데, 모델이 깊어질수록 gradient가 점점 작아져서 결국 가중치들이 제대로 업데이트가 되지 않게 된다.

- sequence-to-sequence 모델
    - RNN기반의 구조를 취함.
    - RNN 모델의 문제점인 모델이 깊어질수록 학습이 잘 이루어지지 않는 gradient vanishing문제가 있음.
    - 인코더 성능과 context사이즈에 따라 성능 차이가 많이 발생.
    - context 사이즈가 너무 작거나, 인코더 성능이 충분하지 못하면, 입력 시퀀스를 제대로 압축하여 충분한 정보를 추출하지 못하고, 디코더의 성능도 이제 제한되게 됨.

    - 위의 문제점들은 번역 task에서 큰 한계점이다.
        - 번역을 요하는 입력 길이가 길어지면 성능이 급격하게 떨어지게 되고, 번역을 위한 RNN모델을 마냥 깊게 만들수 없다 보니 그 성능도 제한되게 됨. 
        - 이러한 문제점 해결위해 Attention 메커니즘이 발표됨.

- RNN vs Attention 메커니즘 비교

|기존RNN 모델|Attention 메커니즘|
|:---:|:---:|
|시점 t의 단어를 번역하기 위해 앞선 시점인 t-1이나 혹은 t-n의 이전 일부 시점들의 정보만을 고려하여 번역 수행 |시점 t의 단어 예측 시 직전 시점의 데이터 및 입력 문장 전체를 고려하여 번역하도록 설계|

- Attention메커니즘?
    - 디코더에서 출력 단어를 예측하는 시점 t에서 t-n ~ t-1의 이전 시점 데이터만 참고하는 것이 아니라 **인코더 전체 입력 문장을 참고하여 예측**

    - 시점 t에서 출력 문장 예측을 위해 입력 문장을 참고할 때, 해당 시점에서 **예측해야할 단어와 관련있는 특정 단어 혹은 부분에 좀더 집중하여 연산을 수행**

    - Attention 메커니즘 이해를 위한 기초지식 : dictionary(Hash table)의 자료 구조
        - 이 자료구조는 key-value형태의 값을 저장할 수 있고, 말 그대로 사전과 같이 단어, 키와 그 설명 밸류를 정의할수 있음. 
        - 여기서 key는 중복될 수 없으며, dictionary내에는 순서가 존재하지 않음.
        - dictionary에서 key를 알면 value를 찾아낼 수 있음.

    - Attention 메커니즘 이해를 위한 기초지식 : Query 행위
        - **Query를 통해 데이터베이스에 정보를 요청하는것.**
        - 우리가 구축해둔 딕셔너리를 하나의 작은 데이터베이스라고 생각하면 query를 통해 딕셔너리에 정보를 요청하게 되고, 우리는 **dictionary의 key를 통해서 해당되는 결과(value)를 찾아 그 값을 반환**해줄 수 있다.

- Attention 메커니즘의 3가지 요소
(번역 task로 대입하여 생각해보자.)
1. Query
    - 시점 t에서 decoder cell의 hidden state
    - ex) 현재 시점에서 출력해야할 단어를 의미
2. key
    - 인코더 셀의 hidden stae
    - ex) 입력 문장의 각 단어들
3. value
    - Query와 key에 mapping되는 값
    - ex) 시점 t의 단어를 예측과정에서 인코더의 hidden state가 얼마나 유사한지, 즉 query가 주어졌을때 각 key들의 어느정도로 영향도를 갖고 있는지를 나타내는 값.

![image](https://github.com/user-attachments/assets/522b53ad-60c2-4d3a-b23b-e300a6abfd72)

- 시점 t에서 예측하여야 할 단어, 쿼리가 결정되면 입력으로 들어왔던 문장의 단어들, 키를 다시 살펴보면서 이번 출력단어와 모든 입력 단어 사이의 유사도 혹은 가중치 밸류를 계산 하여서 이번에 단어를 출력할 때, 어떤 입력 단어를 더 많이 참고하여 출력할지를 결정.

![image](https://github.com/user-attachments/assets/5e883eb8-1806-45e6-98c7-1ad2e2d50421)

- 현재 번역하여야 하는 시점 t의 단어가 입력 문장의 앞 쪽의 단어와 더 높은 유사도를 갖고 있다고 판단이 되면 인코더의 앞쪽 부분의 hidden state에 값을 더 가중하여 분석하고 예측을 진행하게 됨.

이러한 메커니즘은 번역문제에서 매우 큰 성능 향상을 가져옴. 특히 한국어와 영어의 번역 문제와 같이 문장의 어순이 다른 언어 간의 번역에서 매우 효과적임.

- 초창기 연구 : RNN기반의 sequence-to-sequence에 attention mechanism을 추가하는 방향으로 연구가 이루어짐
    - attention 기반 RNN모델 : 학습과정에서 RNN모델의 가중치와 더불어 attention value를 함께 업데이트하며 학습
    - attention 메커니즘의 방법론

    |이름|논문|
    |:---:|:---:|
    |content-base|Graves, Alex, Greg Wayne, and Ivo Danihelka. "Neural turing machines." arXiv preprint arXiv:1410.5401(2014)|
    |Additive|Bahdanau, Dzmitry, Kyung Hyun Cho, and Yoshua Bengio. "Neural machine traslation by jointly learning to align and translate." 3rd International Conference on Learning Representations, ICLR 2015. 2015.|
    |Location-base| Luong, Thang, Hieu Pham, and Christopher D. Manning. "Effective Approaches to Attention-based Neural Machine Translation." EMNLP. 
    |General| " |
    |Dot-Product| " |

## 5-2. Transformer의 전체 구조


2017년 구글에서 발표된 Attention is all you need라는 논문을 통해 공개된 모델로 기존의 어텐션 활용 논문들이 RNN기반 모델에 어텐션 메커니즘을 추가한 반면에, (Attention is all you need) RNN구조를 사용하지 않고 어텐션 만으로 모델을 개발하여 번역 문제에서 그 당시 압도적인 SOTA(State-of-the-art)를 달성


- 구조

    ![image](https://github.com/user-attachments/assets/9e0d49bb-bb17-45ee-b837-3055eef9af14)

    - 기존 seq-to-seq와 동일하게 인코더와 디코더로 구성
    - 다만 차이점은?
        - 입력이 시간 순에 따라 순차적으로 들어오지 않고 한꺼번에 들어온다는 점
        - encoder에서 추출된 context vector가 decoder의 입력 단으로 들어가는 것이 아니라 중간에 들어간다는 것.

## 5-3. Self-Attention의 개념

![image](https://github.com/user-attachments/assets/b14c855d-cf77-4103-acb2-d7c7fdd4c8e8)

- Attention에는 query, key, value가 필요.
- 주어진 query > Key-Value pair를 통해 원하는 단어(query)와 입력 데이터( key)의 연관성 파악 => 이 연산은 많은 입력 데이터 중에서 분석에 실질적으로 필요한 부분에 집중하게 하여 성능향상가져옴.

- self-attention?
    - Attention을 자기 자신에 대해서 수행한다는 의미.
    - Attention의 주어진 단어에서 관련 있는 다른 단어에 집중해 분석 > 인공지능 모델이 입력 문장 자체를 더 잘 이해하도록 만들고 좋은 feature추출

- 기존 attention 메커니즘 vs self-attention

|기존 attention 메커니즘|self-attention|
|:---:|:---:|
|예측하고자 하는 시점 t의 단어가 명확|Query,key,value모두 자기 자신으로 동일|
|Query가 디코더 셀의 hidden state가 되고 key가 인코더 셀의 hidden state가 되어 서로 다른 값을 가짐|문장 내 특정 단어에 대한 feature추출 시 문장의 문맥을 모두 고려하여 feature를 추출|

- 예시

    ![image](https://github.com/user-attachments/assets/f22e2a49-0ab4-496e-8e83-33f2f04011f5)

    - 비슷한 두 문장이지만 it이 가르키는 대상이 다르다.
    - self-attention사용시 전자에는 it이 animal에 더 많은 가중치를 가지지만, 후자에서는 street에 더 많은 가중치를 가진다.

Transformer model에서는 **현 시점 t의 출력 단어 예측** 및 **주어진 입력 데이터 분석**할때도 Self-Attention을 이용하여 **입력 데이터 전체를 고려한 feature를 좀더 효과적으로 추출**

- self-Attention의 활용범위
    - 번역에 사용
    - 자연어 처리에 사용
    - 다른 인공지능 모델에 사용

## 5-4. Multi-Head Attention과 Scaled Dot-Product Attention

Transformer에서 가장 중요한 연산은 트랜스포머 구조그림에서 주황색으로 표시된 모듈인 Multi-Head Attention 모듈이다. 이 모듈에서 실질적인 어텐션 연산과 입력데이터를 분석하는 연산이 이루어지게 된다.

- Multi-Head Attention 구조

    ![image](https://github.com/user-attachments/assets/6a951e52-f70f-4d85-8475-b7a2584e769d)

    - 기존 어텐션 메커니즘과 동일하게 Query, key, value가 입력으로 들어옴.
    - 들어온 입력은 Linear 층을 통해 특징 추출이 이루어지게 됨.
    - Query, Key, value의 feature들은 Scaled Dot-Product Attention을 거친 뒤 다시 한번 Linear층을 통과함.
    - Scaled Dot-Product Attention : 다양한 Feature 추출함
        - 여러개의 head를 이용해서 다양한 피쳐를 추출.
        - 컨볼루션 레이어에서 커널을 여러개 사용하여 다양한 특징 정보를 추출 할 수 있도록 하는 것과 비슷한 원리.
    
    - 여러개의 Scaled Dot-Product Attention으로부터 나온 결과는 하나로 합쳐져(Concat) Linear layer를 통과함.

- Scaled Dot-Product Attention 세부구조

    ![image](https://github.com/user-attachments/assets/e6a87c55-717c-445f-8fbe-8abcf84d018d)

    - 이 부분이 **실질적인 Attention 메커니즘을 수행하는 모듈**
    - Query와 Key 값의 행렬곱을 수행하여 어텐션 스코어를 계산하고 스케일링을 한 뒤에 SoftMax함수를 통과함.
    - 이렇게 얻어진 결과를 attention distribution이라고 하는데, 쿼리가 주어졌을때 각 키들이 얼마나 연관되어 있는지를 나타내는 값.
    - attention distribution : value가 곱해져서 attention value를 계산할수 있음.

    예시를 통해 Scaled Dot-Product Attention 개념을 좀더 보자.
    여기서는 앞서 설명한 셀프 어텐션 과정에서의 연산을 살펴보자.

    ![image](https://github.com/user-attachments/assets/445106ed-159a-411c-86a0-bad791f2041e)

    - 주어진 입력문장 i am a student느 쿼리와 키에서 모두 사용되고, 두 문장의 행렬 곱을 수행
    - 실제 컴퓨터연산에서는 각 단어가 정해진 차원으로 임베딩 되어 하나의 문장은 임베딘 된 벡터의 집합으로 들어오게 됨.
    - 최종 초록색 표가 attention score matrix가 되고, 여기에 softmax function을 취하여 attention distribution을 구하게 됨.

    ![image](https://github.com/user-attachments/assets/da5cfef5-9069-4edf-b8ad-99105cb50855)

    - dk는 트랜스포머의 입출력 크기를 head의 개수로 나눈것.
    - 입출력 크기와 헤드의 개수는 모두 트랜스포머의 사용자정의변수인 하이퍼파라미터로, dk는 쿼리 벡터와 키 벡터의 내적 값을 스케일링 하는 용도로 사용됨.

    - 정해진 어텐션 스코어의 소프트맥스 함수와 스케일림 함수를 겹쳐 attention distribution을 구하고, 여기에 value 행렬을 곱하여 attention value matrix을 만듬.

    - **self-attention을 거쳐 나온 결과로 attention value matrix는 우리의 입력 데이터와 동일한 크기를 갖는다.**

        - 이러한 특징은 self-attention module이 기존의 다른 인공지능 모듈과 잘 어울려 사용될 수 있도록 하는 좋은 특징임.
    
    ![image](https://github.com/user-attachments/assets/8edafc93-2f38-4e7e-8772-64a026027d4c)
    
    - multi-head attention은 위의 scaled Dot-Product Attention연산을 정해진 head개수만큼 독립적으로 수행

    - 큰 사이즈의 입출력 사이즈로 한번에 attention을 수행하는 것보다는 여러개의 attention을 병렬로 수행하는 것이 더 효과적.
    - 논문에서는 8개의 head를 사용하였을때, 성능이 우수. 이때 각 head에 존재하는 가중치 값들은 모두 독립적!



Mask연산 (in scaled Dot-Product Attention)

- RNN 모델의 경우, 단어가 순차적으로 입력되어 들어오기 때문에 문장 끝/문장 시작(<eos>, <sos>) tag 를 통하여 문장을 구분하였음.

- 그러나 Transformer에서는 문장 전체를 한꺼번에 입력으로 받고 분석하므로 문장 끝의 명확한 명시 필요.

    - 이때 문장이 끝나 분석할 필요가 없는 부분을 mask하여 모델이 분석하지 않도록 함.
    - mask 값은 매우 작은 음수값 사용
        - 마스킹된 위치의 값은 softmax function을 거치고 value matrix와 곱해지는 과정에서 0에 매우 근접한 값이 되고, 모델은 입력 문장을 분석할 때 이를 고려하지 않게 됨.
    - Masking은 트랜스포머 디코더의 가장 첫번째 층에서 Masked Multi-Head Self-Attention을 수행할 때도 사용됨. 
        - RNN은 시간 순서대로 데이터가 들어오는 반면, 트랜스포머는 입력값이 한꺼번에 들어옴.
        - 디코더의 입장에서는 시점 t의 단어를 예측할때, 시점 t+1의 단어, 즉, 자기 자신보다 미래에 있는 단어를 참고할 수도 있음. 이를 방지하기 위해 시점 t의 예측을 수행할 때는 시점 t이후에 모든 데이터에 마스킹을 수행하여 그 모델이 참고하지 못하도록 함.
    
## 5-5. Positional encoding

기존 RNN 모델에서는 데이터가 순차적으로 입력되기 때문에 자연스럽게 단어의 위치 정보를 알수있음. 그러나 트랜스포머의 연산에서 한꺼번에 들어온 입력 데이터를 분석하는 과정에서 단어의 위치 정보가 모두 사라짐. 따라서 단어의 위치 정보를 별도 방식으로 명시적으로 알려주어야 함.

![image](https://github.com/user-attachments/assets/c0d107b5-cf9c-4339-9b2b-68e9cadc474f)

- 인코더와 디코더의 각 입력 단에서 positional encoding결과를 입력 단에 더해서 사용하게 됨. 

- 일반적으로 positional embedding을 위해서는 sin함수와 cos함수가 사용됨

* ? 왜 positional embedding에 주기함수를 사용할까?  (찾아봐!- 트랜스포머 논문에 있음.)



## 5-6. Skip connection = Residual connection

![image](https://github.com/user-attachments/assets/bc95e124-baef-4eb5-8f82-5b68b76c8406)

- 입력데이터에 연산을 취하고 나서, 그 결과에 입력 데이터를 한번 더 더해주는것을 의미.
- 즉, 다음 layer로 정보를 넘겨줄때, 함수 F에 의해 변형되어 새롭게 생성된 정보만을 넘겨주는것이 아니라, 원본 입력 데이터도 함께 넘겨주는것.

- 인공지능 모델이 깊어질수록 학습해야할 파라미터가 많아지고, 최적화가 어려워지는 Gradient vanishing 문제가 발생하는데, skip connection을 모델에 추가해줌으로써 Gradient가 Shotr-Cut path를 통하여 전파되도록 학습 효율성을 높여줌.

- LesNet,2016 논문에서 적용된 방법이며, CNN연구에서 많이 쓰임.

## 5-7. Transformer의 Encoder와 Decoder의 구조

![image](https://github.com/user-attachments/assets/9e0d49bb-bb17-45ee-b837-3055eef9af14)

- 인코더
    - inputs 데이터를 Positional Encoding한 뒤 self-attention을 수행.
    - 이 때, **self-attention module은 hyper-parameter인 h의 개수만큼 존재.**
    - self-attention의 결과는 skip-connection으로 연결된 입력 데이터가 더해져 정규화를 거치고 feed forward neural network로 보내짐.
    - Feed Forward neural network를 통과한 뒤, 마찬가지로 Skip-connection으로 연결되는 입력 데이터가 더해져 정규화를 거친 뒤 Context Vector 생성
    - **인코더는 hyper-parameter인 N개수만큼 존재.**
- 디코더
    - 입력데이터를 positional Encoding 한 뒤 Masked Multi-Head Attention을 수행
    - 이때, 모델의 자기 자신보다 미래의 데이터를 참조할 수 없도록 **시점 t이후의 데이터를 모두 마스킹하는 작업이 수행됨.**
    - 여기서도 skip-connection이 존재하고, 정규화 layer을 통과.
    - 디코더는 인코더의 입력을 고려해서 시점 t의 단어 예측을 수행할 차례임. 그래서 두번째 multi-Head Attention은 **Self-Attention이 아닌 일반 Attention이 사용됨.**
    - 이때는 디코더의 첫번째 layer의 결과값, 즉 Masked Self-Attention결과가 쿼리로 들어오고 인코더에서 추출된 Context vector가 key와 value입력으로 들어옴.
    - 마찬가지로 Skip-connection 입력을 더하고 정규화를 통해 디코더의 fully-connected neural network layer로 보내지고, 이 fully-connected neural network layer을 통과한 뒤 다시 Skip-connection과 정규화를 거쳐 최종 Dense layer를 통과하고 예측 결과를 출력.

## 5-8. Self-Attention과 Transformer의 응용

Transformer논문이 높은 인기를 보이는 이유는 Self-attention의 뛰어난 성능 때문. Self-Attention은 기존의 Fully-connected neural network나 CNN 혹은 RNN이 보여주지 못했던 global한 특징 추출에 있어 연산 대비 성능이 탁월함. 또한, 자기 자신의 정보만을 이용한다는 점에서 추가적인 데이터 확보와 노력이 불필요하고, Self-Attention의 출력 값의 크기가 원본 데이터와 동일하다는 특징 때문에 다양한 문제들에 차용되어 사용됨.

- (paper) Non-local Neural Network(Wang et al., 2018)
    - Self-Attention 연산을 이미지에 맞게 변형, 비디오 분석분야에 큰 성능 향상

    - 기존 CNN모델
        - locality 특성 존재
        - 이미지에서 Kernel을 Sliding하는 형태로 특징을 추출하다 보니, 특정 픽셀 주변의 픽셀 정보는 잘 추출할 수 있으나, 극단적으로 멀리 떨어진 상호 관계에 대해서는 모델링 불가

    ![image](https://github.com/user-attachments/assets/2ec061cf-e2fd-4b37-98bd-6ee4753c93e6)

    - 논문에서 제안한 non-local block은 연산 양이 크지 않으면서 기존 CNN의 중간에 끼워 넣어 모델에 주어진 이미지에 전체적인 글로벌 픽셀 릴레이션을 추출할 수 있도록 하였음.
    - 이미지간 행렬곱을 통해 모든 픽셀간의 연관관계를 추출할 수 있도록 한것. 

    ![image](https://github.com/user-attachments/assets/cf641fff-a90c-41a7-a66a-4df2f7f6ccfb)

    - 이러한 연산은 위 그림과 같이 프레임간 연관관계가 중요한 비디오 분석에서 주효하였고 좋은 성능을 보임.

- Transformer기반 인공지능 패러다임 변화

    - BERT (Bidirectional Encoder Representations from TRansformers, 2019)
        - 트랜스포머로 encoder 파트만을 활용하여 대규모 언어모델을 개발
        - BERT를 이용한 인공지능 모델로 각종 자연어 처리 문제에서 SOTA 달성
    - GPT(Generative Pre-trained Transformer, 2018)
        - 트랜스포머의 decoder 파트만 활용하여 대규모 언어모델 개발
    
    - Vision Transforemr(Vit,2021)
        - 트랜스포머 기반의 모델로 이미지 인식 분야에서 압도적인 성능보여줌.

        ![image](https://github.com/user-attachments/assets/b806be1b-ee9b-443b-9730-65c7f83f526d)

        - 이미지를 16x16 size를 가지는 patch로 분할하여 시퀀스 형태로 입력을 만듬.
        - 마치 이미지를 자연어 처리 분야에서 단어가 입력되는 형식과 비슷하게 변경함으로써, 기존 트랜스포머의 인코더 구조를 최대한 비슷하게 활용하고자 함.
        - 연산효율성도 기존 CNN기반 모델보다 크게 축소.

# 6. EMR 정형 데이터 분석기법
: Decision Tree 기반 Ensemble 모델 과 순환신경망모델(RNN, Recurrent Neural Network) 이 가장 우수한 모델이다.


## 6-1. 의사결정나무와 Classification and Regression Tree(CART)

### 6-1-1. Decision Tree 기반 Ensemble 모델

- 의사결정나무 : 데이터의 입력변수들과 이에 해당하는 값들을 탐색하여 데이터 포인트들을 원하는 목적에 맞게 나누는 모델

    - 종류 :  ID3, CHAID, C4.5, C5..0, CART 등 다양한 종류가 있으며 모델이 학습되는 과정, 모델의 형태는 유사함.

    - Random Forest 와 Gradient Boosting Model은 CART를 혼합하여 만들어진 모델.
- Classification and Regression Tree(CART) 
    - 분류와 회귀가 가능한 의사결정나무 모델
    - 의사결정나무 계열의 모델들과 ensemble에 대해 많은 업적을 남긴 Leo Breiman이 제안한 모델

    - 학습과정설명

        ![image](https://github.com/user-attachments/assets/4a9b1946-395c-4521-ad5e-7589f3ac9d13)

        - CART는 다음과 같이 학습됨.
        - 3개의 입력변수 :  집 소유, 결혼, 연소득 
        - 이를 바탕으로 채무 불이행 사람과 이행하는 사람을 분류하는 CART모델을 만든다고 가정.
        - 파란색 공 : 채무불이행
        - 주황색 공 : 채무이행
        - 이러면 마치 10개의 공이 한공간에 있다고 볼수 있음
        - CART모델은 해당 그림과 같이 생성됨.
        - CART모델은 하나의 변수와 기준값을 정하여 데이터를 2개 공간으로 분할함. 이 때, 2개의 공간에서 가능한한 같은 레이블을 갖는 케이스가 분포하게 만드는 변수 및 기준값을 탐색하여 알고리즘 결정.
        - 최종적으로 레이블을 결정하는 판단은 마지막 공간에서 이루어짐.
        - 레이블을 분류하는 Classification이 목적인 예시임.

        - 뿐만아니라, 연속적인 값을 예측하는 회귀를 목적으로도 이와 비슷한 과정으로 모델 생성가능.
    - 단일 CART모델의 장/단점
        - 장점 : 상당히 직관적인 방법으로 케이스의 정답을 예측하므로 설명력이 좋음
        - 단점 : 기존 다른 머신러닝 모델들보다 낮은 예측 성능을 보임
    - CART모델을 여러개 생성하여 혼합하는 Ensemble수행시 매우 뛰어난 성능 보임.

## 6-2. Rnadom Forest
: 앙상블 기법 중 배깅(Bootstrap Aggregating, Bagging)사용한 기법

- 2010년대 중반까지 테이블 데이터에 대해 가장 좋은 성능을 보이는 모델
- 학습과정에서 2번의 랜덤화 과정이 적용됨.
    1. 그중 첫번째는 bagging.
        - bagging에서는 학습 데이터에서 각 케이스를 무작위 복원 추출하여 bootstrap dataset을 생성
        - 무작위 복원 추출을 여러번 반복하여 여러개의 부트스트랩 데이터셋들 생성
        - 그 후 각 부트스트랩 데이터셋에 CART모델을 학습
    2. 랜덤화 과정
        - 입력 변수의 무작위 추출.
            - 앞서 CART모델이 학습 될때에는 매번 데이터를 분할할 때마다 최적의 입력 변수와 기준값을 정함. 이를 위해서는 분할할때마다 내가 갖고 있는 입력 변수들을 일일히 탐색하여 최적의 경우를 결정해야 함.
        - 탐색하는 입력 변수의 후보를 무작위 추출
            - 만약 갖고 있는 입력변수가 5개이면 이중, 3개의 입력 변수를 무작위로 뽑는것. 이후 뽑은 3개의 입력변수에 대해서 탐색을 하여 가장 케이스들을 잘 분할하는 입력 변수를 선정.

        -  각 부트스트랩 데이터셋 에서 생성된 CART모델들이 서로 비슷하게 생성되지 않게 만듬.
            - 유사하게 생성되는 모델들을 혼합하여 예측을 하는것보다, 어느정도 서로 각자의 특성을 갖는 모델을 혼합하여 예측하는 것이 데이터 내 복잡한 관계를 더 잘 찾아내고 예측성능에도 도움됨.

- Random Forest 값 찾기
    - ex) 

        ![image](https://github.com/user-attachments/assets/31a96c6a-f736-4220-9aaa-95045321e51a)

        1. CART 모델이 100개인 랜덤포레스트 생성
        2. 새로운 데이터 등장 시 이 데이터를 100개 CART 모델에 투입하고 각각의 예측된 결과 추출
        3. Random Forest는 각 CART 모델이 예측된 결과의 평균값 혹은 최빈도값을 최종으로 도출

## 6-3. GBM(Gradient Boosting Model)
앙상블 방법중 Boosting이라는 방법 사용

- 배깅 vs boosting
|bagging|boosting|
|:---:|:---:|
|여러개의 모델을 평행하게 생성하고 결과를 혼합하는 방법|순차적으로 모델을 학습하게 됨|


- GBM모델의 원리

    ![image](https://github.com/user-attachments/assets/d79e74d1-a2ee-4bf5-bbb2-91e2a342ab93)

    - step1) 현재 주어진 데이터를 학습하여 CART모델을 하나 생성.
        - 이때, 모델을 일부러 약하게 학습을 하여 현재 데이터의 어느 정도의 error를 갖게 만드는데, 이를 "약한 학습자" 라고 부름.
    - step2) 두번째 CART모델을 생성할때는 이전 모델에서 에러가 큰 케이스에 좀 더 큰 가중치르 두고 약하게 학습 진행
        - 두번째 생성된 모델은 이전의 error를 줄이는데 좀더 초점을 두지만 약한 학습을 수행하여 또 다른 error를 갖는 케이스들이 있을것이기 때문. 이 error가 큰 케이스들은 다음 단계에서 다시 가중치를 주는것.
    - 이과정을 반복하면 결국 데이터 내 전체적인 error가 감소한다는 것이 Boosting의 핵심이다.

모델과 학습 방법의 디테일 차이에 따라 GBM도 여러 개의 모델이 존재.

- XGBoost모델
    - GBM모델중 가장 유명한 모델
    - 기존 Gradient boosting model의 순차적 학습을 컴퓨터에 분산학습을 할 수 있는 방법 제시
    - 몇가지 튜닝을 통해 학습속도를 크게 개선.
- LightGBM
    - GBM의 학습속도를 더 빠르게 개선하는데 초점
- CatBoost
    - 범주형 변수가 많은 데이터의 학습을 개선하는데 초점

위 3가지 모델은 지속적 업데이트로 인해 성능차이가 크게 없다. 각자 패키지 형태로 버전 관리되고 있으며, CPU 및 딥러닝과 마찬가지로 GPU환경에서 동작할수 있게 발전됨.

## 6-4. 의사결정나무 계열의 모델들이 EMR데이터에 많이 사용되는 이유
- 설명력을 가지기 떄문
    - 단일 CART 모델
        - 자세한 규칙으로 결과 예측 가능
    - Random Forest, GBM
        - 자세한 규칙으로 결과를 예측하는 것을 활용하여 각 "변수 중요도" 계산 가능. 이로인해 데이터 내 어떤 변수가 더 큰 영향력을 발휘하고 있는지 볼수 있음.
    - SHAP(SHapley Additive exPlanations)
        - 변수중요도가 일관되게 계산되지 않는 다는 단점을 해결.
        - 입력데이터의 변화에 따라 인공지능 모델의 예측이 어느정도 영향을 받는지 표현
        - 인공지능 모델을 설명하는 방법 제시
        - [SHAP 깃허브저장소](https://github.com/slundberg/shap)
        - (paper) From local explanations to global understanding with explainable AI for trees, Nature Machine Intelligence, 2020 참고.

# 7. 비정형 텍스트 분석 기법

2018년 이후 등장한 사전학습 기반의 기계학습 모델들은 문서분류와 같은 고전적인 자연어 처리 task뿐 아니라, 복잡한 자연어처리(감정분석,질의응답, 챗봇 등) task들에 대해 사람과 유사한 성능을 낼수 있을정도로 빠르게 향상되고 있음.

## 7-1. 텍스트 분석모델

- 단어 임베딩
    - 자연어 처리에서 단어는, 데이터셋에 등장하는 모든 단어를 사전화한 후, 사전상의 순서 혹은 색인으로 치환하여 처리함.

    - 사전에 등장하는 단어를 vector로 치환하는 가장 간단한 방법은?
        - 총 단어 수만큼의 차원을 가진 vector에서 다른 모든 값은 0, 단어 순서에 해당하는 원소만 1로 표시.
        - one-hot-encoding
            - 1: 사과 2: 회사 3: 식사
            - 회사를 one-hot-encoding으로 변환하면 (0,1,0) 이 된다.
            - 단어 문서행렬에 각 행, 즉 문서를 나타내는 vector는 그 문서를 이루는 단어 vector들을 모두 더한것과 같다.
        - one-hot-encoding의 한계점
            - 표현이 희소(sparse)하므로, 사전 내 단어 수가 늘수록 고차원 벡터가 필요함.
            - 단어의 표현형만을 고려하므로 단어 자체의 의미를 담을 수 없음.

        - 이를 해결하기 위해 단어 임베딩 방법론 등장 => Word2Vec 

- Word2Vec

    - Word2Vec으로 임베딩 된 단어들의 예시

    ![image](https://github.com/user-attachments/assets/5698b4ee-ead6-4923-a1d8-8f340da6dd42)

    - Word2Vec은 언어모델을 학습하면서 단어 임베딩을 만드는데, 이때  언어모형(language model)이란 이전에 나온 단어들을 바탕으로 다음 단어를 예측하는 모델을 가르킴.
    - 분류 문제와 다르게 별도의 레이블이 없이 텍스트만 있어도 학습 가능

    - WOrd2Vec 방식
        : CBOW와 Skip-Gram 2가지 방식이 있으며, 모두 **대상 단어와 주변 단어의 관계를 이용**

        ![image](https://github.com/user-attachments/assets/fbaa2703-e8ba-4e1b-b955-c11ee59cf304)

        1. CBOW(continuous bag-of-words) : 주변 단어 임베딩을 더해서 대상 단어를 예측
        2. Skip-Gram : 대상 단어의 임베딩으로 주변단어를 예측

        - ex) "의료 인공지능은 **새로운** 분야이다"
            - **새로운**이 모델링 하고 싶은 대상이라고 가정해보자.
            - 여기에 윈도우를 좌우 1단어라고 하면,
            - 주변 단어는 "인공지능은", "분야이다"가 될것이다.
- Sequence modeling
: 시퀀스의 2가지 중요한 성질은 1. 시퀀스 구성 요소들간의 순서정보 2. 가변적인 길이를 모델링하기 위해 순환신경망(RNN)이 등장함
    - 순환신경망(RNN)
        - 시퀀스 모델링의 가장 기본적 모델
        - 은닉층의 노드에서 활성화 함수를 통해 나온 결과값을 출력 층 방향으로도 보내면서 다시 은닉층 노드에 다음 계산의 입력으로 보내는 특징을 가지고 있음.
        - RNN의 은닉층에서 활성화 함수를 통해 결과를 내보내는 역할을 하는 노드를 cell이라고 한다.
        - Cell ? 이전의 값을 기억하려고 하는 일종의 메모리 역할을 수행하므로 memory cell 또는 RNN cell 이라고 표현
        - 은닉층의 메모리 셀은 재귀적으로 작동
        - 각각의 시점(time step)에서 바로 이전 시점에서의 은닉층의 메모리 셀에서 나온 값을 자신의 입력으로 사용.
- LSTM 과 GRU
    - 이론상 RNN : 임의의 길이의 시퀀스 모두 인코딩 가능 
    - 실제 RNN : 시퀀스 길이가 너무 길 경우 과거의 시퀀스 정보 소실하는 단점 존재. 이러한 단점 해결하기 위해 LSTM, GRU가 제안됨

    - RNN vs LSTM,GRU 차이점
        - 들어온 입력과 과거 정보 중 어떤 것을 더 보존할지 결정하는 관문(gate)존재.
        - 이 관문들로 인해 과거 정보를 효과적으로 보존가능
    - LSTM 구조

        ![image](https://github.com/user-attachments/assets/3a75b645-cf98-44a5-a25a-2582cd50cda5)


        - 입력과 과거 정보들 사이의 흐름을 통제하는 여러 개의 관문(gate)가 존재함.
- Transformer
LSTM과 GRU에 힘입어 순환 신경망은 긴 시퀀스들을 잘 모델링할 수 있게 되었지만 순환신경망에는 근본적인 문제가 존재.

    - 문제점
        - 순환 신경망은 입력을 재귀적으로 처리하며, 이로 인해 시퀀스의 앞부분부터 뒷부분까지 순차적으로 계산해야 하므로, 여러 개의 연산 장치를 통해 입력 시퀀스를 병렬적으로 연산하는 것이 불가능.
        - Bahdanau의 연구를 통해 특정 은닉층 정보를 강조하는 Attention 메커니즘 사용이 더 좋은 성능 창출.

    - 트랜스포머는 시퀀스를 병렬적으로 처리하며, Attention 메커니즘을 통해 단어 각각의 의미를 모델링하는 모델

## 7-2. 사전학습 기반의 텍스트 분석 모델

- 사전학습(pre-training) : 대규모의 데이터를 활용하여 모델을 미리 학습시키는 방법론

- 사전학습된 모델들은 다른 task를 풀 때 사전학습 때 얻은 파라미터로 모델을 초기화한 후, 문제에 맞게 적응(fine-tuning)시켜 사용되며, 이러한 과정을 전이 학습(transfer learning)이라고 함

2018년 이후 자연어처리는 대규모 자연어 코퍼스에 언어 모델을 사전 학습시켜 사용하는것이 대세. 사전 학습 때 언어모델들은 다양한 자연어 처리문제에서 기존 학습 기반 모델들에 비해 월등한 성능 보임.

- BERT
    - 사전학습 기반 자연어 처리 모델 중에서도 가장 보편적으로 많이 사용되는 모델
    - BERT의 언어 모델 사전학습은 2가지 문제를 해결하면서 이루어짐
        1. 첫째로, 마스킹된 언어모델
            - 사람들이 시험을 보는 방식과 유사하게, 문장 일부분을 빈칸으로 처리 후 **해당 빈칸에 들어갈 가장 적합한 단어를 찾는 방식**으로 학습
        2. 둘째로, 다음 문장을 예측하는것
            - 2개의 문장 쌍이 입력으로 들어왔을때 두 문장이 **의미적으로 연결될 수 있는지, 없는지 구분**하는 문제를 풀게 됨.
    - BioBERT, Clinical BERT
        - 의료인공지능 분야에 최적화된 BERT 모델

- GPT3
    - BERT가 다양한 자연어 처리 분야에서 두각을 나타냈지만 BERT는 기본적으로 **빈칸을 예측하도록 학습되었기에 문장 생서 불가능.**
    - GPT-3는 아주 많은 파라미터와 데이터를 사용하여 모델이 자기 회귀적으로 출력을 생성하는 법을 학습.
    - GPT-3가 생성하는 데이터는 사람과 구분이 힘들정도로 높은 퀄리티의 데이터 생성가능.

# 8. 인공지능을 활용한 흉부X선 영상분석

- 흉부X선 검사
    - 가장 흔한 방사선 검사
    - 폐와 심장계통의 질환에 대해 많은 것을 알려주는 중요한 검사
    - 빅데이터 수집에 유리
    - 오진율이 높고 적시 판독이 안되어 현행 판독체계의 개선 필요성 높음
    - 사진에서의 사물인식을 중심으로 발달되어 온 인공진으 기술이 쉽게 적용될수 있음.

## 8-1. 흉부X선 영상 레이블링의 필요성

![image](https://github.com/user-attachments/assets/9c41e8b0-54f2-4772-adbf-a6ed6546cef5)

- label은 supervision 정도에 따라 strong, semi, weak로 구분가능
    - strong : 병변의 위치와 함께 바운더리를 표시한 경우
    - semi : 병변의 유무 또는 종류와 함께 병변의 위치를 레이블링한 경우
    - weak : 병변의 유무 또는 종류를 레이블링한 경우

## 8-2 사례 : Curriculum learning
: 빅데이터로 수집된 흉부X선의 학습이 쉽도록 도와주는 Curriculum learning

- 해당 연구는 서울아산병원과 분당서울대학교 병원에서 strong supervision으로 레이블링한 데이터를 활요함.

- (paper)  Park et al., "A Curriculum learning strategy to enhance the accuracy of classification of various
 lesions in chest-pa x-ray screening for pulmonary abnormalities," Scientific Reports 2019.

![image](https://github.com/user-attachments/assets/fc8b39b4-ee9c-4968-8cf4-6fe9d003486c)

    - 한장의 흉부X선 영상에는 다양한 조직들이 겹쳐져서 표현되기 떄문에 병변의 크기, 발생위치 등에 따라서 학습이 어려울수 있음.
    - 적은양의 strong-supervision의 데이터를 활용하여 curriculum learning진행한다면 weak supervision으로도 좋은 결과를 얻을수 있었던 사례.

    - strong-supervision 정보를 활용 > patch기반 classifier를 학습 > 이를 pre-trained weight로 활용 > weak-supervision으로 transfer learning함 > image기반 classifier를 학습

![image](https://github.com/user-attachments/assets/185931ce-33c8-4785-8243-09d1b8f65547)

    - strong-supervision으로 학습한 후 위크 수퍼비전으로 학습하는 curriculum learning이 처음부터 weak-supervision으로 학습하는 baseline보다 학습속도가 빠르고 정확도도 높음.

    - 스트롱 수퍼비전이 데이터의 일부만 되어도 활용이 가능함을 보임.

## 8-3 사례 : weak-supervision에서 CAM을 활용한 병변 detection

![image](https://github.com/user-attachments/assets/660e6ece-0f93-42f5-b64f-c7eca8fbc2a1)

- weak-supervision으로 학습되어도 class activation mapping(CAM)을 활용하여 학습된 classifier를 해석해 볼수 있다.

![image](https://github.com/user-attachments/assets/f7ba8c67-1692-4119-acb3-490f2928ec92)

- Curriculum learning으로 학습된 흉부X선의 경우, String-supervision과 CAM이 일치하는 것을 확인할 수 있다.

## 8-4 사례 : 정상 흉부X선 생성모델을 이용한 anomaly detection  

- (paper) Tang et al., "A disentangled generative model for disease decomposition in chest X-rays via normal image syntheisis," Medical Image Analysis

![image](https://github.com/user-attachments/assets/a52c22e1-ec67-417d-86e1-326eb7649f2f)

- 정상 흉부 X선의 빅데이터를 활용하여 생성모델을 학습
- 학습된 생성모델을 통하여 입력 흉부 X선(정상 또는 비정상)의 정상 흉부X선을 생성할수 있게 되고 이를 통하여 anomaly를 detection함.

![image](https://github.com/user-attachments/assets/a6bf2905-cf06-41d7-9233-e3d864472b17)

- 이를 통하여 쉽게 구축 가능한 major abnormal pattern뿐만 아니라 드물게 발생하는 pattern들도 detectioni 가능함.



- 흉부 X선 영상의 공개 빅데이터
    ![image](https://github.com/user-attachments/assets/2a7c6f57-0592-4d7d-a523-c55325802502)

# 9. 인공지능을 활용한 의료영상 생성연구

- MRI 촬영시 2~40분이 걸리고, 환자가 숨을 참고있어야 하며 다양한 불편함이 존재. 이를 긴 촬영시간을 단축시킬수 있다면? 긴숨을 참을 필요도 없고, 보다 편안하고 안정되게 MRI촬영받을수 있음. 또한 병원측에서는 보다 많은 환자들을 신속하게 촬영하여 빠른 진단과 치료를 효율적으로 할수 있게 됨.

## 9-1. MRI 영상획득 기본원리

- 영상이 만들어지는 원리

    - 일반디지털 카메라
        - 카메라의 셔터를 누르면 바로 영상이 나옴.
    - MRI
        - 촬영이 시작되면 영상의 주파수 정보들을 일일이 측정.
        - 모든 주파수 정보들을 측정하게 되면, 그때 비로서 측정된 주파수 정보들을 우리가 이해할수 있는 영상으로 변환가능

    - MRI영상획득단계
        - step1) 영상의 주파수 정보 획득 단계
            - 온전한 MRI영상 만들기 위해 획득되어야 하는 주파수의 정보들이 많음.
            - 주파수 정보가 순차적으로 얻어지기 때문에 전체적인 MRI촬영시간이 상당히 길어짐.
        - step2) 획득된 주파수 정보를 영상으로 변환하는 단계
            - 주파수 변환 알고리즘을 사용하여 주파수 정보들을 영상정보로 변환

- MRI촬영시간을 줄이기 위해 할수 있는 일은?
    - MRI영상을 만들기 위한 주파수 정보 필요 -> K-space라는 주파수 공간을 모두 채울수 있을 정도의 주파수 정보 필요. -> 주파수 공간을 모두 채우지 않고, 일부의 주파수 정보만을 획득해서도 MRI영상 만들기

## 9-2. 인공지능 기반 MRI영상 재구성

- MRI영상 재구성 알고리즘
    - 고전적인 주파수 변환 기반 재구성 알고리즘 
        - 단점 : 에일리어징 왜곡현상
            - 주파수 공간을 절반만 채우는 경우 동작을 하지 못함.
            - 영상왜곡 = 에일리어징 왜곡현상이 발생하면 MRI영상은 사용할수 없는게 된다.
                - 영상 상의 특정 부분(상단과 하단부분)이 겹쳐 나타나는게 주 원인임.

                ![image](https://github.com/user-attachments/assets/2992f482-7697-4056-918c-da921de835b7)

                - 가운데 왜곡된 영상을 보면, 영상 윗부분이 모양이 아랫 부분에 불규칙적으로 겹쳐서 나타나고, 아랫부분의 모양이 윗부분의 겹쳐서 나타나고 있는 것을 발견할 수 있다.
                - 이러한 겹쳐서 나타나는 패턴이 불규칙하고 복잡해서 이러한 겹침 현상을 고전적인 주파수 변환 에서는 제대로 고칠수가 없음.

    
- 인공신경망의 MRI 영상 재구성

    ![image](https://github.com/user-attachments/assets/88fccdaa-4c0c-4e03-8319-983972ee1a56)

    - 겹침현상이 복잡하게 일어난 왜곡된 MRI영상이 인공신경망의 입력으로 들어감.
    - 인공신경망의 많은 인공신경들이 서로 정보를 주고받으면서 왜곡패턴을 찾아내고, 그러한 가운데에 겹침현상들을 하나하나 풀어나가게 됨.

    - 왜곡되지 않은 MRI영상과 왜곡된 영상을 입력영상화 해답영상으로 제시하여 여러번 학습후, 정확한 정보의 처리르 통해 온전한 MRI영상을 만들어 낼 수있는 인공지능으로 만들어지게 됨.

- 인공신경망은 다양한 구조로 설계 가능.
    1. 주파수 정보의 복소수적인 면을 다룰 수 있는 신경망
    2. 영상의 전체적인 모습과 세부적인 모습을 단계적으로 살펴볼 수 있는 U모양의 신경망

    3. 영상도메인에서 학습이 가능한 신경망, 주파수 공간. 즉, k-sapce에서 학습이 가능한 신경망

- KIKI-net

    ![image](https://github.com/user-attachments/assets/defb7ddb-fdbe-4150-8fbd-19f369b9c416)

    - k-space와 영상도메인을 반복 이동하면서 학습을 효과적으로 하는 모델
    - (위)주파수 공간의 채워지지 않은 부분들을 채워나가는 인공지능
    - (아래) 영상 도메인에서의 겹침 현상을 풀어나가는 인공지능

## 9-3. 인공지능 기반 MRI영상 생성

- 인공지능을 활용한 대조도영상생성 기술

    - 일반적으로 동일한 부위를 촬영하더라도, 한가지 종류의 영상만을 촬영하는 경우는 드물고, 대부분 2~3 종류의 다른 특성을 갖는 MRI영상을 촬영함.

    
    ![image](https://github.com/user-attachments/assets/271316d8-dd93-412e-8ff1-01314115d176)

    - 위 그림은 무릎부위 MRI 찍은 영상
    - 위처럼 각기 다른 대조도를 보여주느 여러 장의 영상을 촬영
    - MRI의 물리적특성을 이용하여 여러 다른 방식으로 촬영하게 되면 동일 부위에 대해서 서로 다른 진단 정볼르 제공해 줄 수 있는 영상을 얻을 수 있고,  의료진은 이를 종합하여 보다 정확한 진단을 내리는 것이 일반적이다.

    - 하지만 위처럼 여러장의 영상 획득하려면 촬영시간은 더욱 길어짐.

    - 특히 STIR image는 무릎 부위의 지방층 정보를 감소시키는 방식으로 얻은 MRI영상. 다른 3개의 영상보다 획득시간이 더욱 김.

    - 3가지 영상으로부터 STIR image를 자동으로 얻을수 있을까?

- STIR영상들을 자동으로 생성할 수 있는 인공지능개발

    ![image](https://github.com/user-attachments/assets/03267b6a-4e27-40d8-8f54-d045ad7c3cca)

    - T1강조영상, T2강조영상, GRE영상들을 입력으로 받아들이는 인공신경망을 설계하고, 실제 획득한 STIR영상을 해답 영상으로 제시해서 학습시켜 새로운 환자의 영상들이 획득되면 그에 따른 STIR영상들을 자동으로 생성할 수 있는 인공지능이 개발됨.

- BlochGAN

    ![image](https://github.com/user-attachments/assets/26b61508-3856-4778-a1b7-885ff681743d)

    - 단순히 영상생성하는것에만 그치는 것이 아니라, 생성되는 MRI영상이 MRI 물맂거 법칙을 잘 따르면서 생성되도록 하여 보다 정확한 MRI영상을 생성해내는 기술

# 10. 인공지능을 활용한 병리영상 분석

## 10-1. 인공지능 병리 영상분석의 목표 및 목적

- 암진단의 복잡화, 세분화
    - 암조직 주변의 염증세포, 혈관 등과의 **위치 관계 해석** 필요 증가
    - 다양한 **의료정보의 통합** 분석 데이터 기반의 진단 요구

- 다양한 인공지능 기반의 알고리즘 활용
    1. 진단 스크리닝 : 비정상 조직을 구별하여 더 세밀한 진단을 하기 위해서
    2. 형태계측 분석 : 정상 조직 내에 있는 암 조직의 형태와 크기, 모양을 구체적으로 데이터화
    3. 예후 및 치료반응 예측 : 환자의 검진 및 검사로 생성되는 다른 진단 데이터들과 결합
    4. 순서에 따른 감별진단 제시 : 조직에서 가능성 있는 병의 가능성을 순서대로 나열하는 감별진단 제시

- 병리 영상분석의 궁극적 목적 : 병리 진단의 수준을 높여 환자들이 최선의 치료를 받게하는것.
    1. 정확한 진단(Accurate diagnoisi)
        - ex) 전립선암 환자
            - 혈액검사에서 전립선특이항원(PSA) 수치가 높아진 경우 => 전립선 조직에서 미세한 조직을 얻어내는 생검을 받게 됨 > 생검통해 얻은 조직에서 암발견시 전립선 전체 절제수술 / 암 미발견시 미수술.
            - 전립선 조직의 생검은 전립선의 12군데에서 시행하게 됨.
            - 암의크기가 0.5mm가 되지 않을정도로 작은 경우 암 미발견시 적절한 치료 받지 못할수도 있음.
        - ex) 유방암환자
            - 유방암의 림프절 전이 여부를 찾을때 의료진의 진단 오류율이 3.5%가량인데, 인공지능 기반 림프절 암전이 탐지 알고리즘 활용하여 진단보조시, 진단 오류율을 0.5%정도로 매우 낮춘사례있음.
    2. 표준화된 진단(Standardized diagnosis)
        - ex) 유방암환자
            - 유방암 표적치료제(Herceptin)를 사용하려면 HER2유전자가 과발현이 확인되어야 함.
            - 과발현 확인 위해 체취한 암 조직에서 면역조직화학검사(immunohistochemical(IHC) stain)를 수행하여 일정수준(3+) 이나 조금 낮지만 추가적인 형광현장혼성화법(FISH, fluorescence in-situ hybridization)이나 실버동소교잡 반응검사(SISH, silver in-situ hybridization)등에서 이상이 발견된 경우에 확인이 가능
            - 암 조직 내에서 HER2 면역염색검사를 시행하면 종양 조직 군데군데 전형적이지 않은 발현패턴을 보이는 경우가 있어 HER2의 **과발현 여부를 정확하게 평가하기 어려운 경우**가 발생함.
            - 과발현에 대한 진단 차이는 표적치료제 적용 여부와 직결되게 되므로 어던 병리의사가 판독하였느냐에 따라 유방암 환자 치료에 있어서 Herceptin사용 여부에 차이가 발생할 수 있음.
            - 인공지능 기반 병리 알고리즘은 동일 조직에 있어서 **재현성**이 매우 우수한 특성상 정량평가에 강점이 있음.
    3. 예측 진단(predictive diagnosis)
        - 암조직과 암조직 내의 염증세포 등 다양한 종양 주변의 인자들과의 배치 및 거리와 같은 공간적인 관계가 환자의 예후 및 치료제의 치료반응성과 관련이 있음.
        - 인공지능 기반의 영상분석 알고리즘을 활용하면, 암조직과 주변 조직을 인지하고 공간적인 관계의 특징을 수치적으로 추출가능.
    4. 효율적 진단(Efficient diagnosis)
        - 위장관 기질종양은 양성부터 악성까지 다양한 임상행동(clinical behavior)양상을 가지고 있음.
        - 임상행동 양상을 결정하는 가장 중요한 요인 중 하나가 **유사분열의 개수**관찰이다.
        - 병리 의사는 현미경 400배의 고배율로 총 50군데를 관찰하여 유사분열의 수를 수동적으로 세고 있음.
    
## 10-2. 인공지능 병리 영상분석 워크플로우

- 병리 영상분석 알고리즘

    ![image](https://github.com/user-attachments/assets/4a9b6dee-d650-4eb6-9ff2-4f130d3404c1)

    - 병리 영상분석 알고리즘은 환자의 병을 진단하고 예후를 예측하기 위한 영상분류, 병리 영상 내 병변 탐지 및 중요 부분 분할과 같은 병리 영역에서의 필요한 부분을 처리하기 위해 개발됨.

    1. 전체조직영상(whole slide image)으로 디지털화
        - 병리 검칠을 환자에게서 체취함으로서 시작됨!!
        - 환자에게서 작은 크기의 조직을 체취하는 생검이나 또는 수술실 절제 과정을 통해 인체 조직을 체취한 후, 매우 얇은 절편을 내어 인체 조직 표본 슬라이드를 제작.
        - 제작한 슬라이드는 스캐너를 통하여 전체 조직영상 즉, whole slide image로 디지털화 함.

    2. 주석 및 라벨 입력
        - 병리의사는 조직 영상을 검토하여 분석에 적합한 수준의 영상인지를 평가하고 분석 주제에 적합한 영상들을 선별해낸 후, 이미지에서 분석의 중요한 부위를 표시하기 위해서 주석을 달거나 이미지의 암조직 혹은 정상조직 여부와 같은 것을 라벨로 입력
    3. 중요한 특징(feature)들을 추출
        - 그후, 머신러닝 혹은 딥러닝 등의 인공지능 기법을 활용하여 병리 영상에서 분석 결과에 영향을 미치는 중요한 특징들을 추출

    - 추출해낸 특징들을 입력 데이터로 하여 수학적기법이나, 인공지능 기법을 통해 영상을 분류해낼 수 있는 알고리즘이 생성.

- 병리 영상 분석을 위한 다양한 인공지능 방법론
    1. 분류(classification)
        - 입력이미지에 대해서 이미지가 속한 그룹을 예측하는 작업.
        - ex) 대장암 조직이 포함된 영상과 대장암 조직이 없는 정상조직을 분류
        - 분류알고리즘 종류
            1. AlexNet
            2. ResNet
            3. Xception
            4. VGG
            - 주로 machine learning기법이 활용됨
    2. 탐지 및 위치지정(Detection / Localization)
        - 물체의 레이블을 예측하면서 그 물체가 이미지 내의 어느 위치에 있는지 정보를 제공하는 것으로 유방암의 전이가 의심되는 림프절에서 림프절 전이의 위치를 표시해 주는 알고리즘
        - 탐지 및 위치지정 알고리즘 종류
            1. YOLO
            2. R-CNN
    3. 분할(Segmentation)
        - 모든 픽셀에 대해서 각 픽셀이 어떤 그룹에 속해 있는지를 예측하는 것으로 병리 영상에서 중요한 부분을 픽셀 기반으로 나눌 수 있음
        - 분할 알고리즘 종류
            1. FCN
            2. SegNet
            3. DeepLab
            4. U-Net
- 세부 인공지능 병리 영상 주제별 방법론
    - 전립선 생검 조직 선별
        - 전립선 암이 존재하는지를 구별해 내기 위해서는 영상에 전립선암이 포함되었는지를 분류하는 다양한 분류모델들을 활용한 알고리즘을 생성하게 됨.
        - 지도학습 알고리즘 : 자세한 영상주석을 통한 학습 필요
        - 약지도학습 알고리즘 : 영상마다 암조직라벨을 달아 학습필요
    - 신장 생검 내 상세구조 탐지
        - 신장 생검은 사구체, 세뇨관, 혈관 등 여러 조직구조의 변화를 각가 분석하여 진단.
        - 영상에서 사구체의 위치, 개수 등을 파악하고 각 사구체의 상태를 평가하는 알고리즘 필요.

        ![image](https://github.com/user-attachments/assets/d0e14d3b-4fa8-477b-82f9-f41c46e38d07)

        - 신장 생검에서 세뇨관 주변 모세혈관만 추출해내 C4D라는 생체 표지자의 발현 정도를 평가하는 알고리즘을 요약한것.

    - 유방암 분할
        - 병리영상에는 관심 있는 영역인 ROI(Reason of Interest)이외의 구조 또한 포함.
        - 원하는 관심 영역 부분을 자동으로 추출하여 분석하기 위해서는 분할 모델 생성이 필요함.
        - ex) Ki67증식표지자 분석  : 유방암조직에서 ki67증식 표지자가 있는지의 유무를 통해 항암치료의 반응을 예측할수 있음.
        - 병리조직에는 암세포 이외에도 염증세포, 혈관 등 다양한 요소들이 존재하고 여기에 속한 세포들도 Ki67증식표지자를 발현하기 때문에 암조직이 있는 부위에서만 Ki67표지자의 분석이 수행될 수 있도록 분석모델을 만들어야 함. 이를 위해서는 관심영역을 파악하는것이 매우 중요.
    
    ## 10-3. 병리 영상의 특성에 따른 영상분석 모델 개발의 어려움과 극복방안

    1. 크기가 큰 영상
        - 병리영상은 대게 기가 픽셀크기의 초고해상도 이미지
        - 병리 이미지에서 가장 작은 크기로 볼수 있는 생검 조직 영상 한장의 파일크기는 수백메가 바이트 정도이며, 절제 조직 영상의 파일크기는 더 커서 수기가바이트 정도. 
        - 영상의 크기가 매우 커서 단일 이미지로 기계학습 및 딥러닝 수행하기 어려움.
        - 하나의 파일을 수백 픽셀 정도의 크기로 잘게 쪼개어 학습에 사용
    2. 다양한 level(magnification)의 영상
        - 병리 진단시 다양한 배율의 영상이 필요. 
        - 저배율 : 암조직의 전체적인 모양과 주변 조직과의 관계를 파악
        - 중간배율 : 암세포가 만드는 조직학적 구조들의 특성을 파악.
        - 고배율 : 암세포의 핵, 세포질, 핵인 등의 형태적 특성을 자세히 관찰하고 유사분열 등 세포의 변화 상황에 대해서 분석

        - 병리 영상분석 모델 만들때 분석 모델의 주제에 맞게 다양한 배율의 영상을 활용할 필요가 있음.
    3. 디지털 영상데이터가 부족
        - 조직검사는 사람들이 잘 안받음.
    4. 주석데이터 부족
        - solution 
            - 영상에 자세한 병변 주석을 만드는 대신에, 암의 존재 여부 등의 라벨만 붙여 약지도학습기법을 적용하여 알고리즘을 개발
            - 주석 데이터 생성시 몇개의 대표 케이스에 주석을 생성한 후 기계학습 알고리즘을 적용한 후 병리의사가 최종 수정하는 AI-Human협력 방식 개발

    5. 영상의 표준화가 미비하고 정규화 되어 있지 않음
        - CT, MRI 등의 영상은 DICOM표준이 마련되어 사용되고 있음.
        - 병리영상은 표준화되어 있지 않고, 스캐너 제조사 별로 다른 포맷의 이미지를 생성하고 있음.
        - 조직 슬라이드는 의료기관마다 같은 종류의 염색을 수행해도 색상 등이 매우 다양하고 슬라이드 스캐너 별로도 영상의 색이나 대조도 등이 상이함.
        - solution
            - 병리영상 정규화 정립중!
            - 색상 다양성을 정규화하기 위한 기존의 RGB보정 기법 이외에 인공지능 기법인 생성적적대신경망(GAN,Generative Adversarial Network)기법을 활용하여 색상 정규화를 시도
    6. 다수의 영상에 아티팩트 존재
        - 병리영상은 10여단계의 복잡한 과정을 거쳐 생성되기 때문에 다양한 아티팩트를 동반할 수 있음.    
            - ex) 압착(crushing), 절편 과정에서의 칼자국, 커버슬라이드 접착제 및 슬라이드 스캐너의 기술적 한계 등으로 인한 탈초점 
    7. 데이터 과학자 부족
        - solution
            - 학습에 사용될 병리 영상의 질을 자동으로 평가하여 데이터를 취사선택하는 다양한 알고리즘들 개발중
            - 이를 이용시 개발자들은 모든 영상을 일일이 열어보면서 학습에 사용할지를 결정할 필요가 없어 알고리즘 개발 시 큰 도움 받을수 있음.

     

# 11. 인공지능을 활용한 생체 신호 데이터 분석 사례

## 11-1. 생체신호데이터

- 생체신호 데이터 종류

    ![image](https://github.com/user-attachments/assets/6e866675-dbd2-4ba9-b5f6-34eb0a7369f9)

    1. 심전도(ECG, Electrocardiogram)
    2. 동맥압 파형(Arterial Pressure Waveform)
        - 동맥압 파형으로부터 혈관 내 용적 부족을 정확하게 평가해주는 용적상태(PPV,Pulse Pressure Variation)는 현재 거의 모든 환자 모니터에 내장되어 있음
    3. 맥압 파형(PPG, Photo-plethysmogram)
        - 맥압파형기반의 통증(SPI, Surgical Pleth Index)나 용적상태(PVI, Pleth Variability Index)등도 현장에서 널리 사용됨
    4. 뇌파(EEG, Electroencephalogram)

    ![image](https://github.com/user-attachments/assets/3c03bafa-f287-49aa-a4fb-e52f1dbce539)

- 생체 신호 기반의 이차지표

    1. Bispectral Index
    2. FloTrac
    3. HPI

- 생체 신호 데이터의 특징
    - 시간에 따라 계속 **변함**
    - 많은 **노이즈**가 들어가 있음
    - 어떤 파라미터를 이용해야 하는 지 **정해져 있지 않음**
    - 의료인들이 개입해 대부분 정상 값을 만들어 내기 때문에 매우 **비대칭적인(imbalanced)데이터임**


## 11-2. 생체신호분석 인공지능 모델

- 심전도(Electrocardiogram)

    ![image](https://github.com/user-attachments/assets/359ca466-d5a2-4700-b36e-8e771cbc9b37)

    - 심전도는 심장에서 생기는 전기적인 신호를 체표면에서 측정한 신호
    - 전극을 어떻게 붙이냐에 따라 Fontal Plane에서 6개의 lead, Horizontal Plane에서 6개의 lead가 있으며, 이것을 모두 측정한 것을 12 lead ECG라고 함.

    - (paper) Cardiologist-level arrhythmia detection and classification in ambulatory eletrocardiograms using a deep neural network
        ![image](https://github.com/user-attachments/assets/cd47d4a7-2908-46f4-96ee-81ffeb02a08e)

        - [Data & Code](https://stanfordmlgroup.github.io/projects/ecg2/)
        - Andrew Y.Ng이 nature medicine에 출판한 연구 결과로 심전도 분류를 위해 CNN 모델을 이용함.

    - (paper) Prediction of mortality from 12-lead electro-cardiogram voltage data using a deep neural network
        ![image](https://github.com/user-attachments/assets/c6ca377d-1eab-4257-a992-1181263a8cfb)

        - 12-lead ECG로부터 사망을 예측할 수 있다는 연구 결과가 nature medicine에 발표됨

    - (paper) Deep Neural Networks Can Predict New-Onsset Atrial Fibrillation From the 12-Lead ECG and Help Identify Those at Risk of Atrial Fibrillation-Related Stroke
        ![image](https://github.com/user-attachments/assets/8e55c9d1-2658-402c-976c-73ec572a7b18)

        - 1년 이내 발생할 심방 부정맥과 뇌졸증의 리스크를 정확히 예측할 수 있다는 연구결과가 Circulation에 발표됨

- 맥압파형 (PPG, Photo-Plethysmogram)

    ![image](https://github.com/user-attachments/assets/9418956f-c247-4c8c-8d02-ee3303807b42)

    - 심전도와 더불어 웨어러블 장비에서 많이 측정
    - LED에서 나온 빛을 Photodetector에서 측정하여 그 중에 박동성 부분을 그림으로 나타낸것.
    - 맥압파형으로부터 혈압을 추정하는 인공지능 모델
        1. LSTM 이용
        2. feature extraction 후 ada boosting regression등의 방법

- 혈압파형 (Arterial Pressure Waveform)

    ![image](https://github.com/user-attachments/assets/8675e2dc-e0df-4dba-9be7-7a34376dc281)

    - 동맥 내 압력 조직의 산소 공급에 중요한 영향을 주는 인자.
    - 중요한 vital sign이다.
    - 혈압측정방법으론, 팔에 커프를 감아 측정하거나, 동맥에 관을 넣어 침습적으로 측정하는 방식
    - 침습적 혈압측정시 연속적인 혈압 파형을 얻을수 있다는 장점 있음.
    - 혈압파형을 이용한 인공지능 모델
        1. HPI : 수술 중 저혈압을 미리 예측
            - 미국 Edward Lifescience사에서 개발되어 미국 및 국내에서도 임상 현장에서 활용중

            ![image](https://github.com/user-attachments/assets/99372238-dfe7-45c5-a4d0-9e8713aa4851)

            - HPI를 이용하는 것이 저혈압을 줄일 수 있는지에 대해서는 한 연구에서는 저혈압의 75%를 예방 할 수 있다라는 결과 발표됨.

- 뇌파
    - 뇌의 표면에서 발생하는 전기적 신호를 두피에서 측정
    - 임상적으로는 마취중에 환자의 마취 심도를 모니터링
    - 수면분석
    - 간질 등 뇌의 이상을 진단
    - 뇌파을 이용한 인공지능 모델
        1. 뇌파를 분석하여 수면의 심도를 자동적으로 판독
        2. 수술 중 환자에서 마취 심도를 예측

## 11-3. 생체 신호 기반 예후 예측 모델

![image](https://github.com/user-attachments/assets/f25d74d8-6d65-45ee-8f4b-aaba05a9fc00)

- (paper) An Algorithm Based on Deep Learning for Predicting In-Hospital Cardiac Arrest
    - 병실 환자에서 한 시간 단위에 4가지 파라미터(혈압, 심박수, 호흡수, 체온)를 이용하여 입원 환자의 심정지를 예측하는 듀스모델이 뷰노에서 2018년에 개발되어 발표됨. DeepCARS라는 제품명으로 다기관 검증연구 및 식약처 확진 임상시험 마침.

- (paper) A deep learning model for real-time mortality prediction in critically ill children
    
    ![image](https://github.com/user-attachments/assets/3bc5290c-1193-4289-aa90-0b8804ca7cb1)

    - 생체 신호데이터는 환자의 나이나 체중과 같은 임상 정보와 함께 활용될때 진가 발휘함.
    - 중환자에서 9가지 파라미터를 이용하여 중환자실 환자의 사망을 높은 정확도로 예측하는 모델이 소개됨.


- (paper) Development and Evaluation of an Automated Machine Learning Algorithm for In-Hospital Mortality Risk Adjustment Among Critical Care Patients

    ![image](https://github.com/user-attachments/assets/149f7bb6-4905-4688-a6c1-d2454872be4c)

    - 중환자실에서 생체신호 데이터와 검사 결과를 이용하여 사망을 예측하는 모델은 고전적인 사망 예측 모델인 APACHE score보다 훨씬 높은 성능을 보여줌.

- 인공지능 모델은 왜 임상적인 예후 예측에 좋은 성능을 보일까?

    ![image](https://github.com/user-attachments/assets/7ea5faa8-c032-440b-b685-d5a16e3b7d3d)

    - 고전적인 임상 예측 모델은 입력 변수가 출력 변수에 미치는 영향이 일정하고 선형적인 모델을 사용
    - 그러나 실제적 관계는 비선형적이고 일정하지 않은 경우가 많음.
    - 인공지능 모델은 내부에 많은 노드들이 있어 이러한 비선형적인 모델도 쉽게 모델링할수 있음. 

- Overview : Remote Surveillance
    ![image](https://github.com/user-attachments/assets/a94c7298-f3d5-499b-af7f-58cc1c80d348)


# 12. 인공지능을 활용한 EMR 정형데이터 분석사례

## 12-1. 임상현장에 적용하는 임상의사결정지원시스템(CDSS, Clinical Decision Support System)

: 환자의 임상정보를 바탕으로 의료인이 진료 과정에서 적절한 임상적 판단과 의사 결정을 할 수 있도록 돕는 시스템

- ex) (paper)  Transfusion after total knee arthroplasty can be predicted using the machine learning
 algorithm (Knee Surgery, Sports Traumatology, Arthroscopy (2020) 28:1757–1764)

    - 정형외과에서 수행하는 무릎관절 전치환술 환자를 대상으로 진행
    - 무릎관절 전치환순 ? 관절염이 심한 무릎관절을 인공관절로 바꾸는 수술.
    - 수술과정중 많은 출혈발생가능 -> 수혈이 필요할 가능성을 미리 예측
    - 환자들의 EMR데이터를 바탕으로 조기에 출혈 위험도를 미리 예측하는 ML모델 개발

    ![image](https://github.com/user-attachments/assets/3908853e-8673-4e5f-b922-a4ac1cb4d1fc)

    - stpe1) 환자선별하는 포함 및 제외기준 세우기
    - stpe2) 선별된 환자의 데이터를 EMR로부터 추출
        - 환자의 인구통계학정보(demographics), 관측된 환자의 상태나 진단정보(comorbidities), 수술형태, 약제처방 및 투약이력, 진단검사 정보
        - 해당 정보들에서 정형외과 전문의의 전문지식에 의거하여 총 43개의 변수 추출 
    - stpe3) 수혈이 필요한 케이스를 분류하는데에 적합한 변수들을 선택하여 모델의 예측 성능 높이고 경량화진행
        - 변수 선택이유는?
            1. 머신러닝이 중요도가 떨어지는 변수를 학습하는것 방지
            2. 만들어진 모델의 사용성을 높이기 위해
                - ML은 많은 변수를 고려하여 유의미한 관계를 찾아낼 수 있지만, 향후 이 머신러닝 모델을 여러 의료현장에서 사용하기 위해서는 입력 변수의 수를 적정 수준으로 줄여서 어떤 환경에서도 쉽게 이용가능하게 만들기 위함.
    - stpe4) GBM(Gradient Boosting Model)을 학습함.
    - stpe5) 범용적 예측성능파악 위해, 학습에 사용되지 않은 다른 기관의 데이터를 이용하여 수혈 위험도를 예측하는 모델의 성능 검증
        - AUROC(Area Under ROC Curve) 0.880 정도로 수혈이 필요로 하는 환자를 조기에 분류할수 있었음.
    - stpe6) 이후 웹서비스를 통해, ML모델에 투입되어야 할 인자들을 직접 입력하면 해당 환자의 수혈 위험도를 계산해줌. 또한 입력 변수 각각이 수혈 위험도에 어느 정도의 영향력을 발휘하는지를 시각화로 표현.
        
        ![image](https://github.com/user-attachments/assets/69299413-d75d-4ef0-b2e8-131e1f0153a6)


## 12-2. 대규모 EMR 데이터를 활용하여 환자 별 질환 가능성을 예측

- (paper) A clinically applicable approach to continuous prediction of future acute kidney injury(Nature, 2019)

    - 대규모 EMR데이터를 급성신장손상(AKI, Acute Kidney Injuery)이 발생할 위험도가 높은 환자를 예측, 딥마인드 개발

    - 분석에 사용한 대규모 EMR데이터 : 미국보훈처(US Department of Veterans Affairs)의 헬스케어 시스템의 데이터
        - 미국보훈처는 현역 및 퇴역 군인들과 가족들이 이용할수 있는 헬스케어 시스템
    - 데이터의 구성
        - 1234개의 의료기관(1062개 외래 전문기관 / 172개 입원전문병원)
        - 70만 명의 성인환자
    
    - 모델링 결과, 48시간 이전에 데이터를 활용하여 급성신장손상이 발생한 입원환자 55.8%를 조기에 분류해냈으며, 후속 신장 투석 투여가 필요한 급성 신장 손상 케이스를 90.2%를 미리 예측함.

    - 사용모델

        ![image](https://github.com/user-attachments/assets/340029c6-676d-4548-9fb8-e41cfd5b295d)

        - 순환신경망(RNN)
        - 보훈처의 EMR 빅데이터에서 사용가능한 변수는 총 62만개.
        - 이중, 급성 신장손상과 관련이 있는 315개의 변수를 미리 선정.
        - 투입되는 315개의 변수 데이터는 있는 그대로 사용된 것은 아니고 neural network에 들어갈 수 있도록 정해진 차원의 벡터로 임베딩 되는 부분이 입력층에 들어감.
        - RNN을 거쳐 주 target 예측을 수행하는 층(main predictions)과 보조 타겟예측(Auxillary predictions)을 수행하는 층을 출력층으로 설계
        - 보조 타겟예측 층은 딥러닝 모델의 학습을 가속화하기 위해 사용하는 기술적인 부분이고, 실제 급성 신장 손상을 예측하는 부분은 주 타겟예측 층.

        ![image](https://github.com/user-attachments/assets/f58fd132-93c5-4e4c-b444-f4079367d220)

        - EMR데이터는 위와같이 표현되어 순환 신경망 모델에 투입.
        - 동그라미는 6기간 단위의 블록에 해당되고, 6시간 동안 관측된 315개 요인에 대한 임베딩 벡터가 투입.
        - 그리하여, 48시간 동안의 데이터로 24시간 뒤의 급성 신장 손상, 48시간 뒤의 급성 신장 손상, 72시간 뒤의 급성 신장 손상을 예측하는 모델 만듬
        - 24시간 뒤의 stage 3의 급성 신장손상 예측 AUROC 0.988의 높은 성능 기록.
        - 모든 단계의 급성 신장 손상을 예측하는 모델은 AUROC 0.934기록.

## 12-3. 약물 재창출을 위한 EMR 데이터 활용

- 신약개발분야에서 EMR 데이터는 기존 약제에 새로운 적응증을 찾아내는 용도로 활용.

- (paper)  Validating drug repurposing signals using electronic health records: a case study of metformin associated with reduced cancer mortality (Journal of
 the American Medical Informatics Association, 2015)
    - 대규모 EMR데이터를 활용하여 메트포민(Metformin)이라는 약물이 암 진단 후 사망률 감소에 유의미한 영향을 미친다는 연구 발표.
    - 메트포민은 본래 제2형 당뇨병 환자에게 처방되는 약물
    - EMR내 암환자 레지스트리를 분석한 결과 당뇨병을 갖고 있는 암환자와 당뇨병을 갖고 있지 않은 모든 암환자 모두에게 메트포민을 사용하는 경우, 메트포민을 사용하지 않는 경우보다 암 진단 후 사망률이 낮은 것을 발견.

# 13. 인공지능을 활용한 EMR 비정형데이터 분석사례

## 13-1. 예측기반 의료 인공지능

### 13-1-1. 질병진단

- Automatic Coding
    -  비정형 텍스트로부터 질병 코드를 예측하는 문제
    - 2018년 Mullenbach와 동료들이 제안한 **CAML 모델**은 MIMIC-III의 퇴원 요약지로부터 ICD-9 코드를 예측하는 문제를 풀어냄.

    - CAML모델이 퇴원 요약지로부터 ICD-9 코드를 예측하는 예시

        ![image](https://github.com/user-attachments/assets/51a15edf-0aed-40e8-bd74-98b4f44254e2)

        - CAML 모델이 ICD-9 934.1, “Foregin body in main bronchus”를 퇴원 요약지 내의
        “large mucus plug on”이라는 정보를 토대로 예측한 것을 보여줌.
        - 의료 현장에서 별도로 질병 코드를 기입할 필요 없이, 기록지만으로 질병 코드를 추출함으로써 같은 정보를 여러 가지 방법으로 수기로 작성해야하는 번거로움을 줄일 수 있음.
 ### 13-2. 개체명 인식

비정형 데이터인 문자열 내에서 개체명, 이를테면 이미 사회적으로 정의된 사람, 장소, 시간,
단위 등에 해당하는 단어(개체명)를 인식하여 추출 및 분류하는 기법

쉽게 말해, 문서 내에 존재하는 단어(개체)를 보고, 그 단어가 어떤 유형에 해당하는지를 인식하는 기술.

- 개체명 인식은 정보 추출을 목적으로 발전하여, 현재는 자연어 처리, 정보 검색 등에 사용되
고 있다.
    - 예시) 철수[person]는 2021년 7월[time] 병원[organization]에서 근무하고 있다.
        - 철수는 사람 / 날짜는 시간 / 병원은 기고나으로 인식함.
- 논문 “A Survey on Deep Learning for Named Entity Recognition”에서는 NER을 “문자열 안에
서 NE의 위치를 알아내고, 사전 정의한 카테고리에 따라 알맞게 분류하는 작업”이라 정의함.
- 즉, 문자열을 입력으로 받아 단어별로 해당되는 태그를 내뱉게 하는 multi-class 분류 작업으
로 성격을 정의할 수 있다.

- 의료분야에서는 생물의학용어에 해당하는 단어를 인식한 후 추출 및 분류하여, 다양한 태스
크에 활용할 수 있게 해준다.
    - ex) 추출된 생물의학용어들을 외부 데이터베이스에 연결하여 더 보강된 설명을 가져
오는데 사용할 수 있다.

    - Zhu가 제안한 개체명 인식 및 연결 프레임워크

        ![image](https://github.com/user-attachments/assets/6ddf09e8-7950-4235-9cbb-9a78038b14c2)

        -Zhu의 연구는 의학 용어의 표현형만으로는 의미를 판별하기 힘든 단어를 주변 맥락 클러스터를 통해 본래 의미와 가장 가까운 데이터베이스 요소로 연결하는 연구를 제안.
        - 문서 내의 오탈자 및 다양한 약어 표현을 하나의 정제된 표현으로 표현하는데도 개체명
인식이 사용될 수 있음.

## 13-2. 생성기반 의료 인공지능

### 13-2-1. 질의응답(QA)

- 정보검색 및 자연어 처리 (NLP) 분야의 주제 중 하나로 자연 언어로 인간이 제기한 질문에 자동으로 응답하는 시스템을 구축하는 것을 목표 함
- 의료 인공지능 분야에서도 질의 응답은 주목 받는 토픽이며, 매년 개최되는 BioASQ 챌린지
를 비롯하여 다양한 질의 응답 데이터가 등장하고 있음

- 의학 질의 응답 데이터셋의 예시.

    ![image](https://github.com/user-attachments/assets/703c8531-9a0a-4a3f-9693-63db6a7b327b) 
    
    - 과거에는 규칙 기반(rule based) 질의 응답 모델이 강세를 보였으나, 최근 등장한 사전 학습 기법에 힘입어 학습 기반의 BioBERT, ClinicalBERT 등의 모델들이 의료 질의 응답 분야에서 강세를 보이고 있음.
    - 실제 상용화 된 예시로는 IBM 왓슨의 질의 응답 시스템이 있으며, 의사의 진단을 보조하는 용
도로 활용되고 있음

### 13-2-2. 챗봇

- 챗봇은 인공 지능(AI)과 자연어 처리(NLP)를 사용하여 사용자가 텍스트나 그래픽, 음성을 통
해 웹서비스, 앱과 상호 작용할 수 있도록 돕는다.

- 챗봇이 자연스러운 인간 언어를 이해하고 인간의 대화를 시뮬레이션하며 간단한 자동화 작
업을 실행합니다. 메시징 앱이나 모바일 앱, 웹 사이트, 전화선, 음성 지원 앱과 같은 다양한
채널에서 챗봇이 사용될 수 있다.

    - ex 1) 컴퓨터에서 제품을 조사하면 화면에 도움이 필요한지 묻는 창이 나타난다. 
    - ex 2) 콘서트에 가는 길에 스마트폰을 사용하여 채팅을 통해 차량을 요청한다. 
    - ex 3) 음성 명령을 사용하여 이웃 카페에서 커피를 주문하고 주문이 언제 준비되고 비용이 얼마인지 알려주는 응답을 받는다. 
    
- 의료 현장에서도 챗봇은 다양하게 활용될 수 있다. 
    - ex) 구글과 네이버에서 출범한 ARS 인공지능인 전환 챗봇등이 병원 업무들에 투입될
수 있다.

        ![image](https://github.com/user-attachments/assets/307d8d57-fa02-4405-a122-cf11ccf210f5)
        
        - 네이버 AI 고객센터의 예시

### 13-2-3. 기록지 생성

- 의료 인공지능의 또 다른 활용 분야는 방사선 이미지로부터 임상 기록지를 생성하는 것.
- 2018년 Jing은 방사선 이미지를 입력으로 받아, 이 이미지를 가장 잘 설명하는 임상 기록지
를 생성하는 연구를 발표

    ![image](https://github.com/user-attachments/assets/0f1cfa6c-04e7-4a0a-8d76-3b49db580e19)

    - 해당 기술의 목표
        - 단순히 방사선과 임상의의 진단 과정을 대체하는 것이 아니라, 해석 가
능한 과정을 통해 방사선 임상의의 진단을 보조하는 것.

            ![image](https://github.com/user-attachments/assets/f2eda318-1f9e-4f5c-ab86-1424326fa3d0)
            
            - 방사선 기록지를 생성할 때 이미지의 어떤 부분을 모델이 중점적으로 보고 있
            는지 도시화한 그림


# 14. 인공지능의 임상검증 방법론

## 14-1. 알고리즘의 정확도에 대한 임상검증

1. 알고리즘의 정확도에 대한 임상검증

- 정확도 평가 지표
    - 민감도 (recall)
    - 특이도
    - ROC 곡선 면적 (AUC)
    - 양성예측도 (precision)
    - Precision-Recall 곡선 면적
    - F1 점수

- 이중, 민감도, 특이도, ROC 곡선 면적이 가장 자주 이용됨
    - ex) 유방촬영검사 영상에서 유방암을 진단하는 인공지능 알고리즘
        - 유방촬영검사 영상을 인공지능이 분석하여 유방암이 의심되는 곳을 빨간색으로 표시한 예

            ![image](https://github.com/user-attachments/assets/c4fa2a1f-c74f-4138-8878-fe4ff52237e1)
             
                - Permission 얻을 수 있는 곳:
                https://docs.google.com/forms/d/1i-ItabZugyG2Hu-KSk3l05UjfCmzELFl_Hh4LOfu47o/viewform?edit_re quested=true)

            ![image](https://github.com/user-attachments/assets/6102c15c-ee68-48e3-88a8-33a1bae990f7)

            - sensitivity : 민감도는 유방암이 있는 환자 중 인공지능이 유방암이 있다고 결과를 제시한 비율
            - Specificity : 특이도는 유방암이 없는 환자 중 인공지능이 유방암이 없다고 결과를 제시한 비율



        - 실제로는 유방암이 있을 가능성을 마치 확률처럼 가령 0(가능성 가장 낮음) 에서 1(가능성 가장 높음) 사이의 어떤 숫자로 제시하는 것
        - 어느 값 이상이면 인공지능이 유방암을 있다고 판단한 것인지를 미리 정해 놓고 이에 따라 사용자에게 최종 결과를 제시하게 됨
        - 그러므로, 기준을 어떻게 정하는지에 따라 알고리즘의 민감도와 특이도가 달라짐
        - 기준을 0에서 1까지 조금씩 높여가며 민감도와 특이도를 구해 볼 수 있슴.
        - 기준을 높임에 따라 민감도는 점차 낮아지고 특이도는 점차 높아지게 됨
        - 이 때, 1빼기 특이도 값을 x축 그리고 민감도를 y축 으로 해서 그린 그래프가 ROC 곡선이고, ROC 곡선 아래의 면적을 알고리즘의 정확도 지표로 사용하게 됨. ROC 곡선 면적은 0에서 1까지 값을 가지며 값이 클수록 높은 정확도를 의미.


2. 임상적 유용성에 대한 임상검증

- 인공지능 알고리즘이 실제로 환자진료에 어떤 도움을 주는지 를 알아보는 것을 말함
- 알고리즘의 정확도가 높으면 임상적 유용성이 있을 가능성이 높겠지만 반드시 그런 것은 아니며,반대로, 알고리즘의 정확도가 아주 높지 않더라도 인공지능 알고리즘을 어떻게 활용하는가에 따라 임상적인 유용성이 있을 수도 있슴.

- 인공지능 알고리즘의 임상적인 유용성 검증사례
    - ex)뉴욕 마운트사이나이 병원에서 수행된 한 연구
        - 연구 논문: Morey JR, Zhang X, Yaeger KA, Fiano E, Marayati NF, Kellner CP, De Leacy RA, Doshi A, Tuhrim S, Fifi JT. Real-World Experience with Artificial Intelligence-Based Triage in Transferred Large Vessel Occlusion Stroke Patients. Cerebrovasc Dis. 2021;50(4):450-455

        - 두부 전산화단층촬영 영상을 가지고 뇌의 큰 혈관에 혈전증으로 인한 폐쇄가 있는지를 분석
        - 뇌졸중이 의심되어 응급으로 두부 전산화단층촬영 검사를 받는 환자들이 있으면 검사와 동시에 이 인공지능이 영상을 분석할 수 있도록 설치를 하였고, 만일 뇌의 큰 혈관 폐쇄가 의심되는 경우에는 인공지능이 자동으로 혈전용해술 팀에게 즉시 응급 호출을 보내도록 하였습니다. 
        - 이와 달리 통상의 진료 흐름은, 검사 후 영상의학과 의사가 영상을 판독하고 판독 결과를 치료를 담당하는 의료진에 전달한 후 혈전용해술이 필요하다 결정되면 혈전용해술 팀을 소집하는 단계를 거치는데, 진료 업무가 많은 병원의 경우 상당한 시간이 소요 됨
        - 따라서, 인공지능을 사용함으로써 혈전용해술 팀을 소집하는 데에 걸리는 시간을 단축 가능
        - 뇌의 대혈관 폐쇄로 인한 뇌졸중은 치료를 위한 골든타임(golden time)이 짧아 치료까지의 시간을 단축시키는 것이 치료결과에 큰 영향을 미침
        - 실제로, 인공지능을 사용할 경우 환자가 응급실에 도착하는 때부터 혈전용해술 팀에 연락하는 데까지 소요되는 시간이 중위값 기준 40분에서 25분으로짧아졌고 더 일정해졌으며 환자가 응급실에 도착하는 때부터 혈전용해술을 시작하기까지의 시간도 중위값 기준으로 25분이 줄었음.
        -  또 눈여겨 볼 부분은, 이 인공지능이 아주 정확하지는 않아 불필요한 호출을 하는 경우도 발생한다는 것입니다. 하지만 신속한 치료에 따른 환자치료효과의 향상으로 인한 이득이 불필요한 호출로 인한 손해보다 커서 임상적인 유용성이 입증되었음.
        - 만일 알고리즘의 정확도만 가지고 평가를 하였다면 임상적 유용성을 판단하기 어려웠을 것.
    - 2020년 9월 미국의 Centers for Medicare and Medicaid Services
        - 인공지능 알고리즘의 사용에 대해 가산료 형태로 의료보험을 적용하기로 결정
        - 인공지능 알고리즘에 의료보험이 인정되는 최초의 사례
  
- 참고문헌
    1) Park SH, Choi J, Byeon JS. Key Principles of Clinical Validation, Device Approval, and Insurance Coverage Decisions of Artificial Intelligence. Korean J Radiol. 2021 Mar;22(3):442-453.
    2) Park SH, Han K. Methodologic Guide for Evaluating Clinical Performance and Effect of Artificial Intelligence Technology for Medical Diagnosis and Prediction. Radiology. 2018 Mar;286(3):800-809.
    3) Erickson BJ, Kitamura F. Magician's Corner: 9. Performance Metrics for Machine Learning Models. Radiol Artif Intell. 2021 May 12;3(3):e200126.
    4) He X, Frey E. ROC, LROC, FROC, AFROC: an alphabet soup. J Am Coll Radiol. 2009 Sep;6(9):652-5.
    5) 박성호, 임태환 외, 인공지능: 보건의료전문가를 위한 길라잡이, 군자출판사, 2020

# 15. 임상현장의 미충족 수요와 의료인공지능

- X-ray의 조직의 흡수율 차이를 이용한 전산 단층촬영 = CT
    - CT가 의학에서 공간문제를 해결하였음.
- 몸 안에 수소 분자의 수피 내 분포를 이용한 자기 공명영상 = MRI
- 관류강조영상(PWI, perfusion weighted image) : 환자의 해부학적인 구조 및 혈류흐름 파악 가능

- 확산강조영상 (DWI, diffusion weighted image) : 미세조직의 구조 볼수 있음
- 대류영상 (ventilation image) : 폐의 공기의 흐름을 볼수 있음
- 4D 유동강조 MR영상 : 4차원 유동강조 MR영상

- 의료영상장비의 원리
    1. 고유의 특성
    2. SNR(Signal-to-noise ratio)
        - 영상장비의 시그널과 노이즈의 비율
        - SNR이 클수록 영상의 대조도가 높고 Signal-to-noise-ratio가 커지기 때문에 좋은 영상을 얻을수 있음.
    3. Artifact

- 의료영상 모달리티의 특성
    1. CR(Computed Radiography)
    2. DR(Digital Radiography)
    3. US
    4. CT
    5. MRI
    6. 양전자 단층촬영 (PET, Proton Emission Tomography)

    현재는 영상파일 표준인 DICOM을 이용한 영상저장통신장치(PACS)가 개발됨.

- 의료인공지능을 위한 임상 미충족 수요
    - CT영상에서 기도(airway)분할

        ![image](https://github.com/user-attachments/assets/4f94568c-a5a6-4874-8fcc-812409a5eb06)

        - 기도는 폐의 안에서 프랙탈 형태로 분화되어 폐를 가득 채우고 있고, 실제 CT영상에 많은 폐의 기도가 보이지만, 계속 가늘어져서 CT영상에서 부분용적효과를 만들고 호흡이나 심장의 움직임 떄문에 영상에서 연속성이 유지되지 않는 문제가 발생.
        - 따라서, 이를 분할하는 것은 고전적인 영상처리 기법에서 가장 어려운 난제였음!!
            - 영상처리 기도분할(고전영상처리) > 수작업 추가 기도분할(1~2시간) > 완전자동화 딥러닝을 이용한 기도분할(2~8등분)
            - 이제는 자동으로 기도를 만들어줌. 전제조건은 기도분할 프로그램 개발시 남들과 질적으로 다른 수준의 정답을 만든것!

    1. 더 빠르게 더 좋은 SNR로 영상을 촬영하는 것
    2. 의료영상 분할 및 정합을 통한 정량적 측정
    3. 의료영상에서 질병을 진단하거나 치료 결과, 사망을 예측하는 것.

- 임상적 미충족 수요들의 다양한 문제점들

    - 현실 의료영상 데이터는 long tail distribution을 따르기 때문에 균형 있는 학습이 어렵고, 레이블이 매우 비쌈.
    - 모든 과정이 환자의 개인정보를 보호하고 윤리적으로 학습되어야 하며, 의학발달 및 임상진료에 사용되기 위해서 설명가능해야함.
    - 병원의 PACS에 연계되거나, 병기과의 현미경과 결합되거나, 내용기반 검색(Content based image retrieval)등으로 사용될 수 있어야 하하며, 인공지능 모델 경량화 필요.

- 생물학읜 근본원리(central dogma)
    - DNA > RNA> Protein > Cell > organ > Individual Human > Pupulation


 ---
 
 
 [맨 위로 이동하기](#){: .btn .btn--primary }{: .align-right}