# 확률분포

- 확률변수가 특정한 값을 가질 확률을 나타내는 함수.
- 확률분포는 각각의 사건들에 대한 확률 값을 표현할수 있다.

    - 확률변수 : 각각 등장할 수 있는 사건에 대한 값을 나타내기 위한 변수.


1. 이산확률분포 : 확률변수 X의 개수를 정확히 셀수 있을때를 이산확률 분포라고 한다.
    ex) 주사위 눈금 X의 확률분포는 이산확률분포로 나온다(1/6으로)
    - 모든사건들에 대해서 확률값을 더한건 1이다.

2. 연속확률분포 : 확률변수 X의 개수를 정확히 셀수 없을때 연속확률분포라고 한다.(이때는 확률밀도 함수를 이용해 분포를 표현)
    ex) 연속적인 값(키, 달리기 성적)
    - 정규분포형태로 보여줄수있다.(정규분포는 뮤(평균값)와 시그마값(표준편차)으로 표현된다."표준편차의 제곱 = 분산"으로 표현된다.)

    - 실제 세계의 많은 데이터는 정규분포로 표현할수 있다. (ex.인간의아이큐/ 평균값을 100, 표준편차를 24로 두고 보통 정규분포를 진행한다.)
        - 모든 사람들에게 동등한 난이도의 시험지를 풀도록 한 뒤에 각각의 점수에 대해서 평균 값을 구한 뒤에 그 평균값을 100에 맞추어서 각각의 사람들을 점수에 따라서 통계적으로 표현한 것.
        - 정규분포에서는 평균값에다가 시그마값에 2배를 한값을 더한것이 상위 2%정도의 값을 가진다고 보면 된다.
        ![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/73f01e92-c1c0-4b28-b96f-1e4c8b13afb6)

# 이미지 데이터에 대한 확률분포

- 이미지 데이터는 다차원 특징 공간(많은 pixel로 구성되어 있으며 하나의 pixel은 또한 RGB로 구성)의 한 점으로 표현된다.
    - 이미지의 분포를 근사하는 모델을 학습할수 있다.
- 사람의 얼굴에는 **통계적인 평균치**가 존재할 수 있다.
    - 모델은 이를 수치적으로 표현할 수 있게 된다.

- 이미지에서의 다양한 특징들이 각각의 확률 변수가 되는 분포를 의미함.
    - ![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/81f7c241-91f0-494e-bcad-edb8f361fbf3)
    - 다변수 확률분포(multivariate probability distribution)
        - ex) hidden layer에서 dimension=2 라고 가정한다면 위처럼 2개의 특징에 대한 값들이 catch가 될것이다.

# 생성모델(Generative Models)

: 생성 모델은 실존하지 않지만 있을 법한 이미지를 생성할 수 있는 모델을 의미.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/1af22dcd-b0a4-4b59-9a4c-3bbc2dc78ec1)

    - Discriminative(판별모델) : classification model이라면 특정한 decision boundry를 학습하는 형태
    - Generative(생성모델) : 각각의 클래스에 대해서 적절한 분포를 학습하는 형태    

즉, Generative model은 있을법한 이미지와 같은 데이터를 만드는 모델을 의미. 

- 여러개의 joint probability distribution형태로 통계적인 모델로 표현할수 있는 경우가 많다.
- 새로운 data instance를 만들 수 있는 architecture이다.

* instance : 사진 한장과 같은 구별되는 데이터 개체를 의미.

확률 분포를 잘 학습할수 있다면 모델은 통계적인 평균치를 내재할수 있다.

만약 확률 분포가 인간의 얼굴에 대한 분포를 잘 학습했다고 하면, 바로 이러한 형태가 될수 있다.
모델이 학습을 잘했다고 하면 확률값이 높은 부분에 대한 변수, 즉 이미지를 샘플링을 하게 된다면 있을법한 이미지가 나오게 된다.

우리가 분포를 잘 학습한 뒤에 확률이 높은 부분에서부터 출발해서 약간의 노이즈를 섞어 가면서 random하게 sampling을 한다면, 그렇게 만들어진 이미지는 굉장히 다양한 형태로 그럴싸한 이미지들을 만들어 낼수 있을것이다.

# 생성모델의 목표

- 이미지 데이터의 분포를 근사하는 모델 G를 만드는것이 생성모델의 목표.
- 모델 G가 잘 동작한다는 의미는 원래 이미지들의 분포를 잘 모델링 할 수 있다는 것을 의미.

- ![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/a13f76ee-2ebf-420a-9d96-319614bc4d9e)

    - 모델 G는 원래 데이터(이미지)의 분포를 근사할 수 있도록 학습된다.
    - 각각의 점들로 표현된 이유는 실제로 학습할때 가지고 있을 수 있는 데이터는 유한하기 때문이다.
    - (a)부터 (d)까지 학습을 거치고 나면, 우리가 학습을 시키는 모델(초록색/ 생성모델)은 원본데이터의 분포를 잘 따라가는 형태로, 잘 근사해서 학습이 되는 형태로 존재하게 된다.
    - 즉, 학습이 지속될수록 생성 모델 G가 원본 데이터의 분포를 잘 학습할 수 있다는 말이다!
    - 학습이 잘 되었다면, **통계적으로 평균적인 특징을 가지는 데이터**를 쉽게 생성할 수 있다.


# GAN(Generative Adversarial Networks)

- 생성자(generator)와 판별자(discriminator) 2개의 네트워크를 활용한 생성 모델.
- 다음의 목적함수(objective function)를 통해 생성자는 이미지 분포를 학습할 수 있다.

- ![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/126c612c-4df7-4131-ae5f-7c8deeaa6287)

    - 하나의 함수V가 있을때 V는 D와G, 2개의 함수로 구성됨.
    - V라는 함수의 값을 G(생성자)는 min, 즉 낮추고자 노력하고 D(판별자)는 max, 즉 높이고자 노력한다.
    
    - 목적함수 해석)
        - (왼쪽) pdata는 원본데이터의 distrubution을 의미한다. 즉, 원본데이터에서 1개의 데이터인 x를 샘플링한다. = 즉 그냥 한개 꺼내겠다. = dataset에서 하나의 이미지를 뽑아서 넣는것.
        - 이미지를 뽑아서 넣은값 = D(x) 이다. 이것에 log를 취한 값에 기대값([] 이안에 있음 기대값이다.) 즉, 평균값을 구하겠다는 의미이다.
        - 정리1) 여러개의 데이터를 뽑은 다음에 D에 넣고 그 값에 log를 취해서 그 평균값을 구하겠다란 의미.

        - (오른쪽) 생성자에 대한 개념이 포함되어 있음.
        - 기본적으로 생성자는 항상 noise vector로부터 입력을 받아서 새로운 이미지를 만들수가 있다. 
        - pz 같은 경우는 하나의 noise를 뽑을 수 있는, noise를 sampling할수 있는 distribution이다.
        - 그러한 분포에서 random하게 하나의 noise를 sampling한 뒤에 그 noise를 생성자 G에 넣어서 가짜 이미지를 만든 다음에, 그러한 가짜 이미지를 뒤에 넣은값에 -를 붙이고  거기에 1을 더한 값에다가 log를 취한값의 평균값이라 볼수 있다.
    

    - Generator : 기본적으로 생성자, 즉 G는 하나의 noise vector인 d를 받아서 새로운 이미지 instance를 만들수 있다.
    - Discriminator : 판별자, 이미지 x를 받아서 이미지가 얼마나 진짜 같은지에 대한 확률 값을 출력으로 내보낸다. 그래서 어떠한 이미지가 있을때 그 이미지가 real distribution, 즉 학습데이터의 분포에서 나온것인가에 대한 확률 값을 부여하는것.(이떄 진짜 이미지에는 1을 부여, fake에는 0을 부여하는 방식으로 학습함.이때 이러한 출력 값은 확률값으로써 1~0사이의 값으로 존재한다.)
    - **정리) 판별자는 학습을 하면서 원본데이터에 대해서 1로 분류할수 있도록 학습이 된다. 이때 log같은경우는 monolithic(하나로 되어있는)한 함수이기 때문에, logD(x)값을 max한다는 것은 원본 데이터에 대해서는 1을 뱉을수 있도록 학습을 하겠다란 것. 반면에 가짜 이미지가 들어왔을때 G(z)는 그 가짜 이미지에 대해서 0을 뱉을수 있도록 학습을 하겠다라고 보면 된다. 반면에 생성자 G같은경우는 오른쪽에 해당하는 것만 사용하게 된다.  그이유는 왼쪽 term은 G가 포함되지 않기 때문에 마치 상수처럼 볼수 있기 때문이다.그래서 결과적으로 생성자인 G는 오른쪽에 있는 term을 minimize하기 때문에, 즉, 자기가 만든 가짜 이미지가 판별자(D)에 의해서 진짜라고 인식이 될수있도록, 즉, 판별자(D)가 1을 내뱉을수 있도록 학습을 진행하는것. 다시말해, 생성자는 자기가 만드는 이미지가 그럴싸한 이미지로 보일수 있도록 그러한 방향으로 학습을 진행하겠다라고 말할수 있음.**

    - Latent vector = noise vector인 z가 들어왔을때 generator를 거쳐서 하나의 fake image를 만들어내고, 이렇게 만들어진 fake image는 discriminator에 들어가게 되어서, loss를 구한뒤에 (오른쪽에 보이는 term을 낮춰야하기에) 해당 값이 줄어드는 방향으로 생성자인 G를 update하기 위해서 이러한 손실값을 g로 미분해서 거기에 -learning rate를 곱한 값을 이용해서 이와같이 반복적으로 update를 해준다. 
    - 반면에 D같은 경우는 fake image와 real image를 같이 받아서 이러한 real image에 대해서는 1을 부여하고 fake image에 대해서는 0을 부여할수 있도록 그러한 방향으로 학습을 진행하는걸 확인할수 있다. 그래서 전체 loss함수를 D로 미분한뒤에 그러한 gradient값을 타고 올라갈 수 있도록 ascending(오름차순)하는 방향으로 학습하는 걸 확인할수 있다.(Discriminator에는 +가 붙어있고 생성자 part에는 -가 붙어있음을 확인가능.)

    - 동일한 시기에 D와 G가 서로 다른 목적을 가지기 떄문에, 일종의 relax game, 즉, 게임이론에 기반하는 optimization 문제로 볼수 있으며,이런식으로 동일한 식에 대해서 G는 min, D는 max방향으로 학습하게 된다면, 결과적으로 생성자 G는 굉장히 그럴싸한 이미지를 만들어 낼수있는 생성모델이 될것이다.

    - 프로그램상에서는 d를 먼저 학습하고 G를 그 이후 학습하거나, 반대로 G를 먼저학습하고 D를 학습하는 식으로 매번 mini batch마다 이 2개의 network를 한번씩 학습하는 방식을 반복해서 D와 G가 각각 optimal한 point로 잘 학습할 수 있도록 유도한다.

# GAN에서의 기댓값 계산방법
: E(기대값)이 붙는것 자체가 어떤 여러개의 데이터를 다룰때에 대한 평균값을 나타내고자 할때 사용할수 있다.

- 프로그램상에서 기댓값(expected value)을 계산하는 가장 간단한 방법은?
    : 단순히 모든 데이터를 하나씩 확인하여 식에 대입한 뒤에 평균을 계산.

- 왼쪽Term : 원본 데이터 분포(data distribution)에서의 샘플 x를 뽑아 logD(x)의 기댓값 계산

- 오른쪽 term : 노이즈 분포에서의 샘플 z를 뽑아 log(1 - D(G(z)))의 기댓값 계산
 
## 기댓값 공식

- ![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/ced1282d-5321-4f99-b87b-9e05ff29debe)

    - 이산확률변수
        - 각각의 사건값(Xi)에 대해서 확률값(f(Xi))을 곱한뒤에 그 값들을 전부 더하면 된다.

        - ex) 주사위의 기대값은 1x1/6 + 2x1/6 ... 6x1/6 = 21/6 =3.5
            : 주사위를 굴렸을때 나오는 눈금값의 기대값이라고 할수있다.
            - 만약 주사위 두번 굴렸을때 기대값은 7이고, 실제로 처음에 3, 두번쨰 5가 나왔다면 기대값이 8이므로 해당결과로 운이 좋은사람이라고 판단할수 있다.
        - 기대값은 각각의 사건값에 대해서 확률 값을 곱한 것들을 모두 더해서 구할수 있다.

    - 연속확률변수도 적분식에 의해서 기대값을 표현할수 있다.
    - 대문자X는 확률변수를 의미 / 소문자x는 사건을 의미 / f(x)는 확률분포함수를 의미.

# GAN의 수렴과정

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/ff7d18f6-470c-469b-8204-001c31f5c376)

- GAN공식의 목표는 생성자의 분포가 원본학습데이터의 분포를 잘 따를수 있도록 만드는것이다.  = Pg -> Pdata로 수렴할 수 있도록 만드는것. 

- discriminator는 학습이 모두 이루어진 뒤에는 fake image와 real image를 더이상 구분할 수 없기 때문에 항상 1/2 즉, 50%라는 값을 내보내게 되는것이다. 즉, 생성자가 내보낸 가짜 이미지에 대해서 그 뒤는 더이상 구별할 수가 없다

- 시간의 흐름으로 보자.
    -  z 공간에서 매번 z를 sampling해서 생성자에 넣기 때문에 이러한 과정은 z 도메인에서 x의 도메인으로 mapping이 이루어지는것으로 표현가능함.
    - 처음에는 생성자의 분포가 원본 데이터 분포를 잘 학습하지 못했기 때문에 discriminator또한 이를 잘 구분하지 못한다.

    - 학습이 진행될수록 우리가 학습한 분포가 원본데이터에 discriminator을 적절히 따라갈수 있게 되기 때문에, 생성모델의 분포가 원본 데이터의 분포를 잘 학습하는 걸 확인할수 있다.

    - (d)에서 판별모델의 분포는 1/2로 수렴하는것을 확인할수 있다.
        - 바로 앞에 설명했던 목적함수를 이용해서 학습을 하게되면 이러한 목표를 달성할수 있다는것이고 학습이 모두 이루어졌을때 (d)의 형태로 각각의 분포가 수렴하게 될것이다.

- **학습진행시 왜 이러한 생성자에 분포가 Pdata로 수렴할수가 있을까?**
    - 이것이 GAN의 가장 핵심!!!!
    - 학습을 하게 되면 생성모델의 분포는 원본데이터의 분포를 잘 배우게 되고, 우리가 가지고 있는 데이터는 이상적으로, 특정 갯수만큼 데이터를 가지고 있는 건데, 각각의 데이터를 이런식으로 검은색 점으로 변한다고 하면, 학습이 된 이후에 검은색 점에 해당하지 않은 다른 부분들에 대해서 데이터를 꺼내게 된다면 이것이 바로 새로운 이미지 즉, new data instance라고 할수 있다. 즉, 학습 이후에 확률이 높은 부분을 중심으로 해서 약간의 noise를 섞어서 이렇게 데이터를 뽑아내게 된다면 원본데이터는 아니지만, 그럴싸한 새로운 이미지를 만드는것으로 이해할수 있다.

# 증명) global Optimality

: 학습진행시 Pg가 Pdata로 수렴하게 되는지에 대해 증명하기 위해서 가장 먼저 global optimality를 증명해보자.

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/d6fe4783-0c97-4d4f-ac95-50a3b460e633)

-  매 상황에 대해서 생성자와 판별자가 각각 어떤 point로 global Optimality를 가지는지에 대해서 설명하는것.

-  첫번째 명재 : G fixed / G가 고정되어 있는상황에서 뒤에 optimal point는 Pdata(x) / (Pdata(x) + Pg(x)) 라고 할수 있다.

    - 함수 V를 표현한걸 해석해보자.
        - 아까 목적함수를 꺼내보자.
        - E[X] (확률변수X에 대한 개대값) 는 적분공식으로 표현가능하기 때문에, 분포가 Pz(z)이기 떄문에 아래쪽에 그대로 Pz(z)가 들어간다. 이때 dz 즉, z 도메인에서 sampling된 noise vector를 G에 넣어서 우리가 데이터 X를 만들어 낼수 있기 때문에, 이러한 과정은 도메인 Z에서 X로 mapping되는 과정과 마찬가지로 볼수있다. 그래서 이러한 값을 X로 다시 치환해서 하나의 적분식으로 표현할수 있다.
    - function y : 어떠한 함수 y가 있을때 a log(y) + b log(1-y) 라고 했을때, 기본적으로 이런 y값은 a/a+b의 위치에서 극대값을 가질수 있다는 점을 알수 있다. 이를 그림으로 표현한것을 보고 이해하자.

# 증명2) global Optimality

: 생성자의 global Optimal point가 어디인가에 대해 확인해보자. 우리가 학습을 하게되면 생성자의 분포는 원본데이터 distribution을 따라가게 된다. 

![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/b98060d9-9405-43e0-afed-60489949b765)

- 별도의 함수 C를 정의.
    - 이때 C는 이러한 V 값을 최대로 만드는 D에 대한 V함수이다. 즉, 특정한 fixed G함수에 대해서 global Optimal을 가지는 D함수에 대한 V 함수로 정의할수 있다.
    - log의 안쪽으로 각각 분자 부분에 2를 곱해 주고 -log(4)를 붙여서 동일한 값을 구할수 있다. 이렇게 바꿔줬을때 앞의 term과 뒤의 term 에 대한 내용은 KL(Pdata||Pg)로 표현할수 있다. 이건 2개의 분포가 있을때 얼마나 차이가 나는지에 대해 수치적으로 표현하기 위해서 일반적으로 사용할수있는 공식이다. 흔히 "쿨백-라이블러 발산" 이라고도 부른다.(https://daebaq27.tistory.com/88).
    - 두개의 기대값을 더한 값을 이러한 형태로 표현 가능. 이렇게 바꿔주는 이유는, 증명의 편의성떄문에 바꿔준것임.
    - 바로, JSD (https://ddongwon.tistory.com/118)을 쓰기위해서임.
        * 일반적으로 KL은 distance matrix으로 활용하기엔 어렵다. 그래서 JSD같은걸 이용하면 distance matrix으로 효과적으로 사용할수 있다. 이때 JSD는 두개의 분포 P와Q가 있을떄 두 분포의 distance를 구하는데 사용할수 있다.
        * JSD는 distance matrix이기 때문에 최소값을 0으로 가진다는 특징이 있다. 즉, Pdata와 Pg가 동일할때 이 값을 0이란 값을 가지게 되서 사라지게 되기 떄문에, 우리는 최소값으로 -log(4) 값을 얻을수 있다는걸 알수가 있고, 바로 이렇나 global optimum point을 얻을수 있도록 해주는 유일한 솔루션은, 생성자의 분포와 Pdata가 동일할때, 즉, 우리생성자가 내뱉는 이미지가 원본데이터 distribution과 동일할때 이러한 global optimum point를 가질수 있다는 것이다. 따라서 생성자는 매번 D가 이미 잘 수렴해서 global optimal을 가지고  있다고 가정한 상태에서 생성자가 잘 학습 된다면 바로 이렇나 값을 가질수 있도록 잘 수렴해서 Pdata와 같은 분포를 가지는 형태로 수렴할 수 있을것이다. 라고 말하는 내용이다. 물론 이 증명은 생성자와 판별자 각각에 대해서 global optimum point가 존재할수 있다는 내용을 증명한 내용이고,  사실 학습이 잘되어서 이러한 global optimal에 잘 도달할수 있는가에 대한 내용은 엄밀히 말하면 다른 내용이다. 

- GAN의 목적함수를 알았고 그러한 목적함수로 학습을 진행했을때 global optimum point가 어딘지도 수학적으로 알아보았음!!

# GAN 알고리즘

: 실제로 프로그래밍 상에서 어떻게 구현할수 있는지에 check해보기

- ![image](https://github.com/OC-JSPark/oc-jspark.github.io/assets/46878973/c29cae98-ac30-465b-b355-d8d6f09ee0a3)

- 학습 반복횟수 만큼 반복하도록 만들기 = epoch
- 매 epoch마다 K번 discriminator를 학습한 뒤에 
- generator를 학습한다.

- discriminator를 학습할때 m개의 noise를 뽑은 뒤에 마찬가지고 m개의 원본데이터를 sampling한다. 그래서 discriminator같은 경우는 바로 다음에 기울기 값을 구한다음에, 경사를타고 ascending을 해서 이렇나 식의 값을 maximize하는 형태로 학습이 되는걸 확인할수 있다. 그래서 원본데이터에 대해서는 1이라는 값을 내 뱉도록 만들고 이렇게 fake image에 대해서는 0이라는 값을 내보내는 형태로 학습이 되는걸 확인할수 있다.

- 생성자를 학습시킬때에도 m개의 noise를 sampling한 뒤에 m개의 fake image를 만든뒤에 기울기 값을 낮추는 방향으로 생성자를 학습시키는걸 확인할수 있다.

* ?오토인코더계열 체크해봐
(https://github.com/ndb796/Deep-Learning-Paper-Review-and-Practice/blob/master/code_practices/GAN_for_MNIST_Tutorial.ipynb)